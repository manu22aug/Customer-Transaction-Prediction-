{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c6ae5d",
   "metadata": {},
   "source": [
    "###### PROJECT TEAM ID : PTID-CDS-JAN-24-1761"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d539d91",
   "metadata": {},
   "source": [
    "##### PROJECT CODE : PRCP-1003-Customer Transaction Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356947a5",
   "metadata": {},
   "source": [
    "##### PROJECT NAME : Customer Transaction predictio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bb47d",
   "metadata": {},
   "source": [
    "### Business Case:\n",
    "    Predicting customer transactions can provide significant benefits for businesses across various industries.\n",
    "\n",
    "*Improved Customer Experience: By accurately predicting customer transactions, businesses can anticipate customer needs and preferences in advance. This enables personalized recommendations, targeted promotions, and proactive customer service, leading to a more seamless and satisfying customer experience.\n",
    "\n",
    "*Enhanced Marketing Effectiveness: Transaction prediction allows businesses to segment their customer base more effectively based on transaction behavior. This enables targeted marketing campaigns tailored to specific customer segments, resulting in higher engagement, conversion rates, and ROI on marketing spend.\n",
    "\n",
    "*Optimized Inventory Management: Predicting customer transactions helps businesses forecast demand for products or services more accurately. This enables optimized inventory management, reducing stockouts and excess inventory, minimizing storage costs, and improving cash flow.\n",
    "\n",
    "*Fraud Detection and Prevention: Transaction prediction models can identify potentially fraudulent activities in real-time by detecting deviations from normal transaction patterns. This helps businesses mitigate risks associated with fraudulent transactions, reducing financial losses and protecting brand reputation.\n",
    "\n",
    "*Revenue Growth: By understanding customer transaction patterns, businesses can identify opportunities to upsell or cross-sell additional products or services. This increases average transaction value and overall revenue generation.\n",
    "\n",
    "*Churn Prevention: Transaction prediction can also help identify customers at risk of churning based on their transaction behavior. By proactively engaging at-risk customers with targeted retention strategies, such as special offers or loyalty rewards, businesses can reduce churn rates and retain valuable customers.\n",
    "\n",
    "*Operational Efficiency: Predictive analytics can streamline operational processes by automating routine tasks associated with transaction processing, such as order fulfillment, payment verification, and customer support. This improves efficiency, reduces manual errors, and frees up resources for more strategic initiatives.\n",
    "\n",
    "*Competitive Advantage: Businesses that leverage transaction prediction effectively gain a competitive edge by staying ahead of market trends, anticipating customer needs, and delivering superior experiences compared to competitors.\n",
    "\n",
    "*Data-Driven Decision Making: Transaction prediction empowers businesses with actionable insights derived from data analysis. By leveraging these insights, businesses can make informed decisions across various functions, including product development, pricing strategies, and resource allocation.\n",
    "\n",
    "*Scalability and Flexibility: Transaction prediction models can be scaled and adapted to accommodate growth and changes in business dynamics. Whether expanding into new markets, launching new products, or responding to evolving customer preferences, predictive analytics provides the flexibility to adjust strategies accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27596f7",
   "metadata": {},
   "source": [
    "#### Domain Analysis:  \n",
    " Domain analysis of customer transactions involves examining the various aspects related to transactions between a business and its customers within a specific domain or industry.\n",
    "\n",
    ".Transaction Types: Identify the different types of transactions that occur within the domain. This could include purchases, refunds, exchanges, subscriptions, etc.\n",
    "\n",
    ".Transaction Channels: Determine the channels through which transactions take place. This may include in-person transactions at physical locations, online purchases through a website or app, phone orders, etc.\n",
    "\n",
    ".Transaction Volume: Analyze the volume of transactions processed within the domain over a given period. This helps in understanding the scale of business activity and identifying peak periods.\n",
    "\n",
    ".Payment Methods: Explore the various payment methods used by customers to complete transactions. This could involve credit/debit cards, cash, digital wallets, bank transfers, etc.\n",
    "\n",
    ".Transaction Security: Assess the security measures in place to protect customer transactions from fraud and unauthorized access. This includes encryption protocols, PCI compliance, authentication processes, etc.\n",
    "\n",
    ".Transaction Lifecycle: Examine the stages involved in a typical transaction lifecycle, from initiation to completion. This includes order placement, payment processing, fulfillment, and post-transaction activities such as returns or customer support.\n",
    "\n",
    ".Transaction Data: Analyze the data generated by customer transactions, including purchase history, customer demographics, product preferences, etc. This data can be valuable for understanding customer behavior and informing business decisions.\n",
    "\n",
    ".Regulatory Compliance: Ensure that transactions comply with relevant regulations and standards governing the industry, such as consumer protection laws, data privacy regulations (e.g., GDPR), and financial regulations (e.g., PCI DSS).\n",
    "\n",
    ".Transaction Costs: Evaluate the costs associated with processing transactions, including payment processing fees, overhead costs, and any other expenses incurred in facilitating transactions.\n",
    "\n",
    ".Transaction Experience: Assess the overall customer experience during the transaction process, including ease of use, speed, reliability, and responsiveness of transaction systems and interfaces.\n",
    "\n",
    ".Transaction Integration: Consider how transaction systems integrate with other systems within the business ecosystem, such as inventory management, accounting, CRM, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f80fbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from functools import reduce\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score, precision_score ,classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93797f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d08d1fe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>2.1743</td>\n",
       "      <td>-3.4132</td>\n",
       "      <td>9.4763</td>\n",
       "      <td>13.3102</td>\n",
       "      <td>26.5376</td>\n",
       "      <td>1.4403</td>\n",
       "      <td>14.7100</td>\n",
       "      <td>6.0454</td>\n",
       "      <td>9.5426</td>\n",
       "      <td>17.1554</td>\n",
       "      <td>14.1104</td>\n",
       "      <td>24.3627</td>\n",
       "      <td>2.0323</td>\n",
       "      <td>6.7602</td>\n",
       "      <td>3.9141</td>\n",
       "      <td>-0.4851</td>\n",
       "      <td>2.5240</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>2.5516</td>\n",
       "      <td>15.5752</td>\n",
       "      <td>-13.4221</td>\n",
       "      <td>7.2739</td>\n",
       "      <td>16.0094</td>\n",
       "      <td>9.7268</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>12.0039</td>\n",
       "      <td>13.8571</td>\n",
       "      <td>-0.7338</td>\n",
       "      <td>-1.9245</td>\n",
       "      <td>15.4462</td>\n",
       "      <td>12.8287</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>9.6508</td>\n",
       "      <td>6.5674</td>\n",
       "      <td>5.1726</td>\n",
       "      <td>3.1345</td>\n",
       "      <td>29.4547</td>\n",
       "      <td>31.4045</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>15.6599</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>-5.6011</td>\n",
       "      <td>19.0614</td>\n",
       "      <td>11.2663</td>\n",
       "      <td>8.6989</td>\n",
       "      <td>8.3694</td>\n",
       "      <td>11.5659</td>\n",
       "      <td>-16.4727</td>\n",
       "      <td>4.0288</td>\n",
       "      <td>17.9244</td>\n",
       "      <td>18.5177</td>\n",
       "      <td>10.7800</td>\n",
       "      <td>9.0056</td>\n",
       "      <td>16.6964</td>\n",
       "      <td>10.4838</td>\n",
       "      <td>1.6573</td>\n",
       "      <td>12.1749</td>\n",
       "      <td>-13.1324</td>\n",
       "      <td>17.6054</td>\n",
       "      <td>11.5423</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>1.8944</td>\n",
       "      <td>0.6939</td>\n",
       "      <td>-13.6950</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>35.4734</td>\n",
       "      <td>1.7093</td>\n",
       "      <td>15.1866</td>\n",
       "      <td>2.6227</td>\n",
       "      <td>7.3412</td>\n",
       "      <td>32.0888</td>\n",
       "      <td>13.9550</td>\n",
       "      <td>13.0858</td>\n",
       "      <td>6.6203</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>5.3523</td>\n",
       "      <td>8.5426</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>4.1569</td>\n",
       "      <td>3.0454</td>\n",
       "      <td>7.8522</td>\n",
       "      <td>-11.5100</td>\n",
       "      <td>7.5109</td>\n",
       "      <td>31.5899</td>\n",
       "      <td>9.5018</td>\n",
       "      <td>8.2736</td>\n",
       "      <td>10.1633</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>12.5942</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>2.4354</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>16.5346</td>\n",
       "      <td>12.4205</td>\n",
       "      <td>-0.1780</td>\n",
       "      <td>5.7582</td>\n",
       "      <td>7.0513</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>-8.9921</td>\n",
       "      <td>9.7797</td>\n",
       "      <td>18.1577</td>\n",
       "      <td>-1.9721</td>\n",
       "      <td>16.1622</td>\n",
       "      <td>3.6937</td>\n",
       "      <td>6.6803</td>\n",
       "      <td>-0.3243</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>8.6086</td>\n",
       "      <td>11.0738</td>\n",
       "      <td>8.9231</td>\n",
       "      <td>11.7700</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>-4.4223</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>14.8743</td>\n",
       "      <td>9.4317</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>-0.5687</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>-9.6953</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>10.6261</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>-13.5174</td>\n",
       "      <td>2.2439</td>\n",
       "      <td>11.5283</td>\n",
       "      <td>12.0406</td>\n",
       "      <td>4.1006</td>\n",
       "      <td>-7.9078</td>\n",
       "      <td>11.1405</td>\n",
       "      <td>-5.7864</td>\n",
       "      <td>20.7477</td>\n",
       "      <td>6.8874</td>\n",
       "      <td>12.9143</td>\n",
       "      <td>19.5856</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>6.4059</td>\n",
       "      <td>9.3124</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>15.6372</td>\n",
       "      <td>5.8200</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>9.1854</td>\n",
       "      <td>12.5963</td>\n",
       "      <td>-10.3734</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>3.7163</td>\n",
       "      <td>-1.1016</td>\n",
       "      <td>7.3667</td>\n",
       "      <td>9.8565</td>\n",
       "      <td>5.0228</td>\n",
       "      <td>-5.7828</td>\n",
       "      <td>2.3612</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>12.1719</td>\n",
       "      <td>19.7312</td>\n",
       "      <td>19.4465</td>\n",
       "      <td>4.5048</td>\n",
       "      <td>23.2378</td>\n",
       "      <td>6.3191</td>\n",
       "      <td>12.8046</td>\n",
       "      <td>7.4729</td>\n",
       "      <td>15.7811</td>\n",
       "      <td>13.3529</td>\n",
       "      <td>10.1852</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>19.0773</td>\n",
       "      <td>-4.4577</td>\n",
       "      <td>9.5413</td>\n",
       "      <td>11.9052</td>\n",
       "      <td>2.1447</td>\n",
       "      <td>-22.4038</td>\n",
       "      <td>7.0883</td>\n",
       "      <td>14.1613</td>\n",
       "      <td>10.5080</td>\n",
       "      <td>14.2621</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>20.4031</td>\n",
       "      <td>17.0360</td>\n",
       "      <td>1.6981</td>\n",
       "      <td>-0.0269</td>\n",
       "      <td>-0.3939</td>\n",
       "      <td>12.6317</td>\n",
       "      <td>14.8863</td>\n",
       "      <td>1.3854</td>\n",
       "      <td>15.0284</td>\n",
       "      <td>3.9995</td>\n",
       "      <td>5.3683</td>\n",
       "      <td>8.6273</td>\n",
       "      <td>14.1963</td>\n",
       "      <td>20.3882</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>5.7033</td>\n",
       "      <td>4.5255</td>\n",
       "      <td>2.1929</td>\n",
       "      <td>3.1290</td>\n",
       "      <td>2.9044</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>28.7632</td>\n",
       "      <td>-17.2738</td>\n",
       "      <td>2.1056</td>\n",
       "      <td>21.1613</td>\n",
       "      <td>8.9573</td>\n",
       "      <td>2.7768</td>\n",
       "      <td>-2.1746</td>\n",
       "      <td>3.6932</td>\n",
       "      <td>12.4653</td>\n",
       "      <td>14.1978</td>\n",
       "      <td>-2.5511</td>\n",
       "      <td>-0.9479</td>\n",
       "      <td>17.1092</td>\n",
       "      <td>11.5419</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>8.8186</td>\n",
       "      <td>6.6231</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>-11.7218</td>\n",
       "      <td>24.5437</td>\n",
       "      <td>15.5827</td>\n",
       "      <td>3.8212</td>\n",
       "      <td>8.6674</td>\n",
       "      <td>7.3834</td>\n",
       "      <td>-2.4438</td>\n",
       "      <td>10.2158</td>\n",
       "      <td>7.4844</td>\n",
       "      <td>9.1104</td>\n",
       "      <td>4.3649</td>\n",
       "      <td>11.4934</td>\n",
       "      <td>1.7624</td>\n",
       "      <td>4.0714</td>\n",
       "      <td>-1.2681</td>\n",
       "      <td>14.3330</td>\n",
       "      <td>8.0088</td>\n",
       "      <td>4.4015</td>\n",
       "      <td>14.1479</td>\n",
       "      <td>-5.1747</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>14.5362</td>\n",
       "      <td>-1.7624</td>\n",
       "      <td>33.8820</td>\n",
       "      <td>11.6041</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>5.8442</td>\n",
       "      <td>4.7086</td>\n",
       "      <td>5.7141</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>20.5092</td>\n",
       "      <td>3.2790</td>\n",
       "      <td>-5.5952</td>\n",
       "      <td>7.3176</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>-7.0927</td>\n",
       "      <td>-3.9116</td>\n",
       "      <td>7.2569</td>\n",
       "      <td>-5.8234</td>\n",
       "      <td>25.6820</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-9.7009</td>\n",
       "      <td>2.4013</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>9.3908</td>\n",
       "      <td>-13.2648</td>\n",
       "      <td>3.1545</td>\n",
       "      <td>23.0866</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>5.3745</td>\n",
       "      <td>-6.2660</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-0.8417</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>9.1738</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>-3.8294</td>\n",
       "      <td>-1.0370</td>\n",
       "      <td>11.7770</td>\n",
       "      <td>11.2834</td>\n",
       "      <td>8.0485</td>\n",
       "      <td>-24.6840</td>\n",
       "      <td>12.7404</td>\n",
       "      <td>-35.1659</td>\n",
       "      <td>0.7613</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>12.6832</td>\n",
       "      <td>9.5503</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>5.2091</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>12.3972</td>\n",
       "      <td>14.4698</td>\n",
       "      <td>6.5850</td>\n",
       "      <td>3.3164</td>\n",
       "      <td>9.4638</td>\n",
       "      <td>15.7820</td>\n",
       "      <td>-25.0222</td>\n",
       "      <td>3.4418</td>\n",
       "      <td>-4.3923</td>\n",
       "      <td>8.6464</td>\n",
       "      <td>6.3072</td>\n",
       "      <td>5.6221</td>\n",
       "      <td>23.6143</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>-3.9989</td>\n",
       "      <td>4.0462</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>24.4187</td>\n",
       "      <td>4.5290</td>\n",
       "      <td>15.4235</td>\n",
       "      <td>11.6875</td>\n",
       "      <td>23.6273</td>\n",
       "      <td>4.0806</td>\n",
       "      <td>15.2733</td>\n",
       "      <td>0.7839</td>\n",
       "      <td>10.5404</td>\n",
       "      <td>1.6212</td>\n",
       "      <td>-5.2896</td>\n",
       "      <td>1.6027</td>\n",
       "      <td>17.9762</td>\n",
       "      <td>-2.3174</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>4.5474</td>\n",
       "      <td>7.5509</td>\n",
       "      <td>-7.5866</td>\n",
       "      <td>7.0364</td>\n",
       "      <td>14.4027</td>\n",
       "      <td>10.7795</td>\n",
       "      <td>7.2887</td>\n",
       "      <td>-1.0930</td>\n",
       "      <td>11.3596</td>\n",
       "      <td>18.1486</td>\n",
       "      <td>2.8344</td>\n",
       "      <td>1.9480</td>\n",
       "      <td>-19.8592</td>\n",
       "      <td>22.5316</td>\n",
       "      <td>18.6129</td>\n",
       "      <td>1.3512</td>\n",
       "      <td>9.3291</td>\n",
       "      <td>4.2835</td>\n",
       "      <td>10.3907</td>\n",
       "      <td>7.0874</td>\n",
       "      <td>14.3256</td>\n",
       "      <td>14.4135</td>\n",
       "      <td>4.2827</td>\n",
       "      <td>6.9750</td>\n",
       "      <td>1.6480</td>\n",
       "      <td>11.6896</td>\n",
       "      <td>2.5762</td>\n",
       "      <td>-2.5459</td>\n",
       "      <td>5.3446</td>\n",
       "      <td>38.1015</td>\n",
       "      <td>3.5732</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>30.5644</td>\n",
       "      <td>11.3025</td>\n",
       "      <td>3.9618</td>\n",
       "      <td>-8.2464</td>\n",
       "      <td>2.7038</td>\n",
       "      <td>12.3441</td>\n",
       "      <td>12.5431</td>\n",
       "      <td>-1.3683</td>\n",
       "      <td>3.5974</td>\n",
       "      <td>13.9761</td>\n",
       "      <td>14.3003</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>8.9500</td>\n",
       "      <td>7.1954</td>\n",
       "      <td>-1.1984</td>\n",
       "      <td>1.9586</td>\n",
       "      <td>27.5609</td>\n",
       "      <td>24.6065</td>\n",
       "      <td>-2.8233</td>\n",
       "      <td>8.9821</td>\n",
       "      <td>3.8873</td>\n",
       "      <td>15.9638</td>\n",
       "      <td>10.0142</td>\n",
       "      <td>7.8388</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>2.9253</td>\n",
       "      <td>10.4994</td>\n",
       "      <td>4.1622</td>\n",
       "      <td>3.7613</td>\n",
       "      <td>2.3701</td>\n",
       "      <td>18.0984</td>\n",
       "      <td>17.1765</td>\n",
       "      <td>7.6508</td>\n",
       "      <td>18.2452</td>\n",
       "      <td>17.0336</td>\n",
       "      <td>-10.9370</td>\n",
       "      <td>12.0500</td>\n",
       "      <td>-1.2155</td>\n",
       "      <td>19.9750</td>\n",
       "      <td>12.3892</td>\n",
       "      <td>31.8833</td>\n",
       "      <td>5.9684</td>\n",
       "      <td>7.2084</td>\n",
       "      <td>3.8899</td>\n",
       "      <td>-11.0882</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>2.5881</td>\n",
       "      <td>-2.7018</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>-7.1541</td>\n",
       "      <td>-6.1920</td>\n",
       "      <td>18.2366</td>\n",
       "      <td>11.7134</td>\n",
       "      <td>14.7483</td>\n",
       "      <td>8.1013</td>\n",
       "      <td>11.8771</td>\n",
       "      <td>13.9552</td>\n",
       "      <td>-10.4701</td>\n",
       "      <td>5.6961</td>\n",
       "      <td>-3.7546</td>\n",
       "      <td>8.4117</td>\n",
       "      <td>1.8986</td>\n",
       "      <td>7.2601</td>\n",
       "      <td>-0.4639</td>\n",
       "      <td>-0.0498</td>\n",
       "      <td>7.9336</td>\n",
       "      <td>-12.8279</td>\n",
       "      <td>12.4124</td>\n",
       "      <td>1.8489</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>19.3038</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>-5.5963</td>\n",
       "      <td>14.0685</td>\n",
       "      <td>11.9171</td>\n",
       "      <td>11.5111</td>\n",
       "      <td>6.9087</td>\n",
       "      <td>-65.4863</td>\n",
       "      <td>13.8657</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>-0.1346</td>\n",
       "      <td>14.4268</td>\n",
       "      <td>13.3273</td>\n",
       "      <td>10.4857</td>\n",
       "      <td>-1.4367</td>\n",
       "      <td>5.7555</td>\n",
       "      <td>-8.5414</td>\n",
       "      <td>14.1482</td>\n",
       "      <td>16.9840</td>\n",
       "      <td>6.1812</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>9.2048</td>\n",
       "      <td>8.6591</td>\n",
       "      <td>-27.7439</td>\n",
       "      <td>-0.4952</td>\n",
       "      <td>-1.7839</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>-4.3205</td>\n",
       "      <td>6.9860</td>\n",
       "      <td>1.6184</td>\n",
       "      <td>5.0301</td>\n",
       "      <td>-3.2431</td>\n",
       "      <td>40.1236</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>-0.7264</td>\n",
       "      <td>4.5886</td>\n",
       "      <td>-4.5346</td>\n",
       "      <td>23.3521</td>\n",
       "      <td>1.0273</td>\n",
       "      <td>19.1600</td>\n",
       "      <td>7.1734</td>\n",
       "      <td>14.3937</td>\n",
       "      <td>2.9598</td>\n",
       "      <td>13.3317</td>\n",
       "      <td>-9.2587</td>\n",
       "      <td>-6.7075</td>\n",
       "      <td>7.8984</td>\n",
       "      <td>14.5265</td>\n",
       "      <td>7.0799</td>\n",
       "      <td>20.1670</td>\n",
       "      <td>8.0053</td>\n",
       "      <td>3.7954</td>\n",
       "      <td>-39.7997</td>\n",
       "      <td>7.0065</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>14.0553</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>14.7246</td>\n",
       "      <td>35.2988</td>\n",
       "      <td>1.6844</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>-22.9264</td>\n",
       "      <td>12.3562</td>\n",
       "      <td>17.3410</td>\n",
       "      <td>1.6940</td>\n",
       "      <td>7.1179</td>\n",
       "      <td>5.1934</td>\n",
       "      <td>8.8230</td>\n",
       "      <td>10.6617</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>28.2749</td>\n",
       "      <td>-0.1937</td>\n",
       "      <td>5.9654</td>\n",
       "      <td>1.0719</td>\n",
       "      <td>7.9923</td>\n",
       "      <td>2.9138</td>\n",
       "      <td>-3.6135</td>\n",
       "      <td>1.4684</td>\n",
       "      <td>25.6795</td>\n",
       "      <td>13.8224</td>\n",
       "      <td>4.7478</td>\n",
       "      <td>41.1037</td>\n",
       "      <td>12.7140</td>\n",
       "      <td>5.2964</td>\n",
       "      <td>9.7289</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>12.1316</td>\n",
       "      <td>12.5815</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>5.6518</td>\n",
       "      <td>10.9346</td>\n",
       "      <td>11.4266</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>7.7532</td>\n",
       "      <td>6.6173</td>\n",
       "      <td>-6.8304</td>\n",
       "      <td>6.4730</td>\n",
       "      <td>17.1728</td>\n",
       "      <td>25.8128</td>\n",
       "      <td>2.6791</td>\n",
       "      <td>13.9547</td>\n",
       "      <td>6.6289</td>\n",
       "      <td>-4.3965</td>\n",
       "      <td>11.7159</td>\n",
       "      <td>16.1080</td>\n",
       "      <td>7.6874</td>\n",
       "      <td>9.1570</td>\n",
       "      <td>11.5670</td>\n",
       "      <td>-12.7047</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>9.9110</td>\n",
       "      <td>20.1461</td>\n",
       "      <td>1.2995</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>19.8234</td>\n",
       "      <td>4.7022</td>\n",
       "      <td>10.6101</td>\n",
       "      <td>13.0021</td>\n",
       "      <td>-12.6068</td>\n",
       "      <td>27.0846</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>33.5107</td>\n",
       "      <td>5.6953</td>\n",
       "      <td>5.4663</td>\n",
       "      <td>18.2201</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>21.2607</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>-1.7759</td>\n",
       "      <td>3.1283</td>\n",
       "      <td>5.5518</td>\n",
       "      <td>1.4493</td>\n",
       "      <td>-2.6627</td>\n",
       "      <td>19.8056</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>-3.3456</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>1.7189</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>9.7685</td>\n",
       "      <td>4.8910</td>\n",
       "      <td>12.2198</td>\n",
       "      <td>11.8503</td>\n",
       "      <td>-7.8931</td>\n",
       "      <td>6.4209</td>\n",
       "      <td>5.9270</td>\n",
       "      <td>16.0201</td>\n",
       "      <td>-0.2829</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>train_199995</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4880</td>\n",
       "      <td>-0.4956</td>\n",
       "      <td>8.2622</td>\n",
       "      <td>3.5142</td>\n",
       "      <td>10.3404</td>\n",
       "      <td>11.6081</td>\n",
       "      <td>5.6709</td>\n",
       "      <td>15.1516</td>\n",
       "      <td>-0.6209</td>\n",
       "      <td>5.6669</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>-9.5348</td>\n",
       "      <td>13.9860</td>\n",
       "      <td>5.2982</td>\n",
       "      <td>8.2705</td>\n",
       "      <td>14.1527</td>\n",
       "      <td>7.4540</td>\n",
       "      <td>-5.0105</td>\n",
       "      <td>12.0465</td>\n",
       "      <td>8.6349</td>\n",
       "      <td>9.9137</td>\n",
       "      <td>25.1376</td>\n",
       "      <td>1.0914</td>\n",
       "      <td>3.2326</td>\n",
       "      <td>7.7802</td>\n",
       "      <td>13.9939</td>\n",
       "      <td>2.9085</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>4.2369</td>\n",
       "      <td>7.5665</td>\n",
       "      <td>-9.2149</td>\n",
       "      <td>9.5746</td>\n",
       "      <td>1.4012</td>\n",
       "      <td>7.4211</td>\n",
       "      <td>11.0075</td>\n",
       "      <td>7.8080</td>\n",
       "      <td>4.5567</td>\n",
       "      <td>4.9861</td>\n",
       "      <td>9.7471</td>\n",
       "      <td>0.0722</td>\n",
       "      <td>5.9053</td>\n",
       "      <td>8.1743</td>\n",
       "      <td>10.8800</td>\n",
       "      <td>11.1665</td>\n",
       "      <td>4.2600</td>\n",
       "      <td>-2.1296</td>\n",
       "      <td>8.7833</td>\n",
       "      <td>-15.5727</td>\n",
       "      <td>-8.4916</td>\n",
       "      <td>22.1905</td>\n",
       "      <td>12.4110</td>\n",
       "      <td>15.1168</td>\n",
       "      <td>1.6041</td>\n",
       "      <td>6.1868</td>\n",
       "      <td>10.9576</td>\n",
       "      <td>18.7371</td>\n",
       "      <td>15.2986</td>\n",
       "      <td>5.7322</td>\n",
       "      <td>5.1244</td>\n",
       "      <td>9.8225</td>\n",
       "      <td>14.0315</td>\n",
       "      <td>-23.6064</td>\n",
       "      <td>-1.3403</td>\n",
       "      <td>-2.5577</td>\n",
       "      <td>6.3582</td>\n",
       "      <td>-5.4557</td>\n",
       "      <td>5.6063</td>\n",
       "      <td>7.0054</td>\n",
       "      <td>5.0171</td>\n",
       "      <td>-5.0055</td>\n",
       "      <td>28.9502</td>\n",
       "      <td>1.2297</td>\n",
       "      <td>4.4918</td>\n",
       "      <td>19.5568</td>\n",
       "      <td>20.8357</td>\n",
       "      <td>19.2136</td>\n",
       "      <td>17.6422</td>\n",
       "      <td>17.9836</td>\n",
       "      <td>4.0395</td>\n",
       "      <td>14.0761</td>\n",
       "      <td>-5.7878</td>\n",
       "      <td>16.3870</td>\n",
       "      <td>-14.1721</td>\n",
       "      <td>-13.0269</td>\n",
       "      <td>-2.5955</td>\n",
       "      <td>21.4526</td>\n",
       "      <td>15.6163</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>8.2110</td>\n",
       "      <td>-0.8553</td>\n",
       "      <td>-12.1682</td>\n",
       "      <td>6.7779</td>\n",
       "      <td>7.3895</td>\n",
       "      <td>10.5084</td>\n",
       "      <td>15.5057</td>\n",
       "      <td>-0.6812</td>\n",
       "      <td>5.8999</td>\n",
       "      <td>6.1825</td>\n",
       "      <td>3.1038</td>\n",
       "      <td>-1.6930</td>\n",
       "      <td>-18.8473</td>\n",
       "      <td>9.9358</td>\n",
       "      <td>25.3359</td>\n",
       "      <td>1.3647</td>\n",
       "      <td>11.8509</td>\n",
       "      <td>5.0357</td>\n",
       "      <td>6.4630</td>\n",
       "      <td>18.4008</td>\n",
       "      <td>14.3787</td>\n",
       "      <td>19.0369</td>\n",
       "      <td>-0.6364</td>\n",
       "      <td>6.9155</td>\n",
       "      <td>3.6763</td>\n",
       "      <td>3.1460</td>\n",
       "      <td>4.9442</td>\n",
       "      <td>-1.8289</td>\n",
       "      <td>1.3521</td>\n",
       "      <td>34.6265</td>\n",
       "      <td>-0.6869</td>\n",
       "      <td>-5.3781</td>\n",
       "      <td>20.5030</td>\n",
       "      <td>10.9614</td>\n",
       "      <td>4.9677</td>\n",
       "      <td>6.1408</td>\n",
       "      <td>2.2575</td>\n",
       "      <td>12.8757</td>\n",
       "      <td>14.2253</td>\n",
       "      <td>-1.2868</td>\n",
       "      <td>0.2212</td>\n",
       "      <td>16.8661</td>\n",
       "      <td>12.7663</td>\n",
       "      <td>1.2414</td>\n",
       "      <td>7.1304</td>\n",
       "      <td>7.4108</td>\n",
       "      <td>-6.3369</td>\n",
       "      <td>3.0760</td>\n",
       "      <td>24.9796</td>\n",
       "      <td>20.3410</td>\n",
       "      <td>5.3312</td>\n",
       "      <td>23.7116</td>\n",
       "      <td>2.4745</td>\n",
       "      <td>11.2013</td>\n",
       "      <td>17.8165</td>\n",
       "      <td>13.0057</td>\n",
       "      <td>9.5506</td>\n",
       "      <td>5.3589</td>\n",
       "      <td>13.2491</td>\n",
       "      <td>-3.3068</td>\n",
       "      <td>3.6998</td>\n",
       "      <td>2.5927</td>\n",
       "      <td>14.3025</td>\n",
       "      <td>8.1596</td>\n",
       "      <td>7.9609</td>\n",
       "      <td>18.3343</td>\n",
       "      <td>4.3086</td>\n",
       "      <td>1.3546</td>\n",
       "      <td>12.4158</td>\n",
       "      <td>-5.3985</td>\n",
       "      <td>16.3683</td>\n",
       "      <td>10.4522</td>\n",
       "      <td>35.4923</td>\n",
       "      <td>5.5477</td>\n",
       "      <td>7.4244</td>\n",
       "      <td>12.5459</td>\n",
       "      <td>-6.7840</td>\n",
       "      <td>31.1895</td>\n",
       "      <td>2.6529</td>\n",
       "      <td>-11.1867</td>\n",
       "      <td>9.8865</td>\n",
       "      <td>5.4730</td>\n",
       "      <td>-5.3880</td>\n",
       "      <td>-0.4698</td>\n",
       "      <td>24.4025</td>\n",
       "      <td>-5.4493</td>\n",
       "      <td>11.3529</td>\n",
       "      <td>7.7075</td>\n",
       "      <td>-5.0491</td>\n",
       "      <td>13.0756</td>\n",
       "      <td>15.8271</td>\n",
       "      <td>3.3580</td>\n",
       "      <td>-14.3371</td>\n",
       "      <td>10.4421</td>\n",
       "      <td>7.6530</td>\n",
       "      <td>9.4585</td>\n",
       "      <td>22.7783</td>\n",
       "      <td>-4.0305</td>\n",
       "      <td>4.2233</td>\n",
       "      <td>-6.3906</td>\n",
       "      <td>13.5058</td>\n",
       "      <td>-0.4594</td>\n",
       "      <td>6.1415</td>\n",
       "      <td>13.2305</td>\n",
       "      <td>3.9901</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>18.0249</td>\n",
       "      <td>-1.7939</td>\n",
       "      <td>2.1661</td>\n",
       "      <td>8.5326</td>\n",
       "      <td>16.6660</td>\n",
       "      <td>-17.8661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>train_199996</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9149</td>\n",
       "      <td>-2.4484</td>\n",
       "      <td>16.7052</td>\n",
       "      <td>6.6345</td>\n",
       "      <td>8.3096</td>\n",
       "      <td>-10.5628</td>\n",
       "      <td>5.8802</td>\n",
       "      <td>21.5940</td>\n",
       "      <td>-3.6797</td>\n",
       "      <td>6.0019</td>\n",
       "      <td>6.5576</td>\n",
       "      <td>-11.8776</td>\n",
       "      <td>14.4131</td>\n",
       "      <td>3.3087</td>\n",
       "      <td>3.5800</td>\n",
       "      <td>14.1597</td>\n",
       "      <td>7.5191</td>\n",
       "      <td>-8.8715</td>\n",
       "      <td>17.9467</td>\n",
       "      <td>17.0237</td>\n",
       "      <td>6.6459</td>\n",
       "      <td>18.2345</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>2.2532</td>\n",
       "      <td>15.4977</td>\n",
       "      <td>13.3282</td>\n",
       "      <td>5.2281</td>\n",
       "      <td>-3.7424</td>\n",
       "      <td>5.5144</td>\n",
       "      <td>5.7148</td>\n",
       "      <td>-13.7470</td>\n",
       "      <td>7.4369</td>\n",
       "      <td>1.3041</td>\n",
       "      <td>12.7552</td>\n",
       "      <td>12.5362</td>\n",
       "      <td>-1.1002</td>\n",
       "      <td>2.4370</td>\n",
       "      <td>6.2631</td>\n",
       "      <td>14.8565</td>\n",
       "      <td>-2.9862</td>\n",
       "      <td>-7.8820</td>\n",
       "      <td>7.1320</td>\n",
       "      <td>11.8869</td>\n",
       "      <td>11.4218</td>\n",
       "      <td>8.9282</td>\n",
       "      <td>-27.2007</td>\n",
       "      <td>14.5962</td>\n",
       "      <td>-19.8502</td>\n",
       "      <td>26.0775</td>\n",
       "      <td>24.3915</td>\n",
       "      <td>12.6910</td>\n",
       "      <td>10.2453</td>\n",
       "      <td>6.8173</td>\n",
       "      <td>4.5666</td>\n",
       "      <td>-9.5685</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.9534</td>\n",
       "      <td>7.3660</td>\n",
       "      <td>4.7038</td>\n",
       "      <td>9.4559</td>\n",
       "      <td>6.0037</td>\n",
       "      <td>-10.8728</td>\n",
       "      <td>0.7859</td>\n",
       "      <td>4.7000</td>\n",
       "      <td>7.8077</td>\n",
       "      <td>-1.7926</td>\n",
       "      <td>6.1534</td>\n",
       "      <td>12.9087</td>\n",
       "      <td>5.0398</td>\n",
       "      <td>-0.4247</td>\n",
       "      <td>22.6256</td>\n",
       "      <td>0.7166</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>13.5821</td>\n",
       "      <td>20.3267</td>\n",
       "      <td>25.5380</td>\n",
       "      <td>14.0155</td>\n",
       "      <td>17.3326</td>\n",
       "      <td>4.2046</td>\n",
       "      <td>14.0195</td>\n",
       "      <td>11.4812</td>\n",
       "      <td>17.9954</td>\n",
       "      <td>-18.3549</td>\n",
       "      <td>-3.4537</td>\n",
       "      <td>1.1233</td>\n",
       "      <td>22.3135</td>\n",
       "      <td>1.9795</td>\n",
       "      <td>16.0239</td>\n",
       "      <td>4.7492</td>\n",
       "      <td>0.2446</td>\n",
       "      <td>-39.6406</td>\n",
       "      <td>6.9473</td>\n",
       "      <td>9.9392</td>\n",
       "      <td>11.1977</td>\n",
       "      <td>14.1006</td>\n",
       "      <td>-0.8012</td>\n",
       "      <td>18.8214</td>\n",
       "      <td>32.9827</td>\n",
       "      <td>1.7989</td>\n",
       "      <td>-0.2476</td>\n",
       "      <td>-15.5294</td>\n",
       "      <td>9.5501</td>\n",
       "      <td>11.8548</td>\n",
       "      <td>1.5127</td>\n",
       "      <td>11.3998</td>\n",
       "      <td>4.2304</td>\n",
       "      <td>6.6777</td>\n",
       "      <td>11.3434</td>\n",
       "      <td>14.2993</td>\n",
       "      <td>13.1205</td>\n",
       "      <td>13.3224</td>\n",
       "      <td>7.3143</td>\n",
       "      <td>3.6817</td>\n",
       "      <td>9.7780</td>\n",
       "      <td>4.0491</td>\n",
       "      <td>2.7221</td>\n",
       "      <td>4.4344</td>\n",
       "      <td>3.7648</td>\n",
       "      <td>2.1927</td>\n",
       "      <td>-2.9197</td>\n",
       "      <td>23.0679</td>\n",
       "      <td>12.2112</td>\n",
       "      <td>3.7517</td>\n",
       "      <td>6.7907</td>\n",
       "      <td>6.5622</td>\n",
       "      <td>13.0283</td>\n",
       "      <td>12.2389</td>\n",
       "      <td>4.0627</td>\n",
       "      <td>-1.2406</td>\n",
       "      <td>13.9757</td>\n",
       "      <td>12.6133</td>\n",
       "      <td>0.6524</td>\n",
       "      <td>8.3929</td>\n",
       "      <td>6.9125</td>\n",
       "      <td>-6.0942</td>\n",
       "      <td>-6.3209</td>\n",
       "      <td>38.8105</td>\n",
       "      <td>17.6153</td>\n",
       "      <td>-2.9070</td>\n",
       "      <td>0.8270</td>\n",
       "      <td>2.0615</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>6.5953</td>\n",
       "      <td>17.2099</td>\n",
       "      <td>9.3960</td>\n",
       "      <td>9.9801</td>\n",
       "      <td>3.7881</td>\n",
       "      <td>2.9866</td>\n",
       "      <td>3.8695</td>\n",
       "      <td>17.8068</td>\n",
       "      <td>18.7807</td>\n",
       "      <td>9.4546</td>\n",
       "      <td>4.4657</td>\n",
       "      <td>17.8085</td>\n",
       "      <td>13.3077</td>\n",
       "      <td>-1.3209</td>\n",
       "      <td>12.7288</td>\n",
       "      <td>-12.3625</td>\n",
       "      <td>15.3500</td>\n",
       "      <td>11.1798</td>\n",
       "      <td>35.1445</td>\n",
       "      <td>5.5375</td>\n",
       "      <td>5.6397</td>\n",
       "      <td>17.0598</td>\n",
       "      <td>-9.7142</td>\n",
       "      <td>15.5117</td>\n",
       "      <td>3.3696</td>\n",
       "      <td>-17.1855</td>\n",
       "      <td>2.8292</td>\n",
       "      <td>5.2606</td>\n",
       "      <td>2.6836</td>\n",
       "      <td>5.8767</td>\n",
       "      <td>25.1262</td>\n",
       "      <td>7.3478</td>\n",
       "      <td>27.1264</td>\n",
       "      <td>11.8542</td>\n",
       "      <td>9.7999</td>\n",
       "      <td>11.1395</td>\n",
       "      <td>-3.2870</td>\n",
       "      <td>0.4285</td>\n",
       "      <td>2.5058</td>\n",
       "      <td>10.0339</td>\n",
       "      <td>9.1610</td>\n",
       "      <td>9.4318</td>\n",
       "      <td>13.4913</td>\n",
       "      <td>4.6247</td>\n",
       "      <td>6.2906</td>\n",
       "      <td>-17.8522</td>\n",
       "      <td>18.6751</td>\n",
       "      <td>-0.1162</td>\n",
       "      <td>4.9611</td>\n",
       "      <td>4.6549</td>\n",
       "      <td>0.6998</td>\n",
       "      <td>1.8341</td>\n",
       "      <td>22.2717</td>\n",
       "      <td>1.7337</td>\n",
       "      <td>-2.1651</td>\n",
       "      <td>6.7419</td>\n",
       "      <td>15.9054</td>\n",
       "      <td>0.3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>train_199997</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2232</td>\n",
       "      <td>-5.0518</td>\n",
       "      <td>10.5127</td>\n",
       "      <td>5.6456</td>\n",
       "      <td>9.3410</td>\n",
       "      <td>-5.4086</td>\n",
       "      <td>4.5555</td>\n",
       "      <td>21.5571</td>\n",
       "      <td>0.1202</td>\n",
       "      <td>6.1629</td>\n",
       "      <td>4.4004</td>\n",
       "      <td>-0.4651</td>\n",
       "      <td>13.8775</td>\n",
       "      <td>9.7414</td>\n",
       "      <td>10.9044</td>\n",
       "      <td>14.5597</td>\n",
       "      <td>9.6214</td>\n",
       "      <td>-1.6429</td>\n",
       "      <td>23.1127</td>\n",
       "      <td>12.1517</td>\n",
       "      <td>16.2577</td>\n",
       "      <td>3.1453</td>\n",
       "      <td>3.1008</td>\n",
       "      <td>2.1497</td>\n",
       "      <td>10.2715</td>\n",
       "      <td>13.5637</td>\n",
       "      <td>4.9473</td>\n",
       "      <td>-0.9905</td>\n",
       "      <td>6.2801</td>\n",
       "      <td>9.4902</td>\n",
       "      <td>-12.8549</td>\n",
       "      <td>11.0403</td>\n",
       "      <td>1.4306</td>\n",
       "      <td>13.8533</td>\n",
       "      <td>11.7484</td>\n",
       "      <td>6.8969</td>\n",
       "      <td>6.4162</td>\n",
       "      <td>3.4246</td>\n",
       "      <td>12.1170</td>\n",
       "      <td>3.4096</td>\n",
       "      <td>-8.8763</td>\n",
       "      <td>9.5230</td>\n",
       "      <td>11.2566</td>\n",
       "      <td>11.4025</td>\n",
       "      <td>11.8492</td>\n",
       "      <td>-49.5007</td>\n",
       "      <td>7.4376</td>\n",
       "      <td>-21.2946</td>\n",
       "      <td>16.5701</td>\n",
       "      <td>15.9192</td>\n",
       "      <td>11.4688</td>\n",
       "      <td>16.3800</td>\n",
       "      <td>-5.7152</td>\n",
       "      <td>6.0771</td>\n",
       "      <td>7.5194</td>\n",
       "      <td>9.6364</td>\n",
       "      <td>15.3166</td>\n",
       "      <td>5.4830</td>\n",
       "      <td>0.6006</td>\n",
       "      <td>9.5466</td>\n",
       "      <td>22.0960</td>\n",
       "      <td>-6.7813</td>\n",
       "      <td>3.6870</td>\n",
       "      <td>-4.0387</td>\n",
       "      <td>5.8101</td>\n",
       "      <td>3.7793</td>\n",
       "      <td>5.7782</td>\n",
       "      <td>14.5730</td>\n",
       "      <td>5.0075</td>\n",
       "      <td>-1.0104</td>\n",
       "      <td>25.6050</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>3.3822</td>\n",
       "      <td>13.4685</td>\n",
       "      <td>10.8834</td>\n",
       "      <td>9.2657</td>\n",
       "      <td>-4.1948</td>\n",
       "      <td>12.1229</td>\n",
       "      <td>7.5949</td>\n",
       "      <td>11.9158</td>\n",
       "      <td>11.9537</td>\n",
       "      <td>16.9399</td>\n",
       "      <td>-2.2643</td>\n",
       "      <td>-3.3658</td>\n",
       "      <td>6.4020</td>\n",
       "      <td>18.2095</td>\n",
       "      <td>17.4710</td>\n",
       "      <td>6.3349</td>\n",
       "      <td>7.4740</td>\n",
       "      <td>4.8024</td>\n",
       "      <td>-0.3345</td>\n",
       "      <td>7.0295</td>\n",
       "      <td>16.5425</td>\n",
       "      <td>10.5645</td>\n",
       "      <td>12.7330</td>\n",
       "      <td>-0.9946</td>\n",
       "      <td>23.7210</td>\n",
       "      <td>11.2390</td>\n",
       "      <td>1.0012</td>\n",
       "      <td>-1.1083</td>\n",
       "      <td>-8.0574</td>\n",
       "      <td>10.0606</td>\n",
       "      <td>25.2535</td>\n",
       "      <td>1.8019</td>\n",
       "      <td>10.4973</td>\n",
       "      <td>4.2183</td>\n",
       "      <td>9.1158</td>\n",
       "      <td>10.1525</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>15.2503</td>\n",
       "      <td>3.4797</td>\n",
       "      <td>8.7901</td>\n",
       "      <td>2.9000</td>\n",
       "      <td>0.6471</td>\n",
       "      <td>2.3316</td>\n",
       "      <td>1.5084</td>\n",
       "      <td>0.2888</td>\n",
       "      <td>43.0307</td>\n",
       "      <td>-4.4543</td>\n",
       "      <td>3.2765</td>\n",
       "      <td>28.2664</td>\n",
       "      <td>12.1189</td>\n",
       "      <td>3.1526</td>\n",
       "      <td>14.2214</td>\n",
       "      <td>3.3878</td>\n",
       "      <td>13.2410</td>\n",
       "      <td>12.9788</td>\n",
       "      <td>4.5766</td>\n",
       "      <td>-4.8512</td>\n",
       "      <td>16.6344</td>\n",
       "      <td>12.3827</td>\n",
       "      <td>0.5293</td>\n",
       "      <td>8.0588</td>\n",
       "      <td>7.1081</td>\n",
       "      <td>-9.2317</td>\n",
       "      <td>-11.9277</td>\n",
       "      <td>20.5706</td>\n",
       "      <td>22.5568</td>\n",
       "      <td>3.0665</td>\n",
       "      <td>1.0527</td>\n",
       "      <td>7.4011</td>\n",
       "      <td>4.3367</td>\n",
       "      <td>1.4242</td>\n",
       "      <td>11.3654</td>\n",
       "      <td>9.1812</td>\n",
       "      <td>2.7627</td>\n",
       "      <td>12.2434</td>\n",
       "      <td>-0.2420</td>\n",
       "      <td>4.1575</td>\n",
       "      <td>4.7996</td>\n",
       "      <td>20.6307</td>\n",
       "      <td>10.2890</td>\n",
       "      <td>5.6890</td>\n",
       "      <td>13.4601</td>\n",
       "      <td>-0.9774</td>\n",
       "      <td>2.3728</td>\n",
       "      <td>11.7245</td>\n",
       "      <td>-9.6385</td>\n",
       "      <td>17.3101</td>\n",
       "      <td>14.0422</td>\n",
       "      <td>19.9293</td>\n",
       "      <td>5.3427</td>\n",
       "      <td>5.4776</td>\n",
       "      <td>13.1202</td>\n",
       "      <td>5.3500</td>\n",
       "      <td>31.7346</td>\n",
       "      <td>3.1693</td>\n",
       "      <td>-19.4779</td>\n",
       "      <td>6.8053</td>\n",
       "      <td>5.6281</td>\n",
       "      <td>-0.8774</td>\n",
       "      <td>-8.9508</td>\n",
       "      <td>17.4931</td>\n",
       "      <td>-1.6530</td>\n",
       "      <td>32.0032</td>\n",
       "      <td>12.5749</td>\n",
       "      <td>5.8756</td>\n",
       "      <td>8.8059</td>\n",
       "      <td>-10.6367</td>\n",
       "      <td>5.4401</td>\n",
       "      <td>-12.7967</td>\n",
       "      <td>8.7990</td>\n",
       "      <td>0.7021</td>\n",
       "      <td>14.9744</td>\n",
       "      <td>18.9211</td>\n",
       "      <td>0.3016</td>\n",
       "      <td>11.2869</td>\n",
       "      <td>-6.3741</td>\n",
       "      <td>12.9726</td>\n",
       "      <td>2.3425</td>\n",
       "      <td>4.0651</td>\n",
       "      <td>5.4414</td>\n",
       "      <td>3.1032</td>\n",
       "      <td>4.8793</td>\n",
       "      <td>23.5311</td>\n",
       "      <td>-1.5736</td>\n",
       "      <td>1.2832</td>\n",
       "      <td>8.7155</td>\n",
       "      <td>13.8329</td>\n",
       "      <td>4.1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>train_199998</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7148</td>\n",
       "      <td>-8.6098</td>\n",
       "      <td>13.6104</td>\n",
       "      <td>5.7930</td>\n",
       "      <td>12.5173</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>6.0479</td>\n",
       "      <td>17.0152</td>\n",
       "      <td>-2.1926</td>\n",
       "      <td>8.7542</td>\n",
       "      <td>1.4245</td>\n",
       "      <td>0.7086</td>\n",
       "      <td>14.2110</td>\n",
       "      <td>6.5641</td>\n",
       "      <td>7.6177</td>\n",
       "      <td>13.8771</td>\n",
       "      <td>9.0479</td>\n",
       "      <td>-11.8164</td>\n",
       "      <td>14.0831</td>\n",
       "      <td>-2.0345</td>\n",
       "      <td>18.3863</td>\n",
       "      <td>3.0911</td>\n",
       "      <td>5.5803</td>\n",
       "      <td>3.7091</td>\n",
       "      <td>12.8219</td>\n",
       "      <td>13.8866</td>\n",
       "      <td>-3.3859</td>\n",
       "      <td>-0.4440</td>\n",
       "      <td>5.4817</td>\n",
       "      <td>4.0902</td>\n",
       "      <td>-7.7085</td>\n",
       "      <td>10.3952</td>\n",
       "      <td>2.5739</td>\n",
       "      <td>17.8529</td>\n",
       "      <td>11.3433</td>\n",
       "      <td>5.0534</td>\n",
       "      <td>-3.0055</td>\n",
       "      <td>3.9433</td>\n",
       "      <td>11.0759</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>-11.7669</td>\n",
       "      <td>11.8626</td>\n",
       "      <td>10.7766</td>\n",
       "      <td>11.6900</td>\n",
       "      <td>12.9929</td>\n",
       "      <td>-42.9704</td>\n",
       "      <td>12.7881</td>\n",
       "      <td>4.4044</td>\n",
       "      <td>27.0880</td>\n",
       "      <td>14.0471</td>\n",
       "      <td>13.4318</td>\n",
       "      <td>9.4325</td>\n",
       "      <td>1.0213</td>\n",
       "      <td>6.2404</td>\n",
       "      <td>-8.1836</td>\n",
       "      <td>4.1057</td>\n",
       "      <td>10.7941</td>\n",
       "      <td>5.9704</td>\n",
       "      <td>-4.6315</td>\n",
       "      <td>9.9272</td>\n",
       "      <td>14.4322</td>\n",
       "      <td>-13.8557</td>\n",
       "      <td>-1.8803</td>\n",
       "      <td>1.8243</td>\n",
       "      <td>4.8059</td>\n",
       "      <td>-1.6255</td>\n",
       "      <td>5.1595</td>\n",
       "      <td>-2.8395</td>\n",
       "      <td>5.0116</td>\n",
       "      <td>2.4464</td>\n",
       "      <td>24.0896</td>\n",
       "      <td>0.8953</td>\n",
       "      <td>-2.6184</td>\n",
       "      <td>27.7040</td>\n",
       "      <td>43.5092</td>\n",
       "      <td>16.4079</td>\n",
       "      <td>14.4559</td>\n",
       "      <td>27.7355</td>\n",
       "      <td>5.5360</td>\n",
       "      <td>16.7484</td>\n",
       "      <td>9.6956</td>\n",
       "      <td>21.4391</td>\n",
       "      <td>-5.1839</td>\n",
       "      <td>6.8296</td>\n",
       "      <td>-9.0318</td>\n",
       "      <td>24.2122</td>\n",
       "      <td>-7.5779</td>\n",
       "      <td>5.6786</td>\n",
       "      <td>13.1278</td>\n",
       "      <td>7.0086</td>\n",
       "      <td>-32.3247</td>\n",
       "      <td>7.0141</td>\n",
       "      <td>6.9451</td>\n",
       "      <td>10.0272</td>\n",
       "      <td>10.0716</td>\n",
       "      <td>-0.3385</td>\n",
       "      <td>19.4605</td>\n",
       "      <td>26.9480</td>\n",
       "      <td>1.7079</td>\n",
       "      <td>-4.8882</td>\n",
       "      <td>-2.3891</td>\n",
       "      <td>24.6626</td>\n",
       "      <td>19.7783</td>\n",
       "      <td>1.5780</td>\n",
       "      <td>14.3962</td>\n",
       "      <td>4.8206</td>\n",
       "      <td>12.2354</td>\n",
       "      <td>33.9267</td>\n",
       "      <td>14.2625</td>\n",
       "      <td>26.2407</td>\n",
       "      <td>2.9091</td>\n",
       "      <td>6.4540</td>\n",
       "      <td>5.3290</td>\n",
       "      <td>10.6131</td>\n",
       "      <td>3.4212</td>\n",
       "      <td>-1.8915</td>\n",
       "      <td>2.1376</td>\n",
       "      <td>46.4915</td>\n",
       "      <td>1.0591</td>\n",
       "      <td>3.3543</td>\n",
       "      <td>18.1250</td>\n",
       "      <td>10.0102</td>\n",
       "      <td>9.3483</td>\n",
       "      <td>11.0467</td>\n",
       "      <td>2.3866</td>\n",
       "      <td>12.2352</td>\n",
       "      <td>13.5462</td>\n",
       "      <td>3.0043</td>\n",
       "      <td>5.3751</td>\n",
       "      <td>17.1567</td>\n",
       "      <td>11.6873</td>\n",
       "      <td>0.6677</td>\n",
       "      <td>8.3511</td>\n",
       "      <td>6.5834</td>\n",
       "      <td>1.6146</td>\n",
       "      <td>4.8462</td>\n",
       "      <td>15.2331</td>\n",
       "      <td>3.8390</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>2.2357</td>\n",
       "      <td>14.8203</td>\n",
       "      <td>5.8648</td>\n",
       "      <td>8.7190</td>\n",
       "      <td>15.1468</td>\n",
       "      <td>9.9930</td>\n",
       "      <td>10.4543</td>\n",
       "      <td>10.9535</td>\n",
       "      <td>-10.3405</td>\n",
       "      <td>3.6463</td>\n",
       "      <td>-16.8622</td>\n",
       "      <td>18.8580</td>\n",
       "      <td>8.2192</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>16.7224</td>\n",
       "      <td>8.8882</td>\n",
       "      <td>-3.2567</td>\n",
       "      <td>12.9142</td>\n",
       "      <td>-8.5421</td>\n",
       "      <td>15.9319</td>\n",
       "      <td>5.8348</td>\n",
       "      <td>40.3378</td>\n",
       "      <td>5.5357</td>\n",
       "      <td>4.6151</td>\n",
       "      <td>8.5910</td>\n",
       "      <td>-12.6998</td>\n",
       "      <td>25.8578</td>\n",
       "      <td>2.2346</td>\n",
       "      <td>-6.4988</td>\n",
       "      <td>2.6702</td>\n",
       "      <td>5.3868</td>\n",
       "      <td>-7.1875</td>\n",
       "      <td>8.1477</td>\n",
       "      <td>22.4362</td>\n",
       "      <td>-2.5914</td>\n",
       "      <td>8.8704</td>\n",
       "      <td>11.6621</td>\n",
       "      <td>7.4904</td>\n",
       "      <td>8.1808</td>\n",
       "      <td>-11.4177</td>\n",
       "      <td>2.8379</td>\n",
       "      <td>3.8748</td>\n",
       "      <td>8.7410</td>\n",
       "      <td>8.9998</td>\n",
       "      <td>16.4058</td>\n",
       "      <td>11.3244</td>\n",
       "      <td>-2.1751</td>\n",
       "      <td>12.4735</td>\n",
       "      <td>-18.3932</td>\n",
       "      <td>12.6337</td>\n",
       "      <td>0.3243</td>\n",
       "      <td>2.6840</td>\n",
       "      <td>8.6587</td>\n",
       "      <td>2.7337</td>\n",
       "      <td>11.1178</td>\n",
       "      <td>20.4158</td>\n",
       "      <td>-0.0786</td>\n",
       "      <td>6.7980</td>\n",
       "      <td>10.0342</td>\n",
       "      <td>15.5289</td>\n",
       "      <td>-13.9001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>train_199999</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8762</td>\n",
       "      <td>-5.7105</td>\n",
       "      <td>12.1183</td>\n",
       "      <td>8.0328</td>\n",
       "      <td>11.5577</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>5.2839</td>\n",
       "      <td>15.2058</td>\n",
       "      <td>-0.4541</td>\n",
       "      <td>9.3688</td>\n",
       "      <td>-7.3826</td>\n",
       "      <td>-8.7049</td>\n",
       "      <td>14.2486</td>\n",
       "      <td>15.0849</td>\n",
       "      <td>5.2313</td>\n",
       "      <td>14.3572</td>\n",
       "      <td>12.5523</td>\n",
       "      <td>-6.5066</td>\n",
       "      <td>11.3592</td>\n",
       "      <td>11.4779</td>\n",
       "      <td>15.4997</td>\n",
       "      <td>3.8474</td>\n",
       "      <td>2.4381</td>\n",
       "      <td>2.8295</td>\n",
       "      <td>10.6681</td>\n",
       "      <td>13.7167</td>\n",
       "      <td>-7.7771</td>\n",
       "      <td>-2.7798</td>\n",
       "      <td>6.2885</td>\n",
       "      <td>6.0089</td>\n",
       "      <td>2.1547</td>\n",
       "      <td>10.8181</td>\n",
       "      <td>-0.2712</td>\n",
       "      <td>12.5254</td>\n",
       "      <td>11.6304</td>\n",
       "      <td>-1.4949</td>\n",
       "      <td>7.9509</td>\n",
       "      <td>2.2480</td>\n",
       "      <td>8.1459</td>\n",
       "      <td>0.7928</td>\n",
       "      <td>-7.9028</td>\n",
       "      <td>7.4223</td>\n",
       "      <td>11.4249</td>\n",
       "      <td>11.9103</td>\n",
       "      <td>8.7002</td>\n",
       "      <td>-6.6883</td>\n",
       "      <td>10.5219</td>\n",
       "      <td>-25.9933</td>\n",
       "      <td>11.6241</td>\n",
       "      <td>13.4670</td>\n",
       "      <td>12.3563</td>\n",
       "      <td>3.4031</td>\n",
       "      <td>-12.9247</td>\n",
       "      <td>6.2607</td>\n",
       "      <td>11.8525</td>\n",
       "      <td>8.8581</td>\n",
       "      <td>20.6438</td>\n",
       "      <td>6.5641</td>\n",
       "      <td>0.5322</td>\n",
       "      <td>10.0740</td>\n",
       "      <td>11.2477</td>\n",
       "      <td>-19.5169</td>\n",
       "      <td>-1.6499</td>\n",
       "      <td>5.3036</td>\n",
       "      <td>5.6244</td>\n",
       "      <td>1.2976</td>\n",
       "      <td>5.4680</td>\n",
       "      <td>10.3979</td>\n",
       "      <td>5.0209</td>\n",
       "      <td>-6.9248</td>\n",
       "      <td>32.4865</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>4.3880</td>\n",
       "      <td>16.1819</td>\n",
       "      <td>11.5080</td>\n",
       "      <td>11.9092</td>\n",
       "      <td>6.3494</td>\n",
       "      <td>23.0598</td>\n",
       "      <td>2.4466</td>\n",
       "      <td>15.6721</td>\n",
       "      <td>9.3809</td>\n",
       "      <td>14.7593</td>\n",
       "      <td>-12.8156</td>\n",
       "      <td>3.4928</td>\n",
       "      <td>-3.1634</td>\n",
       "      <td>21.5742</td>\n",
       "      <td>9.7015</td>\n",
       "      <td>22.4258</td>\n",
       "      <td>7.1213</td>\n",
       "      <td>2.4050</td>\n",
       "      <td>-3.1107</td>\n",
       "      <td>7.1529</td>\n",
       "      <td>16.2315</td>\n",
       "      <td>11.5051</td>\n",
       "      <td>16.5967</td>\n",
       "      <td>0.6444</td>\n",
       "      <td>21.0773</td>\n",
       "      <td>25.7270</td>\n",
       "      <td>2.4916</td>\n",
       "      <td>-3.0062</td>\n",
       "      <td>0.9636</td>\n",
       "      <td>13.4966</td>\n",
       "      <td>31.3629</td>\n",
       "      <td>1.5517</td>\n",
       "      <td>12.1898</td>\n",
       "      <td>3.5216</td>\n",
       "      <td>6.8915</td>\n",
       "      <td>9.0475</td>\n",
       "      <td>14.1260</td>\n",
       "      <td>15.9937</td>\n",
       "      <td>6.7106</td>\n",
       "      <td>8.1964</td>\n",
       "      <td>6.5520</td>\n",
       "      <td>6.6900</td>\n",
       "      <td>2.4643</td>\n",
       "      <td>4.5841</td>\n",
       "      <td>0.7221</td>\n",
       "      <td>23.4020</td>\n",
       "      <td>-10.3157</td>\n",
       "      <td>6.4857</td>\n",
       "      <td>33.4300</td>\n",
       "      <td>12.2439</td>\n",
       "      <td>4.3416</td>\n",
       "      <td>10.3869</td>\n",
       "      <td>1.3913</td>\n",
       "      <td>12.7127</td>\n",
       "      <td>13.8530</td>\n",
       "      <td>4.6685</td>\n",
       "      <td>-1.6082</td>\n",
       "      <td>10.1564</td>\n",
       "      <td>11.7936</td>\n",
       "      <td>0.2316</td>\n",
       "      <td>8.1760</td>\n",
       "      <td>7.6166</td>\n",
       "      <td>-18.3865</td>\n",
       "      <td>-7.3542</td>\n",
       "      <td>32.6663</td>\n",
       "      <td>15.5464</td>\n",
       "      <td>-0.9083</td>\n",
       "      <td>1.0662</td>\n",
       "      <td>5.3922</td>\n",
       "      <td>9.7708</td>\n",
       "      <td>11.4687</td>\n",
       "      <td>8.6980</td>\n",
       "      <td>8.0106</td>\n",
       "      <td>13.1911</td>\n",
       "      <td>12.3484</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>3.8811</td>\n",
       "      <td>-9.4762</td>\n",
       "      <td>11.6612</td>\n",
       "      <td>13.1571</td>\n",
       "      <td>8.5043</td>\n",
       "      <td>17.0369</td>\n",
       "      <td>7.1124</td>\n",
       "      <td>-13.1967</td>\n",
       "      <td>13.9404</td>\n",
       "      <td>-8.3303</td>\n",
       "      <td>29.0140</td>\n",
       "      <td>9.6174</td>\n",
       "      <td>15.9041</td>\n",
       "      <td>5.3187</td>\n",
       "      <td>6.2987</td>\n",
       "      <td>13.0729</td>\n",
       "      <td>-4.2045</td>\n",
       "      <td>19.2141</td>\n",
       "      <td>3.2902</td>\n",
       "      <td>-1.2175</td>\n",
       "      <td>4.1583</td>\n",
       "      <td>5.7675</td>\n",
       "      <td>5.7719</td>\n",
       "      <td>-1.2139</td>\n",
       "      <td>21.8496</td>\n",
       "      <td>-3.5368</td>\n",
       "      <td>25.9094</td>\n",
       "      <td>11.7673</td>\n",
       "      <td>1.9765</td>\n",
       "      <td>15.9218</td>\n",
       "      <td>3.9350</td>\n",
       "      <td>4.3993</td>\n",
       "      <td>-10.3268</td>\n",
       "      <td>10.5200</td>\n",
       "      <td>9.9587</td>\n",
       "      <td>11.9242</td>\n",
       "      <td>7.0626</td>\n",
       "      <td>-6.5429</td>\n",
       "      <td>10.5947</td>\n",
       "      <td>-3.8827</td>\n",
       "      <td>16.3552</td>\n",
       "      <td>1.7535</td>\n",
       "      <td>8.9842</td>\n",
       "      <td>1.6893</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.3766</td>\n",
       "      <td>15.2101</td>\n",
       "      <td>-2.4907</td>\n",
       "      <td>-2.2342</td>\n",
       "      <td>8.1857</td>\n",
       "      <td>12.1284</td>\n",
       "      <td>0.1385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows  202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID_code  target    var_0   var_1    var_2   var_3    var_4  \\\n",
       "0            train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607   \n",
       "1            train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622   \n",
       "2            train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825   \n",
       "3            train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846   \n",
       "4            train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772   \n",
       "...              ...     ...      ...     ...      ...     ...      ...   \n",
       "199995  train_199995       0  11.4880 -0.4956   8.2622  3.5142  10.3404   \n",
       "199996  train_199996       0   4.9149 -2.4484  16.7052  6.6345   8.3096   \n",
       "199997  train_199997       0  11.2232 -5.0518  10.5127  5.6456   9.3410   \n",
       "199998  train_199998       0   9.7148 -8.6098  13.6104  5.7930  12.5173   \n",
       "199999  train_199999       0  10.8762 -5.7105  12.1183  8.0328  11.5577   \n",
       "\n",
       "          var_5   var_6    var_7   var_8   var_9  var_10   var_11   var_12  \\\n",
       "0       -9.2834  5.1187  18.6266 -4.9200  5.7470  2.9252   3.1821  14.0137   \n",
       "1        7.0433  5.6208  16.5338  3.1468  8.0851 -0.4032   8.0585  14.0239   \n",
       "2       -9.0837  6.9427  14.6155 -4.9193  5.9525 -0.3249 -11.2648  14.1929   \n",
       "3       -1.8361  5.8428  14.9250 -5.8609  8.2450  2.3061   2.8102  13.8463   \n",
       "4        2.4486  5.9405  19.2514  6.2654  7.6784 -9.4458 -12.1419  13.8481   \n",
       "...         ...     ...      ...     ...     ...     ...      ...      ...   \n",
       "199995  11.6081  5.6709  15.1516 -0.6209  5.6669  3.7574  -9.5348  13.9860   \n",
       "199996 -10.5628  5.8802  21.5940 -3.6797  6.0019  6.5576 -11.8776  14.4131   \n",
       "199997  -5.4086  4.5555  21.5571  0.1202  6.1629  4.4004  -0.4651  13.8775   \n",
       "199998   0.5339  6.0479  17.0152 -2.1926  8.7542  1.4245   0.7086  14.2110   \n",
       "199999   0.3488  5.2839  15.2058 -0.4541  9.3688 -7.3826  -8.7049  14.2486   \n",
       "\n",
       "         var_13   var_14   var_15   var_16   var_17   var_18   var_19  \\\n",
       "0        0.5745   8.7989  14.5691   5.7487  -7.2393   4.2840  30.7133   \n",
       "1        8.4135   5.4345  13.7003  13.8275 -15.5849   7.8000  28.5708   \n",
       "2        7.3124   7.5244  14.6472   7.6782  -1.7395   4.7011  20.4775   \n",
       "3       11.9704   6.4569  14.8372  10.7430  -0.4299  15.9426  13.7257   \n",
       "4        7.8895   7.7894  15.0553   8.4871  -3.0680   6.5263  11.3152   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   5.2982   8.2705  14.1527   7.4540  -5.0105  12.0465   8.6349   \n",
       "199996   3.3087   3.5800  14.1597   7.5191  -8.8715  17.9467  17.0237   \n",
       "199997   9.7414  10.9044  14.5597   9.6214  -1.6429  23.1127  12.1517   \n",
       "199998   6.5641   7.6177  13.8771   9.0479 -11.8164  14.0831  -2.0345   \n",
       "199999  15.0849   5.2313  14.3572  12.5523  -6.5066  11.3592  11.4779   \n",
       "\n",
       "         var_20   var_21   var_22  var_23   var_24   var_25   var_26  var_27  \\\n",
       "0       10.5350  16.2191   2.5791  2.4716  14.3831  13.4325  -5.1488 -0.4073   \n",
       "1        3.4287   2.7407   8.5524  3.3716   6.9779  13.8910 -11.7684 -2.5586   \n",
       "2       17.7559  18.1377   1.2145  3.5137   5.6777  13.2177  -7.9940 -2.9029   \n",
       "3       20.3010  12.5579   6.8202  2.7229  12.1354  13.7367   0.8135 -0.9059   \n",
       "4       21.4246  18.9608  10.1102  2.7142  14.2080  13.5433   3.1736 -3.3423   \n",
       "...         ...      ...      ...     ...      ...      ...      ...     ...   \n",
       "199995   9.9137  25.1376   1.0914  3.2326   7.7802  13.9939   2.9085  0.1005   \n",
       "199996   6.6459  18.2345   0.8982  2.2532  15.4977  13.3282   5.2281 -3.7424   \n",
       "199997  16.2577   3.1453   3.1008  2.1497  10.2715  13.5637   4.9473 -0.9905   \n",
       "199998  18.3863   3.0911   5.5803  3.7091  12.8219  13.8866  -3.3859 -0.4440   \n",
       "199999  15.4997   3.8474   2.4381  2.8295  10.6681  13.7167  -7.7771 -2.7798   \n",
       "\n",
       "        var_28  var_29   var_30   var_31  var_32   var_33   var_34   var_35  \\\n",
       "0       4.9306  5.9965  -0.3085  12.9041 -3.8766  16.8911  11.1920  10.5785   \n",
       "1       5.0464  0.5481  -9.2987   7.8755  1.2859  19.3710  11.3702   0.7399   \n",
       "2       5.8463  6.1439 -11.1025  12.4858 -2.2871  19.0422  11.0449   4.1087   \n",
       "3       5.9070  2.8407 -15.2398  10.4407 -2.5731   6.1796  10.6093  -5.9158   \n",
       "4       5.9015  7.9352  -3.1582   9.4668 -0.0083  19.3239  12.4057   0.6329   \n",
       "...        ...     ...      ...      ...     ...      ...      ...      ...   \n",
       "199995  4.2369  7.5665  -9.2149   9.5746  1.4012   7.4211  11.0075   7.8080   \n",
       "199996  5.5144  5.7148 -13.7470   7.4369  1.3041  12.7552  12.5362  -1.1002   \n",
       "199997  6.2801  9.4902 -12.8549  11.0403  1.4306  13.8533  11.7484   6.8969   \n",
       "199998  5.4817  4.0902  -7.7085  10.3952  2.5739  17.8529  11.3433   5.0534   \n",
       "199999  6.2885  6.0089   2.1547  10.8181 -0.2712  12.5254  11.6304  -1.4949   \n",
       "\n",
       "        var_36  var_37   var_38  var_39   var_40   var_41   var_42   var_43  \\\n",
       "0       0.6764  7.8871   4.6667  3.8743  -5.2387   7.3746  11.5767  12.0446   \n",
       "1       2.7995  5.8434  10.8160  3.6783 -11.1147   1.8730   9.8775  11.7842   \n",
       "2       4.6974  6.9346  10.8917  0.9003 -13.5174   2.2439  11.5283  12.0406   \n",
       "3       8.1723  2.8521   9.1738  0.6665  -3.8294  -1.0370  11.7770  11.2834   \n",
       "4       2.7922  5.8184  19.3038  1.4450  -5.5963  14.0685  11.9171  11.5111   \n",
       "...        ...     ...      ...     ...      ...      ...      ...      ...   \n",
       "199995  4.5567  4.9861   9.7471  0.0722   5.9053   8.1743  10.8800  11.1665   \n",
       "199996  2.4370  6.2631  14.8565 -2.9862  -7.8820   7.1320  11.8869  11.4218   \n",
       "199997  6.4162  3.4246  12.1170  3.4096  -8.8763   9.5230  11.2566  11.4025   \n",
       "199998 -3.0055  3.9433  11.0759  1.2173 -11.7669  11.8626  10.7766  11.6900   \n",
       "199999  7.9509  2.2480   8.1459  0.7928  -7.9028   7.4223  11.4249  11.9103   \n",
       "\n",
       "         var_44   var_45   var_46   var_47   var_48   var_49   var_50  \\\n",
       "0       11.6418  -7.0170   5.9226 -14.2136  16.0283   5.3253  12.9194   \n",
       "1        1.2444 -47.3797   7.3718   0.1948  34.4014  25.7037  11.8343   \n",
       "2        4.1006  -7.9078  11.1405  -5.7864  20.7477   6.8874  12.9143   \n",
       "3        8.0485 -24.6840  12.7404 -35.1659   0.7613   8.3838  12.6832   \n",
       "4        6.9087 -65.4863  13.8657   0.0444  -0.1346  14.4268  13.3273   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   4.2600  -2.1296   8.7833 -15.5727  -8.4916  22.1905  12.4110   \n",
       "199996   8.9282 -27.2007  14.5962 -19.8502  26.0775  24.3915  12.6910   \n",
       "199997  11.8492 -49.5007   7.4376 -21.2946  16.5701  15.9192  11.4688   \n",
       "199998  12.9929 -42.9704  12.7881   4.4044  27.0880  14.0471  13.4318   \n",
       "199999   8.7002  -6.6883  10.5219 -25.9933  11.6241  13.4670  12.3563   \n",
       "\n",
       "         var_51   var_52  var_53   var_54   var_55   var_56  var_57  var_58  \\\n",
       "0       29.0460  -0.6940  5.1736  -0.7474  14.8322  11.2668  5.3822  2.0183   \n",
       "1       13.2256  -4.1083  6.6885  -8.0946  18.5995  19.3219  7.0118  1.9210   \n",
       "2       19.5856   0.7268  6.4059   9.3124   6.2846  15.6372  5.8200  1.1000   \n",
       "3        9.5503   1.7895  5.2091   8.0913  12.3972  14.4698  6.5850  3.3164   \n",
       "4       10.4857  -1.4367  5.7555  -8.5414  14.1482  16.9840  6.1812  1.9548   \n",
       "...         ...      ...     ...      ...      ...      ...     ...     ...   \n",
       "199995  15.1168   1.6041  6.1868  10.9576  18.7371  15.2986  5.7322  5.1244   \n",
       "199996  10.2453   6.8173  4.5666  -9.5685  18.4685  16.9534  7.3660  4.7038   \n",
       "199997  16.3800  -5.7152  6.0771   7.5194   9.6364  15.3166  5.4830  0.6006   \n",
       "199998   9.4325   1.0213  6.2404  -8.1836   4.1057  10.7941  5.9704 -4.6315   \n",
       "199999   3.4031 -12.9247  6.2607  11.8525   8.8581  20.6438  6.5641  0.5322   \n",
       "\n",
       "         var_59   var_60   var_61  var_62  var_63  var_64  var_65  var_66  \\\n",
       "0       10.1166  16.1828   4.9590  2.0771 -0.2154  8.6748  9.5319  5.8056   \n",
       "1        8.8682   8.0109  -7.2417  1.7944 -1.3147  8.1042  1.5365  5.4007   \n",
       "2        9.1854  12.5963 -10.3734  0.8748  5.8042  3.7163 -1.1016  7.3667   \n",
       "3        9.4638  15.7820 -25.0222  3.4418 -4.3923  8.6464  6.3072  5.6221   \n",
       "4        9.2048   8.6591 -27.7439 -0.4952 -1.7839  5.2670 -4.3205  6.9860   \n",
       "...         ...      ...      ...     ...     ...     ...     ...     ...   \n",
       "199995   9.8225  14.0315 -23.6064 -1.3403 -2.5577  6.3582 -5.4557  5.6063   \n",
       "199996   9.4559   6.0037 -10.8728  0.7859  4.7000  7.8077 -1.7926  6.1534   \n",
       "199997   9.5466  22.0960  -6.7813  3.6870 -4.0387  5.8101  3.7793  5.7782   \n",
       "199998   9.9272  14.4322 -13.8557 -1.8803  1.8243  4.8059 -1.6255  5.1595   \n",
       "199999  10.0740  11.2477 -19.5169 -1.6499  5.3036  5.6244  1.2976  5.4680   \n",
       "\n",
       "         var_67  var_68  var_69   var_70  var_71  var_72   var_73   var_74  \\\n",
       "0       22.4321  5.0109 -4.7010  21.6374  0.5663  5.1999   8.8600  43.1127   \n",
       "1        7.9344  5.0220  2.2302  40.5632  0.5134  3.1701  20.1068   7.7841   \n",
       "2        9.8565  5.0228 -5.7828   2.3612  0.8520  6.3577  12.1719  19.7312   \n",
       "3       23.6143  5.0220 -3.9989   4.0462  0.2500  1.2516  24.4187   4.5290   \n",
       "4        1.6184  5.0301 -3.2431  40.1236  0.7737 -0.7264   4.5886  -4.5346   \n",
       "...         ...     ...     ...      ...     ...     ...      ...      ...   \n",
       "199995   7.0054  5.0171 -5.0055  28.9502  1.2297  4.4918  19.5568  20.8357   \n",
       "199996  12.9087  5.0398 -0.4247  22.6256  0.7166  0.6533  13.5821  20.3267   \n",
       "199997  14.5730  5.0075 -1.0104  25.6050  0.2655  3.3822  13.4685  10.8834   \n",
       "199998  -2.8395  5.0116  2.4464  24.0896  0.8953 -2.6184  27.7040  43.5092   \n",
       "199999  10.3979  5.0209 -6.9248  32.4865  0.8271  4.3880  16.1819  11.5080   \n",
       "\n",
       "         var_75   var_76   var_77  var_78   var_79   var_80   var_81   var_82  \\\n",
       "0       18.3816  -2.3440  23.4104  6.5199  12.1983  13.6468  13.8372   1.3675   \n",
       "1        7.0529   3.2709  23.4822  5.5075  13.7814   2.5462  18.1782   0.3683   \n",
       "2       19.4465   4.5048  23.2378  6.3191  12.8046   7.4729  15.7811  13.3529   \n",
       "3       15.4235  11.6875  23.6273  4.0806  15.2733   0.7839  10.5404   1.6212   \n",
       "4       23.3521   1.0273  19.1600  7.1734  14.3937   2.9598  13.3317  -9.2587   \n",
       "...         ...      ...      ...     ...      ...      ...      ...      ...   \n",
       "199995  19.2136  17.6422  17.9836  4.0395  14.0761  -5.7878  16.3870 -14.1721   \n",
       "199996  25.5380  14.0155  17.3326  4.2046  14.0195  11.4812  17.9954 -18.3549   \n",
       "199997   9.2657  -4.1948  12.1229  7.5949  11.9158  11.9537  16.9399  -2.2643   \n",
       "199998  16.4079  14.4559  27.7355  5.5360  16.7484   9.6956  21.4391  -5.1839   \n",
       "199999  11.9092   6.3494  23.0598  2.4466  15.6721   9.3809  14.7593 -12.8156   \n",
       "\n",
       "         var_83  var_84   var_85   var_86   var_87   var_88   var_89   var_90  \\\n",
       "0        2.9423 -4.5213  21.4669   9.3225  16.4597   7.9984  -1.7069 -21.4494   \n",
       "1       -4.8210 -5.4850  13.7867 -13.5901  11.0993   7.9022  12.2301   0.4768   \n",
       "2       10.1852  5.4604  19.0773  -4.4577   9.5413  11.9052   2.1447 -22.4038   \n",
       "3       -5.2896  1.6027  17.9762  -2.3174  15.6298   4.5474   7.5509  -7.5866   \n",
       "4       -6.7075  7.8984  14.5265   7.0799  20.1670   8.0053   3.7954 -39.7997   \n",
       "...         ...     ...      ...      ...      ...      ...      ...      ...   \n",
       "199995 -13.0269 -2.5955  21.4526  15.6163   0.9845   8.2110  -0.8553 -12.1682   \n",
       "199996  -3.4537  1.1233  22.3135   1.9795  16.0239   4.7492   0.2446 -39.6406   \n",
       "199997  -3.3658  6.4020  18.2095  17.4710   6.3349   7.4740   4.8024  -0.3345   \n",
       "199998   6.8296 -9.0318  24.2122  -7.5779   5.6786  13.1278   7.0086 -32.3247   \n",
       "199999   3.4928 -3.1634  21.5742   9.7015  22.4258   7.1213   2.4050  -3.1107   \n",
       "\n",
       "        var_91   var_92   var_93   var_94  var_95   var_96   var_97  var_98  \\\n",
       "0       6.7806  11.0924   9.9913  14.8421  0.1812   8.9642  16.2572  2.1743   \n",
       "1       6.8852   8.0905  10.9631  11.7569 -1.2722  24.7876  26.6881  1.8944   \n",
       "2       7.0883  14.1613  10.5080  14.2621  0.2647  20.4031  17.0360  1.6981   \n",
       "3       7.0364  14.4027  10.7795   7.2887 -1.0930  11.3596  18.1486  2.8344   \n",
       "4       7.0065   9.3627  10.4316  14.0553  0.0213  14.7246  35.2988  1.6844   \n",
       "...        ...      ...      ...      ...     ...      ...      ...     ...   \n",
       "199995  6.7779   7.3895  10.5084  15.5057 -0.6812   5.8999   6.1825  3.1038   \n",
       "199996  6.9473   9.9392  11.1977  14.1006 -0.8012  18.8214  32.9827  1.7989   \n",
       "199997  7.0295  16.5425  10.5645  12.7330 -0.9946  23.7210  11.2390  1.0012   \n",
       "199998  7.0141   6.9451  10.0272  10.0716 -0.3385  19.4605  26.9480  1.7079   \n",
       "199999  7.1529  16.2315  11.5051  16.5967  0.6444  21.0773  25.7270  2.4916   \n",
       "\n",
       "        var_99  var_100  var_101  var_102  var_103  var_104  var_105  var_106  \\\n",
       "0      -3.4132   9.4763  13.3102  26.5376   1.4403  14.7100   6.0454   9.5426   \n",
       "1       0.6939 -13.6950   8.4068  35.4734   1.7093  15.1866   2.6227   7.3412   \n",
       "2      -0.0269  -0.3939  12.6317  14.8863   1.3854  15.0284   3.9995   5.3683   \n",
       "3       1.9480 -19.8592  22.5316  18.6129   1.3512   9.3291   4.2835  10.3907   \n",
       "4       0.6715 -22.9264  12.3562  17.3410   1.6940   7.1179   5.1934   8.8230   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "199995 -1.6930 -18.8473   9.9358  25.3359   1.3647  11.8509   5.0357   6.4630   \n",
       "199996 -0.2476 -15.5294   9.5501  11.8548   1.5127  11.3998   4.2304   6.6777   \n",
       "199997 -1.1083  -8.0574  10.0606  25.2535   1.8019  10.4973   4.2183   9.1158   \n",
       "199998 -4.8882  -2.3891  24.6626  19.7783   1.5780  14.3962   4.8206  12.2354   \n",
       "199999 -3.0062   0.9636  13.4966  31.3629   1.5517  12.1898   3.5216   6.8915   \n",
       "\n",
       "        var_107  var_108  var_109  var_110  var_111  var_112  var_113  \\\n",
       "0       17.1554  14.1104  24.3627   2.0323   6.7602   3.9141  -0.4851   \n",
       "1       32.0888  13.9550  13.0858   6.6203   7.1051   5.3523   8.5426   \n",
       "2        8.6273  14.1963  20.3882   3.2304   5.7033   4.5255   2.1929   \n",
       "3        7.0874  14.3256  14.4135   4.2827   6.9750   1.6480  11.6896   \n",
       "4       10.6617  14.0837  28.2749  -0.1937   5.9654   1.0719   7.9923   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  18.4008  14.3787  19.0369  -0.6364   6.9155   3.6763   3.1460   \n",
       "199996  11.3434  14.2993  13.1205  13.3224   7.3143   3.6817   9.7780   \n",
       "199997  10.1525  14.0837  15.2503   3.4797   8.7901   2.9000   0.6471   \n",
       "199998  33.9267  14.2625  26.2407   2.9091   6.4540   5.3290  10.6131   \n",
       "199999   9.0475  14.1260  15.9937   6.7106   8.1964   6.5520   6.6900   \n",
       "\n",
       "        var_114  var_115  var_116  var_117  var_118  var_119  var_120  \\\n",
       "0        2.5240   1.5093   2.5516  15.5752 -13.4221   7.2739  16.0094   \n",
       "1        3.6159   4.1569   3.0454   7.8522 -11.5100   7.5109  31.5899   \n",
       "2        3.1290   2.9044   1.1696  28.7632 -17.2738   2.1056  21.1613   \n",
       "3        2.5762  -2.5459   5.3446  38.1015   3.5732   5.0988  30.5644   \n",
       "4        2.9138  -3.6135   1.4684  25.6795  13.8224   4.7478  41.1037   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   4.9442  -1.8289   1.3521  34.6265  -0.6869  -5.3781  20.5030   \n",
       "199996   4.0491   2.7221   4.4344   3.7648   2.1927  -2.9197  23.0679   \n",
       "199997   2.3316   1.5084   0.2888  43.0307  -4.4543   3.2765  28.2664   \n",
       "199998   3.4212  -1.8915   2.1376  46.4915   1.0591   3.3543  18.1250   \n",
       "199999   2.4643   4.5841   0.7221  23.4020 -10.3157   6.4857  33.4300   \n",
       "\n",
       "        var_121  var_122  var_123  var_124  var_125  var_126  var_127  \\\n",
       "0        9.7268   0.8897   0.7754   4.2218  12.0039  13.8571  -0.7338   \n",
       "1        9.5018   8.2736  10.1633   0.1225  12.5942  14.5697   2.4354   \n",
       "2        8.9573   2.7768  -2.1746   3.6932  12.4653  14.1978  -2.5511   \n",
       "3       11.3025   3.9618  -8.2464   2.7038  12.3441  12.5431  -1.3683   \n",
       "4       12.7140   5.2964   9.7289   3.9370  12.1316  12.5815   7.0642   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  10.9614   4.9677   6.1408   2.2575  12.8757  14.2253  -1.2868   \n",
       "199996  12.2112   3.7517   6.7907   6.5622  13.0283  12.2389   4.0627   \n",
       "199997  12.1189   3.1526  14.2214   3.3878  13.2410  12.9788   4.5766   \n",
       "199998  10.0102   9.3483  11.0467   2.3866  12.2352  13.5462   3.0043   \n",
       "199999  12.2439   4.3416  10.3869   1.3913  12.7127  13.8530   4.6685   \n",
       "\n",
       "        var_128  var_129  var_130  var_131  var_132  var_133  var_134  \\\n",
       "0       -1.9245  15.4462  12.8287   0.3587   9.6508   6.5674   5.1726   \n",
       "1        0.8194  16.5346  12.4205  -0.1780   5.7582   7.0513   1.9568   \n",
       "2       -0.9479  17.1092  11.5419   0.0975   8.8186   6.6231   3.9358   \n",
       "3        3.5974  13.9761  14.3003   1.0486   8.9500   7.1954  -1.1984   \n",
       "4        5.6518  10.9346  11.4266   0.9442   7.7532   6.6173  -6.8304   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   0.2212  16.8661  12.7663   1.2414   7.1304   7.4108  -6.3369   \n",
       "199996  -1.2406  13.9757  12.6133   0.6524   8.3929   6.9125  -6.0942   \n",
       "199997  -4.8512  16.6344  12.3827   0.5293   8.0588   7.1081  -9.2317   \n",
       "199998   5.3751  17.1567  11.6873   0.6677   8.3511   6.5834   1.6146   \n",
       "199999  -1.6082  10.1564  11.7936   0.2316   8.1760   7.6166 -18.3865   \n",
       "\n",
       "        var_135  var_136  var_137  var_138  var_139  var_140  var_141  \\\n",
       "0        3.1345  29.4547  31.4045   2.8279  15.6599   8.3307  -5.6011   \n",
       "1       -8.9921   9.7797  18.1577  -1.9721  16.1622   3.6937   6.6803   \n",
       "2      -11.7218  24.5437  15.5827   3.8212   8.6674   7.3834  -2.4438   \n",
       "3        1.9586  27.5609  24.6065  -2.8233   8.9821   3.8873  15.9638   \n",
       "4        6.4730  17.1728  25.8128   2.6791  13.9547   6.6289  -4.3965   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   3.0760  24.9796  20.3410   5.3312  23.7116   2.4745  11.2013   \n",
       "199996  -6.3209  38.8105  17.6153  -2.9070   0.8270   2.0615   0.9315   \n",
       "199997 -11.9277  20.5706  22.5568   3.0665   1.0527   7.4011   4.3367   \n",
       "199998   4.8462  15.2331   3.8390   0.6656   2.2357  14.8203   5.8648   \n",
       "199999  -7.3542  32.6663  15.5464  -0.9083   1.0662   5.3922   9.7708   \n",
       "\n",
       "        var_142  var_143  var_144  var_145  var_146  var_147  var_148  \\\n",
       "0       19.0614  11.2663   8.6989   8.3694  11.5659 -16.4727   4.0288   \n",
       "1       -0.3243  12.2806   8.6086  11.0738   8.9231  11.7700   4.2578   \n",
       "2       10.2158   7.4844   9.1104   4.3649  11.4934   1.7624   4.0714   \n",
       "3       10.0142   7.8388   9.9718   2.9253  10.4994   4.1622   3.7613   \n",
       "4       11.7159  16.1080   7.6874   9.1570  11.5670 -12.7047   3.7574   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  17.8165  13.0057   9.5506   5.3589  13.2491  -3.3068   3.6998   \n",
       "199996   6.5953  17.2099   9.3960   9.9801   3.7881   2.9866   3.8695   \n",
       "199997   1.4242  11.3654   9.1812   2.7627  12.2434  -0.2420   4.1575   \n",
       "199998   8.7190  15.1468   9.9930  10.4543  10.9535 -10.3405   3.6463   \n",
       "199999  11.4687   8.6980   8.0106  13.1911  12.3484   0.2655   3.8811   \n",
       "\n",
       "        var_149  var_150  var_151  var_152  var_153  var_154  var_155  \\\n",
       "0       17.9244  18.5177  10.7800   9.0056  16.6964  10.4838   1.6573   \n",
       "1       -4.4223  20.6294  14.8743   9.4317  16.7242  -0.5687   0.1898   \n",
       "2       -1.2681  14.3330   8.0088   4.4015  14.1479  -5.1747   0.5778   \n",
       "3        2.3701  18.0984  17.1765   7.6508  18.2452  17.0336 -10.9370   \n",
       "4        9.9110  20.1461   1.2995   5.8493  19.8234   4.7022  10.6101   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   2.5927  14.3025   8.1596   7.9609  18.3343   4.3086   1.3546   \n",
       "199996  17.8068  18.7807   9.4546   4.4657  17.8085  13.3077  -1.3209   \n",
       "199997   4.7996  20.6307  10.2890   5.6890  13.4601  -0.9774   2.3728   \n",
       "199998 -16.8622  18.8580   8.2192  -0.4073  16.7224   8.8882  -3.2567   \n",
       "199999  -9.4762  11.6612  13.1571   8.5043  17.0369   7.1124 -13.1967   \n",
       "\n",
       "        var_156  var_157  var_158  var_159  var_160  var_161  var_162  \\\n",
       "0       12.1749 -13.1324  17.6054  11.5423  15.4576   5.3133   3.6159   \n",
       "1       12.2419  -9.6953  22.3949  10.6261  29.4846   5.8683   3.8208   \n",
       "2       14.5362  -1.7624  33.8820  11.6041  13.2070   5.8442   4.7086   \n",
       "3       12.0500  -1.2155  19.9750  12.3892  31.8833   5.9684   7.2084   \n",
       "4       13.0021 -12.6068  27.0846   8.0913  33.5107   5.6953   5.4663   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  12.4158  -5.3985  16.3683  10.4522  35.4923   5.5477   7.4244   \n",
       "199996  12.7288 -12.3625  15.3500  11.1798  35.1445   5.5375   5.6397   \n",
       "199997  11.7245  -9.6385  17.3101  14.0422  19.9293   5.3427   5.4776   \n",
       "199998  12.9142  -8.5421  15.9319   5.8348  40.3378   5.5357   4.6151   \n",
       "199999  13.9404  -8.3303  29.0140   9.6174  15.9041   5.3187   6.2987   \n",
       "\n",
       "        var_163  var_164  var_165  var_166  var_167  var_168  var_169  \\\n",
       "0        5.0384   6.6760  12.6644   2.7004  -0.6975   9.5981   5.4879   \n",
       "1       15.8348  -5.0121  15.1345   3.2003   9.3192   3.8821   5.7999   \n",
       "2        5.7141  -1.0410  20.5092   3.2790  -5.5952   7.3176   5.7690   \n",
       "3        3.8899 -11.0882  17.2502   2.5881  -2.7018   0.5641   5.3430   \n",
       "4       18.2201   6.5769  21.2607   3.2304  -1.7759   3.1283   5.5518   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  12.5459  -6.7840  31.1895   2.6529 -11.1867   9.8865   5.4730   \n",
       "199996  17.0598  -9.7142  15.5117   3.3696 -17.1855   2.8292   5.2606   \n",
       "199997  13.1202   5.3500  31.7346   3.1693 -19.4779   6.8053   5.6281   \n",
       "199998   8.5910 -12.6998  25.8578   2.2346  -6.4988   2.6702   5.3868   \n",
       "199999  13.0729  -4.2045  19.2141   3.2902  -1.2175   4.1583   5.7675   \n",
       "\n",
       "        var_170  var_171  var_172  var_173  var_174  var_175  var_176  \\\n",
       "0       -4.7645  -8.4254  20.8773   3.1531  18.5618   7.7423 -10.1245   \n",
       "1        5.5378   5.0988  22.0330   5.5134  30.2645  10.4968  -7.2352   \n",
       "2       -7.0927  -3.9116   7.2569  -5.8234  25.6820  10.9202  -0.3104   \n",
       "3       -7.1541  -6.1920  18.2366  11.7134  14.7483   8.1013  11.8771   \n",
       "4        1.4493  -2.6627  19.8056   2.3705  18.4685  16.3309  -3.3456   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  -5.3880  -0.4698  24.4025  -5.4493  11.3529   7.7075  -5.0491   \n",
       "199996   2.6836   5.8767  25.1262   7.3478  27.1264  11.8542   9.7999   \n",
       "199997  -0.8774  -8.9508  17.4931  -1.6530  32.0032  12.5749   5.8756   \n",
       "199998  -7.1875   8.1477  22.4362  -2.5914   8.8704  11.6621   7.4904   \n",
       "199999   5.7719  -1.2139  21.8496  -3.5368  25.9094  11.7673   1.9765   \n",
       "\n",
       "        var_177  var_178  var_179  var_180  var_181  var_182  var_183  \\\n",
       "0       13.7241  -3.5189   1.7202  -8.4051   9.0164   3.0657  14.3691   \n",
       "1       16.5721  -7.3477  11.0752  -5.5937   9.4878 -14.9100   9.4245   \n",
       "2        8.8438  -9.7009   2.4013  -4.2935   9.3908 -13.2648   3.1545   \n",
       "3       13.9552 -10.4701   5.6961  -3.7546   8.4117   1.8986   7.2601   \n",
       "4       13.5261   1.7189   5.1743  -7.6938   9.7685   4.8910  12.2198   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  13.0756  15.8271   3.3580 -14.3371  10.4421   7.6530   9.4585   \n",
       "199996  11.1395  -3.2870   0.4285   2.5058  10.0339   9.1610   9.4318   \n",
       "199997   8.8059 -10.6367   5.4401 -12.7967   8.7990   0.7021  14.9744   \n",
       "199998   8.1808 -11.4177   2.8379   3.8748   8.7410   8.9998  16.4058   \n",
       "199999  15.9218   3.9350   4.3993 -10.3268  10.5200   9.9587  11.9242   \n",
       "\n",
       "        var_184  var_185  var_186  var_187  var_188  var_189  var_190  \\\n",
       "0       25.8398   5.8764  11.8411 -19.7159  17.5743   0.5857   4.4354   \n",
       "1       22.5441  -4.8622   7.6543 -15.9319  13.3175  -0.3566   7.6421   \n",
       "2       23.0866  -5.3000   5.3745  -6.2660  10.1934  -0.8417   2.9057   \n",
       "3       -0.4639  -0.0498   7.9336 -12.8279  12.4124   1.8489   4.4666   \n",
       "4       11.8503  -7.8931   6.4209   5.9270  16.0201  -0.2829  -1.4905   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  22.7783  -4.0305   4.2233  -6.3906  13.5058  -0.4594   6.1415   \n",
       "199996  13.4913   4.6247   6.2906 -17.8522  18.6751  -0.1162   4.9611   \n",
       "199997  18.9211   0.3016  11.2869  -6.3741  12.9726   2.3425   4.0651   \n",
       "199998  11.3244  -2.1751  12.4735 -18.3932  12.6337   0.3243   2.6840   \n",
       "199999   7.0626  -6.5429  10.5947  -3.8827  16.3552   1.7535   8.9842   \n",
       "\n",
       "        var_191  var_192  var_193  var_194  var_195  var_196  var_197  \\\n",
       "0        3.9642   3.1364   1.6910  18.5227  -2.3978   7.8784   8.5635   \n",
       "1        7.7214   2.5837  10.9516  15.4305   2.0339   8.1267   8.7889   \n",
       "2        9.7905   1.6704   1.6858  21.6042   3.1417  -6.5213   8.2675   \n",
       "3        4.7433   0.7178   1.4214  23.0347  -1.2706  -2.9275  10.2922   \n",
       "4        9.5214  -0.1508   9.1942  13.2876  -1.5121   3.9267   9.5031   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  13.2305   3.9901   0.9388  18.0249  -1.7939   2.1661   8.5326   \n",
       "199996   4.6549   0.6998   1.8341  22.2717   1.7337  -2.1651   6.7419   \n",
       "199997   5.4414   3.1032   4.8793  23.5311  -1.5736   1.2832   8.7155   \n",
       "199998   8.6587   2.7337  11.1178  20.4158  -0.0786   6.7980  10.0342   \n",
       "199999   1.6893   0.1276   0.3766  15.2101  -2.4907  -2.2342   8.1857   \n",
       "\n",
       "        var_198  var_199  \n",
       "0       12.7803  -1.0914  \n",
       "1       18.3560   1.9518  \n",
       "2       14.7222   0.3965  \n",
       "3       17.9697  -8.9996  \n",
       "4       17.9974  -8.8104  \n",
       "...         ...      ...  \n",
       "199995  16.6660 -17.8661  \n",
       "199996  15.9054   0.3388  \n",
       "199997  13.8329   4.1995  \n",
       "199998  15.5289 -13.9001  \n",
       "199999  12.1284   0.1385  \n",
       "\n",
       "[200000 rows x 202 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61fce74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.093</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.284</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>2.1743</td>\n",
       "      <td>-3.4132</td>\n",
       "      <td>9.4763</td>\n",
       "      <td>13.3102</td>\n",
       "      <td>26.5376</td>\n",
       "      <td>1.4403</td>\n",
       "      <td>14.7100</td>\n",
       "      <td>6.0454</td>\n",
       "      <td>9.5426</td>\n",
       "      <td>17.1554</td>\n",
       "      <td>14.1104</td>\n",
       "      <td>24.3627</td>\n",
       "      <td>2.0323</td>\n",
       "      <td>6.7602</td>\n",
       "      <td>3.9141</td>\n",
       "      <td>-0.4851</td>\n",
       "      <td>2.5240</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>2.5516</td>\n",
       "      <td>15.5752</td>\n",
       "      <td>-13.4221</td>\n",
       "      <td>7.2739</td>\n",
       "      <td>16.0094</td>\n",
       "      <td>9.7268</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>12.0039</td>\n",
       "      <td>13.8571</td>\n",
       "      <td>-0.7338</td>\n",
       "      <td>-1.9245</td>\n",
       "      <td>15.4462</td>\n",
       "      <td>12.8287</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>9.6508</td>\n",
       "      <td>6.5674</td>\n",
       "      <td>5.1726</td>\n",
       "      <td>3.1345</td>\n",
       "      <td>29.4547</td>\n",
       "      <td>31.4045</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>15.6599</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>-5.6011</td>\n",
       "      <td>19.0614</td>\n",
       "      <td>11.2663</td>\n",
       "      <td>8.6989</td>\n",
       "      <td>8.3694</td>\n",
       "      <td>11.5659</td>\n",
       "      <td>-16.4727</td>\n",
       "      <td>4.0288</td>\n",
       "      <td>17.9244</td>\n",
       "      <td>18.5177</td>\n",
       "      <td>10.7800</td>\n",
       "      <td>9.0056</td>\n",
       "      <td>16.6964</td>\n",
       "      <td>10.4838</td>\n",
       "      <td>1.6573</td>\n",
       "      <td>12.1749</td>\n",
       "      <td>-13.1324</td>\n",
       "      <td>17.6054</td>\n",
       "      <td>11.5423</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.389</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.800</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>1.8944</td>\n",
       "      <td>0.6939</td>\n",
       "      <td>-13.6950</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>35.4734</td>\n",
       "      <td>1.7093</td>\n",
       "      <td>15.1866</td>\n",
       "      <td>2.6227</td>\n",
       "      <td>7.3412</td>\n",
       "      <td>32.0888</td>\n",
       "      <td>13.9550</td>\n",
       "      <td>13.0858</td>\n",
       "      <td>6.6203</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>5.3523</td>\n",
       "      <td>8.5426</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>4.1569</td>\n",
       "      <td>3.0454</td>\n",
       "      <td>7.8522</td>\n",
       "      <td>-11.5100</td>\n",
       "      <td>7.5109</td>\n",
       "      <td>31.5899</td>\n",
       "      <td>9.5018</td>\n",
       "      <td>8.2736</td>\n",
       "      <td>10.1633</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>12.5942</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>2.4354</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>16.5346</td>\n",
       "      <td>12.4205</td>\n",
       "      <td>-0.1780</td>\n",
       "      <td>5.7582</td>\n",
       "      <td>7.0513</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>-8.9921</td>\n",
       "      <td>9.7797</td>\n",
       "      <td>18.1577</td>\n",
       "      <td>-1.9721</td>\n",
       "      <td>16.1622</td>\n",
       "      <td>3.6937</td>\n",
       "      <td>6.6803</td>\n",
       "      <td>-0.3243</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>8.6086</td>\n",
       "      <td>11.0738</td>\n",
       "      <td>8.9231</td>\n",
       "      <td>11.7700</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>-4.4223</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>14.8743</td>\n",
       "      <td>9.4317</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>-0.5687</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>-9.6953</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>10.6261</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2  var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.093  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.389  12.3622  7.0433  5.6208   \n",
       "\n",
       "     var_7   var_8   var_9  var_10  var_11   var_12  var_13  var_14   var_15  \\\n",
       "0  18.6266 -4.9200  5.7470  2.9252  3.1821  14.0137  0.5745  8.7989  14.5691   \n",
       "1  16.5338  3.1468  8.0851 -0.4032  8.0585  14.0239  8.4135  5.4345  13.7003   \n",
       "\n",
       "    var_16   var_17  var_18   var_19   var_20   var_21  var_22  var_23  \\\n",
       "0   5.7487  -7.2393   4.284  30.7133  10.5350  16.2191  2.5791  2.4716   \n",
       "1  13.8275 -15.5849   7.800  28.5708   3.4287   2.7407  8.5524  3.3716   \n",
       "\n",
       "    var_24   var_25   var_26  var_27  var_28  var_29  var_30   var_31  var_32  \\\n",
       "0  14.3831  13.4325  -5.1488 -0.4073  4.9306  5.9965 -0.3085  12.9041 -3.8766   \n",
       "1   6.9779  13.8910 -11.7684 -2.5586  5.0464  0.5481 -9.2987   7.8755  1.2859   \n",
       "\n",
       "    var_33   var_34   var_35  var_36  var_37   var_38  var_39   var_40  \\\n",
       "0  16.8911  11.1920  10.5785  0.6764  7.8871   4.6667  3.8743  -5.2387   \n",
       "1  19.3710  11.3702   0.7399  2.7995  5.8434  10.8160  3.6783 -11.1147   \n",
       "\n",
       "   var_41   var_42   var_43   var_44   var_45  var_46   var_47   var_48  \\\n",
       "0  7.3746  11.5767  12.0446  11.6418  -7.0170  5.9226 -14.2136  16.0283   \n",
       "1  1.8730   9.8775  11.7842   1.2444 -47.3797  7.3718   0.1948  34.4014   \n",
       "\n",
       "    var_49   var_50   var_51  var_52  var_53  var_54   var_55   var_56  \\\n",
       "0   5.3253  12.9194  29.0460 -0.6940  5.1736 -0.7474  14.8322  11.2668   \n",
       "1  25.7037  11.8343  13.2256 -4.1083  6.6885 -8.0946  18.5995  19.3219   \n",
       "\n",
       "   var_57  var_58   var_59   var_60  var_61  var_62  var_63  var_64  var_65  \\\n",
       "0  5.3822  2.0183  10.1166  16.1828  4.9590  2.0771 -0.2154  8.6748  9.5319   \n",
       "1  7.0118  1.9210   8.8682   8.0109 -7.2417  1.7944 -1.3147  8.1042  1.5365   \n",
       "\n",
       "   var_66   var_67  var_68  var_69   var_70  var_71  var_72   var_73   var_74  \\\n",
       "0  5.8056  22.4321  5.0109 -4.7010  21.6374  0.5663  5.1999   8.8600  43.1127   \n",
       "1  5.4007   7.9344  5.0220  2.2302  40.5632  0.5134  3.1701  20.1068   7.7841   \n",
       "\n",
       "    var_75  var_76   var_77  var_78   var_79   var_80   var_81  var_82  \\\n",
       "0  18.3816 -2.3440  23.4104  6.5199  12.1983  13.6468  13.8372  1.3675   \n",
       "1   7.0529  3.2709  23.4822  5.5075  13.7814   2.5462  18.1782  0.3683   \n",
       "\n",
       "   var_83  var_84   var_85   var_86   var_87  var_88   var_89   var_90  \\\n",
       "0  2.9423 -4.5213  21.4669   9.3225  16.4597  7.9984  -1.7069 -21.4494   \n",
       "1 -4.8210 -5.4850  13.7867 -13.5901  11.0993  7.9022  12.2301   0.4768   \n",
       "\n",
       "   var_91   var_92   var_93   var_94  var_95   var_96   var_97  var_98  \\\n",
       "0  6.7806  11.0924   9.9913  14.8421  0.1812   8.9642  16.2572  2.1743   \n",
       "1  6.8852   8.0905  10.9631  11.7569 -1.2722  24.7876  26.6881  1.8944   \n",
       "\n",
       "   var_99  var_100  var_101  var_102  var_103  var_104  var_105  var_106  \\\n",
       "0 -3.4132   9.4763  13.3102  26.5376   1.4403  14.7100   6.0454   9.5426   \n",
       "1  0.6939 -13.6950   8.4068  35.4734   1.7093  15.1866   2.6227   7.3412   \n",
       "\n",
       "   var_107  var_108  var_109  var_110  var_111  var_112  var_113  var_114  \\\n",
       "0  17.1554  14.1104  24.3627   2.0323   6.7602   3.9141  -0.4851   2.5240   \n",
       "1  32.0888  13.9550  13.0858   6.6203   7.1051   5.3523   8.5426   3.6159   \n",
       "\n",
       "   var_115  var_116  var_117  var_118  var_119  var_120  var_121  var_122  \\\n",
       "0   1.5093   2.5516  15.5752 -13.4221   7.2739  16.0094   9.7268   0.8897   \n",
       "1   4.1569   3.0454   7.8522 -11.5100   7.5109  31.5899   9.5018   8.2736   \n",
       "\n",
       "   var_123  var_124  var_125  var_126  var_127  var_128  var_129  var_130  \\\n",
       "0   0.7754   4.2218  12.0039  13.8571  -0.7338  -1.9245  15.4462  12.8287   \n",
       "1  10.1633   0.1225  12.5942  14.5697   2.4354   0.8194  16.5346  12.4205   \n",
       "\n",
       "   var_131  var_132  var_133  var_134  var_135  var_136  var_137  var_138  \\\n",
       "0   0.3587   9.6508   6.5674   5.1726   3.1345  29.4547  31.4045   2.8279   \n",
       "1  -0.1780   5.7582   7.0513   1.9568  -8.9921   9.7797  18.1577  -1.9721   \n",
       "\n",
       "   var_139  var_140  var_141  var_142  var_143  var_144  var_145  var_146  \\\n",
       "0  15.6599   8.3307  -5.6011  19.0614  11.2663   8.6989   8.3694  11.5659   \n",
       "1  16.1622   3.6937   6.6803  -0.3243  12.2806   8.6086  11.0738   8.9231   \n",
       "\n",
       "   var_147  var_148  var_149  var_150  var_151  var_152  var_153  var_154  \\\n",
       "0 -16.4727   4.0288  17.9244  18.5177  10.7800   9.0056  16.6964  10.4838   \n",
       "1  11.7700   4.2578  -4.4223  20.6294  14.8743   9.4317  16.7242  -0.5687   \n",
       "\n",
       "   var_155  var_156  var_157  var_158  var_159  var_160  var_161  var_162  \\\n",
       "0   1.6573  12.1749 -13.1324  17.6054  11.5423  15.4576   5.3133   3.6159   \n",
       "1   0.1898  12.2419  -9.6953  22.3949  10.6261  29.4846   5.8683   3.8208   \n",
       "\n",
       "   var_163  var_164  var_165  var_166  var_167  var_168  var_169  var_170  \\\n",
       "0   5.0384   6.6760  12.6644   2.7004  -0.6975   9.5981   5.4879  -4.7645   \n",
       "1  15.8348  -5.0121  15.1345   3.2003   9.3192   3.8821   5.7999   5.5378   \n",
       "\n",
       "   var_171  var_172  var_173  var_174  var_175  var_176  var_177  var_178  \\\n",
       "0  -8.4254  20.8773   3.1531  18.5618   7.7423 -10.1245  13.7241  -3.5189   \n",
       "1   5.0988  22.0330   5.5134  30.2645  10.4968  -7.2352  16.5721  -7.3477   \n",
       "\n",
       "   var_179  var_180  var_181  var_182  var_183  var_184  var_185  var_186  \\\n",
       "0   1.7202  -8.4051   9.0164   3.0657  14.3691  25.8398   5.8764  11.8411   \n",
       "1  11.0752  -5.5937   9.4878 -14.9100   9.4245  22.5441  -4.8622   7.6543   \n",
       "\n",
       "   var_187  var_188  var_189  var_190  var_191  var_192  var_193  var_194  \\\n",
       "0 -19.7159  17.5743   0.5857   4.4354   3.9642   3.1364   1.6910  18.5227   \n",
       "1 -15.9319  13.3175  -0.3566   7.6421   7.7214   2.5837  10.9516  15.4305   \n",
       "\n",
       "   var_195  var_196  var_197  var_198  var_199  \n",
       "0  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   2.0339   8.1267   8.7889  18.3560   1.9518  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d161f3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 202 entries, ID_code to var_199\n",
      "dtypes: float64(200), int64(1), object(1)\n",
      "memory usage: 308.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876723c3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.100490</td>\n",
       "      <td>10.679914</td>\n",
       "      <td>-1.627622</td>\n",
       "      <td>10.715192</td>\n",
       "      <td>6.796529</td>\n",
       "      <td>11.078333</td>\n",
       "      <td>-5.065317</td>\n",
       "      <td>5.408949</td>\n",
       "      <td>16.545850</td>\n",
       "      <td>0.284162</td>\n",
       "      <td>7.567236</td>\n",
       "      <td>0.394340</td>\n",
       "      <td>-3.245596</td>\n",
       "      <td>14.023978</td>\n",
       "      <td>8.530232</td>\n",
       "      <td>7.537606</td>\n",
       "      <td>14.573126</td>\n",
       "      <td>9.333264</td>\n",
       "      <td>-5.696731</td>\n",
       "      <td>15.244013</td>\n",
       "      <td>12.438567</td>\n",
       "      <td>13.290894</td>\n",
       "      <td>17.257883</td>\n",
       "      <td>4.305430</td>\n",
       "      <td>3.019540</td>\n",
       "      <td>10.584400</td>\n",
       "      <td>13.667496</td>\n",
       "      <td>-4.055133</td>\n",
       "      <td>-1.137908</td>\n",
       "      <td>5.532980</td>\n",
       "      <td>5.053874</td>\n",
       "      <td>-7.687740</td>\n",
       "      <td>10.393046</td>\n",
       "      <td>-0.512886</td>\n",
       "      <td>14.774147</td>\n",
       "      <td>11.434250</td>\n",
       "      <td>3.842499</td>\n",
       "      <td>2.187230</td>\n",
       "      <td>5.868899</td>\n",
       "      <td>10.642131</td>\n",
       "      <td>0.662956</td>\n",
       "      <td>-6.725505</td>\n",
       "      <td>9.299858</td>\n",
       "      <td>11.222356</td>\n",
       "      <td>11.569954</td>\n",
       "      <td>8.948289</td>\n",
       "      <td>-12.699667</td>\n",
       "      <td>11.326488</td>\n",
       "      <td>-12.471737</td>\n",
       "      <td>14.704713</td>\n",
       "      <td>16.682499</td>\n",
       "      <td>12.740986</td>\n",
       "      <td>13.428912</td>\n",
       "      <td>-2.528816</td>\n",
       "      <td>6.008569</td>\n",
       "      <td>1.137117</td>\n",
       "      <td>12.745852</td>\n",
       "      <td>16.629165</td>\n",
       "      <td>6.272014</td>\n",
       "      <td>3.177633</td>\n",
       "      <td>8.931124</td>\n",
       "      <td>12.155618</td>\n",
       "      <td>-11.946744</td>\n",
       "      <td>0.874170</td>\n",
       "      <td>0.661173</td>\n",
       "      <td>6.369157</td>\n",
       "      <td>0.982891</td>\n",
       "      <td>5.794039</td>\n",
       "      <td>11.943223</td>\n",
       "      <td>5.018893</td>\n",
       "      <td>-3.331515</td>\n",
       "      <td>24.446811</td>\n",
       "      <td>0.669756</td>\n",
       "      <td>0.640553</td>\n",
       "      <td>19.610888</td>\n",
       "      <td>19.518846</td>\n",
       "      <td>16.853732</td>\n",
       "      <td>6.050871</td>\n",
       "      <td>19.066993</td>\n",
       "      <td>5.349479</td>\n",
       "      <td>14.402136</td>\n",
       "      <td>5.795044</td>\n",
       "      <td>14.719024</td>\n",
       "      <td>-3.471273</td>\n",
       "      <td>1.025817</td>\n",
       "      <td>-2.590209</td>\n",
       "      <td>18.362721</td>\n",
       "      <td>5.621058</td>\n",
       "      <td>11.351483</td>\n",
       "      <td>8.702924</td>\n",
       "      <td>3.725208</td>\n",
       "      <td>-16.548147</td>\n",
       "      <td>6.987541</td>\n",
       "      <td>12.739578</td>\n",
       "      <td>10.556740</td>\n",
       "      <td>10.999162</td>\n",
       "      <td>-0.084344</td>\n",
       "      <td>14.400433</td>\n",
       "      <td>18.539645</td>\n",
       "      <td>1.752012</td>\n",
       "      <td>-0.746296</td>\n",
       "      <td>-6.600518</td>\n",
       "      <td>13.413526</td>\n",
       "      <td>22.294908</td>\n",
       "      <td>1.568393</td>\n",
       "      <td>11.509834</td>\n",
       "      <td>4.244744</td>\n",
       "      <td>8.617657</td>\n",
       "      <td>17.796266</td>\n",
       "      <td>14.224435</td>\n",
       "      <td>18.458001</td>\n",
       "      <td>5.513238</td>\n",
       "      <td>6.312603</td>\n",
       "      <td>3.317843</td>\n",
       "      <td>8.136542</td>\n",
       "      <td>3.081191</td>\n",
       "      <td>2.213717</td>\n",
       "      <td>2.402570</td>\n",
       "      <td>16.102233</td>\n",
       "      <td>-5.305132</td>\n",
       "      <td>3.032849</td>\n",
       "      <td>24.521078</td>\n",
       "      <td>11.310591</td>\n",
       "      <td>1.192984</td>\n",
       "      <td>7.076254</td>\n",
       "      <td>4.272740</td>\n",
       "      <td>12.489165</td>\n",
       "      <td>13.202326</td>\n",
       "      <td>0.851507</td>\n",
       "      <td>-1.127952</td>\n",
       "      <td>15.460314</td>\n",
       "      <td>12.257151</td>\n",
       "      <td>0.544674</td>\n",
       "      <td>7.799676</td>\n",
       "      <td>6.813270</td>\n",
       "      <td>-4.826053</td>\n",
       "      <td>-4.259472</td>\n",
       "      <td>22.968602</td>\n",
       "      <td>17.613651</td>\n",
       "      <td>1.210792</td>\n",
       "      <td>7.760193</td>\n",
       "      <td>3.423636</td>\n",
       "      <td>2.897596</td>\n",
       "      <td>11.983489</td>\n",
       "      <td>12.333698</td>\n",
       "      <td>8.647632</td>\n",
       "      <td>4.841328</td>\n",
       "      <td>10.341178</td>\n",
       "      <td>-3.300779</td>\n",
       "      <td>3.990726</td>\n",
       "      <td>5.296237</td>\n",
       "      <td>16.817671</td>\n",
       "      <td>10.141542</td>\n",
       "      <td>7.633199</td>\n",
       "      <td>16.727902</td>\n",
       "      <td>6.974955</td>\n",
       "      <td>-2.074128</td>\n",
       "      <td>13.209272</td>\n",
       "      <td>-4.813552</td>\n",
       "      <td>17.914591</td>\n",
       "      <td>10.223282</td>\n",
       "      <td>24.259300</td>\n",
       "      <td>5.633293</td>\n",
       "      <td>5.362896</td>\n",
       "      <td>11.002170</td>\n",
       "      <td>-2.871906</td>\n",
       "      <td>19.315753</td>\n",
       "      <td>2.963335</td>\n",
       "      <td>-4.151155</td>\n",
       "      <td>4.937124</td>\n",
       "      <td>5.636008</td>\n",
       "      <td>-0.004962</td>\n",
       "      <td>-0.831777</td>\n",
       "      <td>19.817094</td>\n",
       "      <td>-0.677967</td>\n",
       "      <td>20.210677</td>\n",
       "      <td>11.640613</td>\n",
       "      <td>-2.799585</td>\n",
       "      <td>11.882933</td>\n",
       "      <td>-1.014064</td>\n",
       "      <td>2.591444</td>\n",
       "      <td>-2.741666</td>\n",
       "      <td>10.085518</td>\n",
       "      <td>0.719109</td>\n",
       "      <td>8.769088</td>\n",
       "      <td>12.756676</td>\n",
       "      <td>-3.983261</td>\n",
       "      <td>8.970274</td>\n",
       "      <td>-10.335043</td>\n",
       "      <td>15.377174</td>\n",
       "      <td>0.746072</td>\n",
       "      <td>3.234440</td>\n",
       "      <td>7.438408</td>\n",
       "      <td>1.927839</td>\n",
       "      <td>3.331774</td>\n",
       "      <td>17.993784</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>2.303335</td>\n",
       "      <td>8.908158</td>\n",
       "      <td>15.870720</td>\n",
       "      <td>-3.326537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.300653</td>\n",
       "      <td>3.040051</td>\n",
       "      <td>4.050044</td>\n",
       "      <td>2.640894</td>\n",
       "      <td>2.043319</td>\n",
       "      <td>1.623150</td>\n",
       "      <td>7.863267</td>\n",
       "      <td>0.866607</td>\n",
       "      <td>3.418076</td>\n",
       "      <td>3.332634</td>\n",
       "      <td>1.235070</td>\n",
       "      <td>5.500793</td>\n",
       "      <td>5.970253</td>\n",
       "      <td>0.190059</td>\n",
       "      <td>4.639536</td>\n",
       "      <td>2.247908</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>2.557421</td>\n",
       "      <td>6.712612</td>\n",
       "      <td>7.851370</td>\n",
       "      <td>7.996694</td>\n",
       "      <td>5.876254</td>\n",
       "      <td>8.196564</td>\n",
       "      <td>2.847958</td>\n",
       "      <td>0.526893</td>\n",
       "      <td>3.777245</td>\n",
       "      <td>0.285535</td>\n",
       "      <td>5.922210</td>\n",
       "      <td>1.523714</td>\n",
       "      <td>0.783367</td>\n",
       "      <td>2.615942</td>\n",
       "      <td>7.965198</td>\n",
       "      <td>2.159891</td>\n",
       "      <td>2.587830</td>\n",
       "      <td>4.322325</td>\n",
       "      <td>0.541614</td>\n",
       "      <td>5.179559</td>\n",
       "      <td>3.119978</td>\n",
       "      <td>2.249730</td>\n",
       "      <td>4.278903</td>\n",
       "      <td>4.068845</td>\n",
       "      <td>8.279259</td>\n",
       "      <td>5.938088</td>\n",
       "      <td>0.695991</td>\n",
       "      <td>0.309599</td>\n",
       "      <td>5.903073</td>\n",
       "      <td>21.404912</td>\n",
       "      <td>2.860511</td>\n",
       "      <td>10.579862</td>\n",
       "      <td>11.384332</td>\n",
       "      <td>7.855762</td>\n",
       "      <td>0.691709</td>\n",
       "      <td>8.187306</td>\n",
       "      <td>4.985532</td>\n",
       "      <td>0.764753</td>\n",
       "      <td>8.414241</td>\n",
       "      <td>5.690072</td>\n",
       "      <td>3.540174</td>\n",
       "      <td>0.795026</td>\n",
       "      <td>4.296686</td>\n",
       "      <td>0.854798</td>\n",
       "      <td>4.222389</td>\n",
       "      <td>11.622948</td>\n",
       "      <td>2.026238</td>\n",
       "      <td>3.113089</td>\n",
       "      <td>1.485854</td>\n",
       "      <td>3.786493</td>\n",
       "      <td>1.121366</td>\n",
       "      <td>7.365115</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>3.955723</td>\n",
       "      <td>11.951742</td>\n",
       "      <td>0.266696</td>\n",
       "      <td>3.944703</td>\n",
       "      <td>7.466303</td>\n",
       "      <td>14.112591</td>\n",
       "      <td>6.055322</td>\n",
       "      <td>7.938351</td>\n",
       "      <td>3.817292</td>\n",
       "      <td>1.993792</td>\n",
       "      <td>1.309055</td>\n",
       "      <td>7.436737</td>\n",
       "      <td>2.299567</td>\n",
       "      <td>8.479255</td>\n",
       "      <td>8.297229</td>\n",
       "      <td>6.225305</td>\n",
       "      <td>3.908536</td>\n",
       "      <td>7.751142</td>\n",
       "      <td>5.661867</td>\n",
       "      <td>2.491460</td>\n",
       "      <td>3.560554</td>\n",
       "      <td>13.152810</td>\n",
       "      <td>0.152641</td>\n",
       "      <td>4.186252</td>\n",
       "      <td>0.543341</td>\n",
       "      <td>2.768099</td>\n",
       "      <td>0.621125</td>\n",
       "      <td>8.525400</td>\n",
       "      <td>12.642382</td>\n",
       "      <td>0.715836</td>\n",
       "      <td>1.862550</td>\n",
       "      <td>9.181683</td>\n",
       "      <td>4.950537</td>\n",
       "      <td>8.628179</td>\n",
       "      <td>0.185020</td>\n",
       "      <td>1.970520</td>\n",
       "      <td>0.855698</td>\n",
       "      <td>1.894899</td>\n",
       "      <td>7.604723</td>\n",
       "      <td>0.171091</td>\n",
       "      <td>4.355031</td>\n",
       "      <td>3.823253</td>\n",
       "      <td>1.082404</td>\n",
       "      <td>1.591170</td>\n",
       "      <td>4.459077</td>\n",
       "      <td>0.985396</td>\n",
       "      <td>2.621851</td>\n",
       "      <td>1.650912</td>\n",
       "      <td>13.297662</td>\n",
       "      <td>8.799268</td>\n",
       "      <td>4.182796</td>\n",
       "      <td>12.121016</td>\n",
       "      <td>1.714416</td>\n",
       "      <td>5.168479</td>\n",
       "      <td>6.147345</td>\n",
       "      <td>2.736821</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>0.776056</td>\n",
       "      <td>3.137684</td>\n",
       "      <td>3.238043</td>\n",
       "      <td>4.136453</td>\n",
       "      <td>0.832199</td>\n",
       "      <td>0.456280</td>\n",
       "      <td>1.456486</td>\n",
       "      <td>0.375603</td>\n",
       "      <td>6.166126</td>\n",
       "      <td>7.617732</td>\n",
       "      <td>10.382235</td>\n",
       "      <td>8.890516</td>\n",
       "      <td>4.551750</td>\n",
       "      <td>7.686433</td>\n",
       "      <td>4.896325</td>\n",
       "      <td>6.715637</td>\n",
       "      <td>5.691936</td>\n",
       "      <td>2.934706</td>\n",
       "      <td>0.922469</td>\n",
       "      <td>3.899281</td>\n",
       "      <td>2.518883</td>\n",
       "      <td>7.413301</td>\n",
       "      <td>0.199192</td>\n",
       "      <td>10.385133</td>\n",
       "      <td>2.464157</td>\n",
       "      <td>3.962426</td>\n",
       "      <td>3.005373</td>\n",
       "      <td>2.014200</td>\n",
       "      <td>4.961678</td>\n",
       "      <td>5.771261</td>\n",
       "      <td>0.955140</td>\n",
       "      <td>5.570272</td>\n",
       "      <td>7.885579</td>\n",
       "      <td>4.122912</td>\n",
       "      <td>10.880263</td>\n",
       "      <td>0.217938</td>\n",
       "      <td>1.419612</td>\n",
       "      <td>5.262056</td>\n",
       "      <td>5.457784</td>\n",
       "      <td>5.024182</td>\n",
       "      <td>0.369684</td>\n",
       "      <td>7.798020</td>\n",
       "      <td>3.105986</td>\n",
       "      <td>0.369437</td>\n",
       "      <td>4.424621</td>\n",
       "      <td>5.378008</td>\n",
       "      <td>8.674171</td>\n",
       "      <td>5.966674</td>\n",
       "      <td>7.136427</td>\n",
       "      <td>2.892167</td>\n",
       "      <td>7.513939</td>\n",
       "      <td>2.628895</td>\n",
       "      <td>8.579810</td>\n",
       "      <td>2.798956</td>\n",
       "      <td>5.261243</td>\n",
       "      <td>1.371862</td>\n",
       "      <td>8.963434</td>\n",
       "      <td>4.474924</td>\n",
       "      <td>9.318280</td>\n",
       "      <td>4.725167</td>\n",
       "      <td>3.189759</td>\n",
       "      <td>11.574708</td>\n",
       "      <td>3.944604</td>\n",
       "      <td>0.976348</td>\n",
       "      <td>4.559922</td>\n",
       "      <td>3.023272</td>\n",
       "      <td>1.478423</td>\n",
       "      <td>3.992030</td>\n",
       "      <td>3.135162</td>\n",
       "      <td>1.429372</td>\n",
       "      <td>5.454369</td>\n",
       "      <td>0.921625</td>\n",
       "      <td>3.010945</td>\n",
       "      <td>10.438015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>-15.043400</td>\n",
       "      <td>2.117100</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>5.074800</td>\n",
       "      <td>-32.562600</td>\n",
       "      <td>2.347300</td>\n",
       "      <td>5.349700</td>\n",
       "      <td>-10.505500</td>\n",
       "      <td>3.970500</td>\n",
       "      <td>-20.731300</td>\n",
       "      <td>-26.095000</td>\n",
       "      <td>13.434600</td>\n",
       "      <td>-6.011100</td>\n",
       "      <td>1.013300</td>\n",
       "      <td>13.076900</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>-33.380200</td>\n",
       "      <td>-10.664200</td>\n",
       "      <td>-12.402500</td>\n",
       "      <td>-5.432200</td>\n",
       "      <td>-10.089000</td>\n",
       "      <td>-5.322500</td>\n",
       "      <td>1.209800</td>\n",
       "      <td>-0.678400</td>\n",
       "      <td>12.720000</td>\n",
       "      <td>-24.243100</td>\n",
       "      <td>-6.166800</td>\n",
       "      <td>2.089600</td>\n",
       "      <td>-4.787200</td>\n",
       "      <td>-34.798400</td>\n",
       "      <td>2.140600</td>\n",
       "      <td>-8.986100</td>\n",
       "      <td>1.508500</td>\n",
       "      <td>9.816900</td>\n",
       "      <td>-16.513600</td>\n",
       "      <td>-8.095100</td>\n",
       "      <td>-1.183400</td>\n",
       "      <td>-6.337100</td>\n",
       "      <td>-14.545700</td>\n",
       "      <td>-35.211700</td>\n",
       "      <td>-8.535900</td>\n",
       "      <td>8.859000</td>\n",
       "      <td>10.652800</td>\n",
       "      <td>-9.939600</td>\n",
       "      <td>-90.252500</td>\n",
       "      <td>1.206200</td>\n",
       "      <td>-47.686200</td>\n",
       "      <td>-23.902200</td>\n",
       "      <td>-8.070700</td>\n",
       "      <td>10.385500</td>\n",
       "      <td>-15.046200</td>\n",
       "      <td>-24.721400</td>\n",
       "      <td>3.344900</td>\n",
       "      <td>-26.778600</td>\n",
       "      <td>-3.782600</td>\n",
       "      <td>2.761800</td>\n",
       "      <td>3.442300</td>\n",
       "      <td>-12.600900</td>\n",
       "      <td>6.184000</td>\n",
       "      <td>-2.100600</td>\n",
       "      <td>-48.802700</td>\n",
       "      <td>-6.328900</td>\n",
       "      <td>-10.554400</td>\n",
       "      <td>1.611700</td>\n",
       "      <td>-14.088800</td>\n",
       "      <td>1.336800</td>\n",
       "      <td>-19.544300</td>\n",
       "      <td>4.993800</td>\n",
       "      <td>-16.309400</td>\n",
       "      <td>-17.027500</td>\n",
       "      <td>-0.224000</td>\n",
       "      <td>-12.383400</td>\n",
       "      <td>-1.665800</td>\n",
       "      <td>-34.101500</td>\n",
       "      <td>-1.293600</td>\n",
       "      <td>-21.633300</td>\n",
       "      <td>7.425700</td>\n",
       "      <td>-1.818300</td>\n",
       "      <td>10.445400</td>\n",
       "      <td>-18.042200</td>\n",
       "      <td>7.586500</td>\n",
       "      <td>-30.026600</td>\n",
       "      <td>-24.220100</td>\n",
       "      <td>-24.439800</td>\n",
       "      <td>7.023000</td>\n",
       "      <td>-19.272200</td>\n",
       "      <td>-8.481600</td>\n",
       "      <td>1.350200</td>\n",
       "      <td>-9.601400</td>\n",
       "      <td>-61.718000</td>\n",
       "      <td>6.521800</td>\n",
       "      <td>-1.018500</td>\n",
       "      <td>8.491600</td>\n",
       "      <td>2.819000</td>\n",
       "      <td>-2.432400</td>\n",
       "      <td>-12.158400</td>\n",
       "      <td>-21.740000</td>\n",
       "      <td>-0.603500</td>\n",
       "      <td>-7.280600</td>\n",
       "      <td>-39.179100</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>-7.382900</td>\n",
       "      <td>0.979300</td>\n",
       "      <td>4.084600</td>\n",
       "      <td>0.715300</td>\n",
       "      <td>0.942400</td>\n",
       "      <td>-5.898000</td>\n",
       "      <td>13.729000</td>\n",
       "      <td>5.769700</td>\n",
       "      <td>-9.239800</td>\n",
       "      <td>2.194200</td>\n",
       "      <td>-2.030200</td>\n",
       "      <td>-5.513900</td>\n",
       "      <td>-0.050500</td>\n",
       "      <td>-6.858600</td>\n",
       "      <td>-3.163000</td>\n",
       "      <td>-31.836900</td>\n",
       "      <td>-37.527700</td>\n",
       "      <td>-9.774200</td>\n",
       "      <td>-18.696200</td>\n",
       "      <td>6.305200</td>\n",
       "      <td>-15.194000</td>\n",
       "      <td>-12.405900</td>\n",
       "      <td>-7.053800</td>\n",
       "      <td>11.486100</td>\n",
       "      <td>11.265400</td>\n",
       "      <td>-8.876900</td>\n",
       "      <td>-11.755900</td>\n",
       "      <td>2.186300</td>\n",
       "      <td>9.528300</td>\n",
       "      <td>-0.954800</td>\n",
       "      <td>2.890000</td>\n",
       "      <td>5.359300</td>\n",
       "      <td>-24.254600</td>\n",
       "      <td>-31.380800</td>\n",
       "      <td>-9.949300</td>\n",
       "      <td>-9.851000</td>\n",
       "      <td>-16.468400</td>\n",
       "      <td>-21.274300</td>\n",
       "      <td>-15.459500</td>\n",
       "      <td>-16.693700</td>\n",
       "      <td>-7.108000</td>\n",
       "      <td>2.806800</td>\n",
       "      <td>5.444300</td>\n",
       "      <td>-8.273400</td>\n",
       "      <td>0.427400</td>\n",
       "      <td>-29.984000</td>\n",
       "      <td>3.320500</td>\n",
       "      <td>-41.168300</td>\n",
       "      <td>9.242000</td>\n",
       "      <td>-2.191500</td>\n",
       "      <td>-2.880000</td>\n",
       "      <td>11.030800</td>\n",
       "      <td>-8.196600</td>\n",
       "      <td>-21.840900</td>\n",
       "      <td>9.996500</td>\n",
       "      <td>-22.990400</td>\n",
       "      <td>-4.554400</td>\n",
       "      <td>-4.641600</td>\n",
       "      <td>-7.452200</td>\n",
       "      <td>4.852600</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>-6.531700</td>\n",
       "      <td>-19.997700</td>\n",
       "      <td>3.816700</td>\n",
       "      <td>1.851200</td>\n",
       "      <td>-35.969500</td>\n",
       "      <td>-5.250200</td>\n",
       "      <td>4.258800</td>\n",
       "      <td>-14.506000</td>\n",
       "      <td>-22.479300</td>\n",
       "      <td>-11.453300</td>\n",
       "      <td>-22.748700</td>\n",
       "      <td>-2.995300</td>\n",
       "      <td>3.241500</td>\n",
       "      <td>-29.116500</td>\n",
       "      <td>4.952100</td>\n",
       "      <td>-29.273400</td>\n",
       "      <td>-7.856100</td>\n",
       "      <td>-22.037400</td>\n",
       "      <td>5.416500</td>\n",
       "      <td>-26.001100</td>\n",
       "      <td>-4.808200</td>\n",
       "      <td>-18.489700</td>\n",
       "      <td>-22.583300</td>\n",
       "      <td>-3.022300</td>\n",
       "      <td>-47.753600</td>\n",
       "      <td>4.412300</td>\n",
       "      <td>-2.554300</td>\n",
       "      <td>-14.093300</td>\n",
       "      <td>-2.691700</td>\n",
       "      <td>-3.814500</td>\n",
       "      <td>-11.783400</td>\n",
       "      <td>8.694400</td>\n",
       "      <td>-5.261000</td>\n",
       "      <td>-14.209600</td>\n",
       "      <td>5.960600</td>\n",
       "      <td>6.299300</td>\n",
       "      <td>-38.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.453850</td>\n",
       "      <td>-4.740025</td>\n",
       "      <td>8.722475</td>\n",
       "      <td>5.254075</td>\n",
       "      <td>9.883175</td>\n",
       "      <td>-11.200350</td>\n",
       "      <td>4.767700</td>\n",
       "      <td>13.943800</td>\n",
       "      <td>-2.317800</td>\n",
       "      <td>6.618800</td>\n",
       "      <td>-3.594950</td>\n",
       "      <td>-7.510600</td>\n",
       "      <td>13.894000</td>\n",
       "      <td>5.072800</td>\n",
       "      <td>5.781875</td>\n",
       "      <td>14.262800</td>\n",
       "      <td>7.452275</td>\n",
       "      <td>-10.476225</td>\n",
       "      <td>9.177950</td>\n",
       "      <td>6.276475</td>\n",
       "      <td>8.627800</td>\n",
       "      <td>11.551000</td>\n",
       "      <td>2.182400</td>\n",
       "      <td>2.634100</td>\n",
       "      <td>7.613000</td>\n",
       "      <td>13.456400</td>\n",
       "      <td>-8.321725</td>\n",
       "      <td>-2.307900</td>\n",
       "      <td>4.992100</td>\n",
       "      <td>3.171700</td>\n",
       "      <td>-13.766175</td>\n",
       "      <td>8.870000</td>\n",
       "      <td>-2.500875</td>\n",
       "      <td>11.456300</td>\n",
       "      <td>11.032300</td>\n",
       "      <td>0.116975</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>4.125475</td>\n",
       "      <td>7.591050</td>\n",
       "      <td>-2.199500</td>\n",
       "      <td>-12.831825</td>\n",
       "      <td>4.519575</td>\n",
       "      <td>10.713200</td>\n",
       "      <td>11.343800</td>\n",
       "      <td>5.313650</td>\n",
       "      <td>-28.730700</td>\n",
       "      <td>9.248750</td>\n",
       "      <td>-20.654525</td>\n",
       "      <td>6.351975</td>\n",
       "      <td>10.653475</td>\n",
       "      <td>12.269000</td>\n",
       "      <td>7.267625</td>\n",
       "      <td>-6.065025</td>\n",
       "      <td>5.435600</td>\n",
       "      <td>-5.147625</td>\n",
       "      <td>8.163900</td>\n",
       "      <td>14.097875</td>\n",
       "      <td>5.687500</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>8.312400</td>\n",
       "      <td>8.912750</td>\n",
       "      <td>-20.901725</td>\n",
       "      <td>-0.572400</td>\n",
       "      <td>-1.588700</td>\n",
       "      <td>5.293500</td>\n",
       "      <td>-1.702800</td>\n",
       "      <td>4.973800</td>\n",
       "      <td>6.753200</td>\n",
       "      <td>5.014000</td>\n",
       "      <td>-6.336625</td>\n",
       "      <td>15.256625</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>-2.197100</td>\n",
       "      <td>14.097275</td>\n",
       "      <td>9.595975</td>\n",
       "      <td>12.480975</td>\n",
       "      <td>0.596300</td>\n",
       "      <td>16.014700</td>\n",
       "      <td>3.817275</td>\n",
       "      <td>13.375400</td>\n",
       "      <td>0.694475</td>\n",
       "      <td>13.214775</td>\n",
       "      <td>-10.004950</td>\n",
       "      <td>-5.106400</td>\n",
       "      <td>-7.216125</td>\n",
       "      <td>15.338575</td>\n",
       "      <td>0.407550</td>\n",
       "      <td>7.247175</td>\n",
       "      <td>6.918775</td>\n",
       "      <td>1.140500</td>\n",
       "      <td>-26.665600</td>\n",
       "      <td>6.869900</td>\n",
       "      <td>9.670300</td>\n",
       "      <td>10.195600</td>\n",
       "      <td>8.828000</td>\n",
       "      <td>-0.527400</td>\n",
       "      <td>7.796950</td>\n",
       "      <td>8.919525</td>\n",
       "      <td>1.267675</td>\n",
       "      <td>-2.106200</td>\n",
       "      <td>-13.198700</td>\n",
       "      <td>9.639800</td>\n",
       "      <td>16.047975</td>\n",
       "      <td>1.428900</td>\n",
       "      <td>10.097900</td>\n",
       "      <td>3.639600</td>\n",
       "      <td>7.282300</td>\n",
       "      <td>12.168075</td>\n",
       "      <td>14.098900</td>\n",
       "      <td>15.107175</td>\n",
       "      <td>2.817475</td>\n",
       "      <td>5.510100</td>\n",
       "      <td>2.092675</td>\n",
       "      <td>4.803250</td>\n",
       "      <td>2.388775</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>6.373500</td>\n",
       "      <td>-11.587850</td>\n",
       "      <td>-0.161975</td>\n",
       "      <td>15.696275</td>\n",
       "      <td>9.996400</td>\n",
       "      <td>-2.565200</td>\n",
       "      <td>2.817050</td>\n",
       "      <td>2.353600</td>\n",
       "      <td>12.245400</td>\n",
       "      <td>12.608400</td>\n",
       "      <td>-1.502325</td>\n",
       "      <td>-3.580725</td>\n",
       "      <td>12.514475</td>\n",
       "      <td>11.619300</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>6.724375</td>\n",
       "      <td>6.543500</td>\n",
       "      <td>-9.625700</td>\n",
       "      <td>-9.957100</td>\n",
       "      <td>14.933900</td>\n",
       "      <td>10.656550</td>\n",
       "      <td>-2.011825</td>\n",
       "      <td>2.387575</td>\n",
       "      <td>-0.121700</td>\n",
       "      <td>-2.153725</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>10.311200</td>\n",
       "      <td>7.968075</td>\n",
       "      <td>1.885875</td>\n",
       "      <td>8.646900</td>\n",
       "      <td>-8.751450</td>\n",
       "      <td>3.853600</td>\n",
       "      <td>-1.903200</td>\n",
       "      <td>14.952200</td>\n",
       "      <td>7.064600</td>\n",
       "      <td>5.567900</td>\n",
       "      <td>15.233000</td>\n",
       "      <td>3.339900</td>\n",
       "      <td>-6.266025</td>\n",
       "      <td>12.475100</td>\n",
       "      <td>-8.939950</td>\n",
       "      <td>12.109200</td>\n",
       "      <td>7.243525</td>\n",
       "      <td>15.696125</td>\n",
       "      <td>5.470500</td>\n",
       "      <td>4.326100</td>\n",
       "      <td>7.029600</td>\n",
       "      <td>-7.094025</td>\n",
       "      <td>15.744550</td>\n",
       "      <td>2.699000</td>\n",
       "      <td>-9.643100</td>\n",
       "      <td>2.703200</td>\n",
       "      <td>5.374600</td>\n",
       "      <td>-3.258500</td>\n",
       "      <td>-4.720350</td>\n",
       "      <td>13.731775</td>\n",
       "      <td>-5.009525</td>\n",
       "      <td>15.064600</td>\n",
       "      <td>9.371600</td>\n",
       "      <td>-8.386500</td>\n",
       "      <td>9.808675</td>\n",
       "      <td>-7.395700</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>-6.673900</td>\n",
       "      <td>9.084700</td>\n",
       "      <td>-6.064425</td>\n",
       "      <td>5.423100</td>\n",
       "      <td>5.663300</td>\n",
       "      <td>-7.360000</td>\n",
       "      <td>6.715200</td>\n",
       "      <td>-19.205125</td>\n",
       "      <td>12.501550</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>5.157400</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>15.629800</td>\n",
       "      <td>-1.170700</td>\n",
       "      <td>-1.946925</td>\n",
       "      <td>8.252800</td>\n",
       "      <td>13.829700</td>\n",
       "      <td>-11.208475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.524750</td>\n",
       "      <td>-1.608050</td>\n",
       "      <td>10.580000</td>\n",
       "      <td>6.825000</td>\n",
       "      <td>11.108250</td>\n",
       "      <td>-4.833150</td>\n",
       "      <td>5.385100</td>\n",
       "      <td>16.456800</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>7.629600</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>-3.286950</td>\n",
       "      <td>14.025500</td>\n",
       "      <td>8.604250</td>\n",
       "      <td>7.520300</td>\n",
       "      <td>14.574100</td>\n",
       "      <td>9.232050</td>\n",
       "      <td>-5.666350</td>\n",
       "      <td>15.196250</td>\n",
       "      <td>12.453900</td>\n",
       "      <td>13.196800</td>\n",
       "      <td>17.234250</td>\n",
       "      <td>4.275150</td>\n",
       "      <td>3.008650</td>\n",
       "      <td>10.380350</td>\n",
       "      <td>13.662500</td>\n",
       "      <td>-4.196900</td>\n",
       "      <td>-1.132100</td>\n",
       "      <td>5.534850</td>\n",
       "      <td>4.950200</td>\n",
       "      <td>-7.411750</td>\n",
       "      <td>10.365650</td>\n",
       "      <td>-0.497650</td>\n",
       "      <td>14.576000</td>\n",
       "      <td>11.435200</td>\n",
       "      <td>3.917750</td>\n",
       "      <td>2.198000</td>\n",
       "      <td>5.900650</td>\n",
       "      <td>10.562700</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>-6.617450</td>\n",
       "      <td>9.162650</td>\n",
       "      <td>11.243400</td>\n",
       "      <td>11.565000</td>\n",
       "      <td>9.437200</td>\n",
       "      <td>-12.547200</td>\n",
       "      <td>11.310750</td>\n",
       "      <td>-12.482400</td>\n",
       "      <td>14.559200</td>\n",
       "      <td>16.672400</td>\n",
       "      <td>12.745600</td>\n",
       "      <td>13.444400</td>\n",
       "      <td>-2.502450</td>\n",
       "      <td>6.027800</td>\n",
       "      <td>1.274050</td>\n",
       "      <td>12.594100</td>\n",
       "      <td>16.648150</td>\n",
       "      <td>6.262500</td>\n",
       "      <td>3.170100</td>\n",
       "      <td>8.901000</td>\n",
       "      <td>12.064350</td>\n",
       "      <td>-11.892000</td>\n",
       "      <td>0.794700</td>\n",
       "      <td>0.681700</td>\n",
       "      <td>6.377700</td>\n",
       "      <td>1.021350</td>\n",
       "      <td>5.782000</td>\n",
       "      <td>11.922000</td>\n",
       "      <td>5.019100</td>\n",
       "      <td>-3.325500</td>\n",
       "      <td>24.445000</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>0.646450</td>\n",
       "      <td>19.309750</td>\n",
       "      <td>19.536650</td>\n",
       "      <td>16.844200</td>\n",
       "      <td>6.297800</td>\n",
       "      <td>18.967850</td>\n",
       "      <td>5.440050</td>\n",
       "      <td>14.388850</td>\n",
       "      <td>6.061750</td>\n",
       "      <td>14.844500</td>\n",
       "      <td>-3.284450</td>\n",
       "      <td>1.069700</td>\n",
       "      <td>-2.517950</td>\n",
       "      <td>18.296450</td>\n",
       "      <td>6.006700</td>\n",
       "      <td>11.288000</td>\n",
       "      <td>8.616200</td>\n",
       "      <td>3.642550</td>\n",
       "      <td>-16.482600</td>\n",
       "      <td>6.986500</td>\n",
       "      <td>12.673500</td>\n",
       "      <td>10.582200</td>\n",
       "      <td>10.983850</td>\n",
       "      <td>-0.098600</td>\n",
       "      <td>14.369900</td>\n",
       "      <td>18.502150</td>\n",
       "      <td>1.768300</td>\n",
       "      <td>-0.771300</td>\n",
       "      <td>-6.401500</td>\n",
       "      <td>13.380850</td>\n",
       "      <td>22.306850</td>\n",
       "      <td>1.566000</td>\n",
       "      <td>11.497950</td>\n",
       "      <td>4.224500</td>\n",
       "      <td>8.605150</td>\n",
       "      <td>17.573200</td>\n",
       "      <td>14.226600</td>\n",
       "      <td>18.281350</td>\n",
       "      <td>5.394300</td>\n",
       "      <td>6.340100</td>\n",
       "      <td>3.408400</td>\n",
       "      <td>8.148550</td>\n",
       "      <td>3.083800</td>\n",
       "      <td>2.249850</td>\n",
       "      <td>2.456300</td>\n",
       "      <td>15.944850</td>\n",
       "      <td>-5.189500</td>\n",
       "      <td>3.023950</td>\n",
       "      <td>24.354700</td>\n",
       "      <td>11.239700</td>\n",
       "      <td>1.200700</td>\n",
       "      <td>7.234300</td>\n",
       "      <td>4.302100</td>\n",
       "      <td>12.486300</td>\n",
       "      <td>13.166800</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>-1.101750</td>\n",
       "      <td>15.426800</td>\n",
       "      <td>12.264650</td>\n",
       "      <td>0.556600</td>\n",
       "      <td>7.809100</td>\n",
       "      <td>6.806700</td>\n",
       "      <td>-4.704250</td>\n",
       "      <td>-4.111900</td>\n",
       "      <td>22.948300</td>\n",
       "      <td>17.257250</td>\n",
       "      <td>1.211750</td>\n",
       "      <td>8.066250</td>\n",
       "      <td>3.564700</td>\n",
       "      <td>2.975500</td>\n",
       "      <td>11.855900</td>\n",
       "      <td>12.356350</td>\n",
       "      <td>8.651850</td>\n",
       "      <td>4.904700</td>\n",
       "      <td>10.395600</td>\n",
       "      <td>-3.178700</td>\n",
       "      <td>3.996000</td>\n",
       "      <td>5.283250</td>\n",
       "      <td>16.736950</td>\n",
       "      <td>10.127900</td>\n",
       "      <td>7.673700</td>\n",
       "      <td>16.649750</td>\n",
       "      <td>6.994050</td>\n",
       "      <td>-2.066100</td>\n",
       "      <td>13.184300</td>\n",
       "      <td>-4.868400</td>\n",
       "      <td>17.630450</td>\n",
       "      <td>10.217550</td>\n",
       "      <td>23.864500</td>\n",
       "      <td>5.633500</td>\n",
       "      <td>5.359700</td>\n",
       "      <td>10.788700</td>\n",
       "      <td>-2.637800</td>\n",
       "      <td>19.270800</td>\n",
       "      <td>2.960200</td>\n",
       "      <td>-4.011600</td>\n",
       "      <td>4.761600</td>\n",
       "      <td>5.634300</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>-0.807350</td>\n",
       "      <td>19.748000</td>\n",
       "      <td>-0.569750</td>\n",
       "      <td>20.206100</td>\n",
       "      <td>11.679800</td>\n",
       "      <td>-2.538450</td>\n",
       "      <td>11.737250</td>\n",
       "      <td>-0.942050</td>\n",
       "      <td>2.512300</td>\n",
       "      <td>-2.688800</td>\n",
       "      <td>10.036050</td>\n",
       "      <td>0.720200</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>12.521000</td>\n",
       "      <td>-3.946950</td>\n",
       "      <td>8.902150</td>\n",
       "      <td>-10.209750</td>\n",
       "      <td>15.239450</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>3.203600</td>\n",
       "      <td>7.347750</td>\n",
       "      <td>1.901300</td>\n",
       "      <td>3.396350</td>\n",
       "      <td>17.957950</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>2.408900</td>\n",
       "      <td>8.888200</td>\n",
       "      <td>15.934050</td>\n",
       "      <td>-2.819550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.758200</td>\n",
       "      <td>1.358625</td>\n",
       "      <td>12.516700</td>\n",
       "      <td>8.324100</td>\n",
       "      <td>12.261125</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.102900</td>\n",
       "      <td>2.937900</td>\n",
       "      <td>8.584425</td>\n",
       "      <td>4.382925</td>\n",
       "      <td>0.852825</td>\n",
       "      <td>14.164200</td>\n",
       "      <td>12.274775</td>\n",
       "      <td>9.270425</td>\n",
       "      <td>14.874500</td>\n",
       "      <td>11.055900</td>\n",
       "      <td>-0.810775</td>\n",
       "      <td>21.013325</td>\n",
       "      <td>18.433300</td>\n",
       "      <td>17.879400</td>\n",
       "      <td>23.089050</td>\n",
       "      <td>6.293200</td>\n",
       "      <td>3.403800</td>\n",
       "      <td>13.479600</td>\n",
       "      <td>13.863700</td>\n",
       "      <td>-0.090200</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>6.093700</td>\n",
       "      <td>6.798925</td>\n",
       "      <td>-1.443450</td>\n",
       "      <td>11.885000</td>\n",
       "      <td>1.469100</td>\n",
       "      <td>18.097125</td>\n",
       "      <td>11.844400</td>\n",
       "      <td>7.487725</td>\n",
       "      <td>4.460400</td>\n",
       "      <td>7.542400</td>\n",
       "      <td>13.598925</td>\n",
       "      <td>3.637825</td>\n",
       "      <td>-0.880875</td>\n",
       "      <td>13.754800</td>\n",
       "      <td>11.756900</td>\n",
       "      <td>11.804600</td>\n",
       "      <td>13.087300</td>\n",
       "      <td>3.150525</td>\n",
       "      <td>13.318300</td>\n",
       "      <td>-4.244525</td>\n",
       "      <td>23.028650</td>\n",
       "      <td>22.549050</td>\n",
       "      <td>13.234500</td>\n",
       "      <td>19.385650</td>\n",
       "      <td>0.944350</td>\n",
       "      <td>6.542900</td>\n",
       "      <td>7.401825</td>\n",
       "      <td>17.086625</td>\n",
       "      <td>19.289700</td>\n",
       "      <td>6.845000</td>\n",
       "      <td>6.209700</td>\n",
       "      <td>9.566525</td>\n",
       "      <td>15.116500</td>\n",
       "      <td>-3.225450</td>\n",
       "      <td>2.228200</td>\n",
       "      <td>3.020300</td>\n",
       "      <td>7.490600</td>\n",
       "      <td>3.739200</td>\n",
       "      <td>6.586200</td>\n",
       "      <td>17.037650</td>\n",
       "      <td>5.024100</td>\n",
       "      <td>-0.498875</td>\n",
       "      <td>33.633150</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>3.510700</td>\n",
       "      <td>25.207125</td>\n",
       "      <td>29.620700</td>\n",
       "      <td>21.432225</td>\n",
       "      <td>11.818800</td>\n",
       "      <td>22.041100</td>\n",
       "      <td>6.867200</td>\n",
       "      <td>15.383100</td>\n",
       "      <td>11.449125</td>\n",
       "      <td>16.340800</td>\n",
       "      <td>3.101725</td>\n",
       "      <td>7.449900</td>\n",
       "      <td>1.986700</td>\n",
       "      <td>21.358850</td>\n",
       "      <td>11.158375</td>\n",
       "      <td>15.433225</td>\n",
       "      <td>10.567025</td>\n",
       "      <td>6.146200</td>\n",
       "      <td>-6.409375</td>\n",
       "      <td>7.101400</td>\n",
       "      <td>15.840225</td>\n",
       "      <td>10.944900</td>\n",
       "      <td>13.089100</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>20.819375</td>\n",
       "      <td>28.158975</td>\n",
       "      <td>2.260900</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>17.250225</td>\n",
       "      <td>28.682225</td>\n",
       "      <td>1.705400</td>\n",
       "      <td>12.902100</td>\n",
       "      <td>4.822200</td>\n",
       "      <td>9.928900</td>\n",
       "      <td>23.348600</td>\n",
       "      <td>14.361800</td>\n",
       "      <td>21.852900</td>\n",
       "      <td>8.104325</td>\n",
       "      <td>7.080300</td>\n",
       "      <td>4.577400</td>\n",
       "      <td>11.596200</td>\n",
       "      <td>3.811900</td>\n",
       "      <td>4.121500</td>\n",
       "      <td>3.665100</td>\n",
       "      <td>25.780825</td>\n",
       "      <td>0.971800</td>\n",
       "      <td>6.098400</td>\n",
       "      <td>33.105275</td>\n",
       "      <td>12.619425</td>\n",
       "      <td>5.091700</td>\n",
       "      <td>11.734750</td>\n",
       "      <td>6.192200</td>\n",
       "      <td>12.718100</td>\n",
       "      <td>13.811700</td>\n",
       "      <td>3.293000</td>\n",
       "      <td>1.351700</td>\n",
       "      <td>18.480400</td>\n",
       "      <td>12.876700</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>8.911425</td>\n",
       "      <td>7.070800</td>\n",
       "      <td>-0.178800</td>\n",
       "      <td>1.125950</td>\n",
       "      <td>31.042425</td>\n",
       "      <td>24.426025</td>\n",
       "      <td>4.391225</td>\n",
       "      <td>13.232525</td>\n",
       "      <td>7.078525</td>\n",
       "      <td>8.192425</td>\n",
       "      <td>16.073925</td>\n",
       "      <td>14.461050</td>\n",
       "      <td>9.315000</td>\n",
       "      <td>7.676925</td>\n",
       "      <td>12.113225</td>\n",
       "      <td>2.028275</td>\n",
       "      <td>4.131600</td>\n",
       "      <td>12.688225</td>\n",
       "      <td>18.682500</td>\n",
       "      <td>13.057600</td>\n",
       "      <td>9.817300</td>\n",
       "      <td>18.263900</td>\n",
       "      <td>10.766350</td>\n",
       "      <td>1.891750</td>\n",
       "      <td>13.929300</td>\n",
       "      <td>-0.988575</td>\n",
       "      <td>23.875325</td>\n",
       "      <td>13.094525</td>\n",
       "      <td>32.622850</td>\n",
       "      <td>5.792000</td>\n",
       "      <td>6.371200</td>\n",
       "      <td>14.623900</td>\n",
       "      <td>1.323600</td>\n",
       "      <td>23.024025</td>\n",
       "      <td>3.241500</td>\n",
       "      <td>1.318725</td>\n",
       "      <td>7.020025</td>\n",
       "      <td>5.905400</td>\n",
       "      <td>3.096400</td>\n",
       "      <td>2.956800</td>\n",
       "      <td>25.907725</td>\n",
       "      <td>3.619900</td>\n",
       "      <td>25.641225</td>\n",
       "      <td>13.745500</td>\n",
       "      <td>2.704400</td>\n",
       "      <td>13.931300</td>\n",
       "      <td>5.338750</td>\n",
       "      <td>4.391125</td>\n",
       "      <td>0.996200</td>\n",
       "      <td>11.011300</td>\n",
       "      <td>7.499175</td>\n",
       "      <td>12.127425</td>\n",
       "      <td>19.456150</td>\n",
       "      <td>-0.590650</td>\n",
       "      <td>11.193800</td>\n",
       "      <td>-1.466000</td>\n",
       "      <td>18.345225</td>\n",
       "      <td>1.482900</td>\n",
       "      <td>6.406200</td>\n",
       "      <td>9.512525</td>\n",
       "      <td>2.949500</td>\n",
       "      <td>6.205800</td>\n",
       "      <td>20.396525</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>6.556725</td>\n",
       "      <td>9.593300</td>\n",
       "      <td>18.064725</td>\n",
       "      <td>4.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.315000</td>\n",
       "      <td>10.376800</td>\n",
       "      <td>19.353000</td>\n",
       "      <td>13.188300</td>\n",
       "      <td>16.671400</td>\n",
       "      <td>17.251600</td>\n",
       "      <td>8.447700</td>\n",
       "      <td>27.691800</td>\n",
       "      <td>10.151300</td>\n",
       "      <td>11.150600</td>\n",
       "      <td>18.670200</td>\n",
       "      <td>17.188700</td>\n",
       "      <td>14.654500</td>\n",
       "      <td>22.331500</td>\n",
       "      <td>14.937700</td>\n",
       "      <td>15.863300</td>\n",
       "      <td>17.950600</td>\n",
       "      <td>19.025900</td>\n",
       "      <td>41.748000</td>\n",
       "      <td>35.183000</td>\n",
       "      <td>31.285900</td>\n",
       "      <td>49.044300</td>\n",
       "      <td>14.594500</td>\n",
       "      <td>4.875200</td>\n",
       "      <td>25.446000</td>\n",
       "      <td>14.654600</td>\n",
       "      <td>15.675100</td>\n",
       "      <td>3.243100</td>\n",
       "      <td>8.787400</td>\n",
       "      <td>13.143100</td>\n",
       "      <td>15.651500</td>\n",
       "      <td>20.171900</td>\n",
       "      <td>6.787100</td>\n",
       "      <td>29.546600</td>\n",
       "      <td>13.287800</td>\n",
       "      <td>21.528900</td>\n",
       "      <td>14.245600</td>\n",
       "      <td>11.863800</td>\n",
       "      <td>29.823500</td>\n",
       "      <td>15.322300</td>\n",
       "      <td>18.105600</td>\n",
       "      <td>26.165800</td>\n",
       "      <td>13.469600</td>\n",
       "      <td>12.577900</td>\n",
       "      <td>34.196100</td>\n",
       "      <td>62.084400</td>\n",
       "      <td>21.293900</td>\n",
       "      <td>20.685400</td>\n",
       "      <td>54.273800</td>\n",
       "      <td>41.153000</td>\n",
       "      <td>15.317200</td>\n",
       "      <td>40.689000</td>\n",
       "      <td>17.096800</td>\n",
       "      <td>8.231500</td>\n",
       "      <td>28.572400</td>\n",
       "      <td>29.092100</td>\n",
       "      <td>29.074100</td>\n",
       "      <td>9.160900</td>\n",
       "      <td>20.483300</td>\n",
       "      <td>11.986700</td>\n",
       "      <td>25.195500</td>\n",
       "      <td>27.102900</td>\n",
       "      <td>7.753600</td>\n",
       "      <td>11.231700</td>\n",
       "      <td>11.153700</td>\n",
       "      <td>15.731300</td>\n",
       "      <td>9.713200</td>\n",
       "      <td>39.396800</td>\n",
       "      <td>5.046900</td>\n",
       "      <td>8.547300</td>\n",
       "      <td>64.464400</td>\n",
       "      <td>1.571900</td>\n",
       "      <td>14.150000</td>\n",
       "      <td>44.536100</td>\n",
       "      <td>70.272000</td>\n",
       "      <td>36.156700</td>\n",
       "      <td>34.435200</td>\n",
       "      <td>30.956900</td>\n",
       "      <td>11.350700</td>\n",
       "      <td>18.225600</td>\n",
       "      <td>30.476900</td>\n",
       "      <td>23.132400</td>\n",
       "      <td>21.893400</td>\n",
       "      <td>27.714300</td>\n",
       "      <td>17.742400</td>\n",
       "      <td>32.901100</td>\n",
       "      <td>34.563700</td>\n",
       "      <td>33.354100</td>\n",
       "      <td>17.459400</td>\n",
       "      <td>15.481600</td>\n",
       "      <td>27.271300</td>\n",
       "      <td>7.489500</td>\n",
       "      <td>26.997600</td>\n",
       "      <td>12.534300</td>\n",
       "      <td>18.975000</td>\n",
       "      <td>1.804000</td>\n",
       "      <td>40.880600</td>\n",
       "      <td>58.287900</td>\n",
       "      <td>4.502800</td>\n",
       "      <td>5.076400</td>\n",
       "      <td>25.140900</td>\n",
       "      <td>28.459400</td>\n",
       "      <td>51.326500</td>\n",
       "      <td>2.188700</td>\n",
       "      <td>19.020600</td>\n",
       "      <td>7.169200</td>\n",
       "      <td>15.307400</td>\n",
       "      <td>46.379500</td>\n",
       "      <td>14.743000</td>\n",
       "      <td>32.059100</td>\n",
       "      <td>19.519300</td>\n",
       "      <td>9.800200</td>\n",
       "      <td>8.431700</td>\n",
       "      <td>21.542100</td>\n",
       "      <td>6.585000</td>\n",
       "      <td>11.950400</td>\n",
       "      <td>8.120700</td>\n",
       "      <td>64.810900</td>\n",
       "      <td>25.263500</td>\n",
       "      <td>15.688500</td>\n",
       "      <td>74.032100</td>\n",
       "      <td>17.307400</td>\n",
       "      <td>18.471400</td>\n",
       "      <td>26.874900</td>\n",
       "      <td>14.991500</td>\n",
       "      <td>13.664200</td>\n",
       "      <td>15.515600</td>\n",
       "      <td>10.597600</td>\n",
       "      <td>9.809600</td>\n",
       "      <td>31.203600</td>\n",
       "      <td>14.989500</td>\n",
       "      <td>2.192300</td>\n",
       "      <td>12.465000</td>\n",
       "      <td>8.309100</td>\n",
       "      <td>12.723600</td>\n",
       "      <td>21.412800</td>\n",
       "      <td>54.579400</td>\n",
       "      <td>44.437600</td>\n",
       "      <td>18.818700</td>\n",
       "      <td>36.097100</td>\n",
       "      <td>21.121900</td>\n",
       "      <td>23.965800</td>\n",
       "      <td>32.891100</td>\n",
       "      <td>22.691600</td>\n",
       "      <td>11.810100</td>\n",
       "      <td>16.008300</td>\n",
       "      <td>20.437300</td>\n",
       "      <td>22.149400</td>\n",
       "      <td>4.752800</td>\n",
       "      <td>48.424000</td>\n",
       "      <td>25.435700</td>\n",
       "      <td>21.124500</td>\n",
       "      <td>18.384600</td>\n",
       "      <td>24.007500</td>\n",
       "      <td>23.242800</td>\n",
       "      <td>16.831600</td>\n",
       "      <td>16.497000</td>\n",
       "      <td>11.972100</td>\n",
       "      <td>44.779500</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>58.394200</td>\n",
       "      <td>6.309900</td>\n",
       "      <td>10.134400</td>\n",
       "      <td>27.564800</td>\n",
       "      <td>12.119300</td>\n",
       "      <td>38.332200</td>\n",
       "      <td>4.220400</td>\n",
       "      <td>21.276600</td>\n",
       "      <td>14.886100</td>\n",
       "      <td>7.089000</td>\n",
       "      <td>16.731900</td>\n",
       "      <td>17.917300</td>\n",
       "      <td>53.591900</td>\n",
       "      <td>18.855400</td>\n",
       "      <td>43.546800</td>\n",
       "      <td>20.854800</td>\n",
       "      <td>20.245200</td>\n",
       "      <td>20.596500</td>\n",
       "      <td>29.841300</td>\n",
       "      <td>13.448700</td>\n",
       "      <td>12.750500</td>\n",
       "      <td>14.393900</td>\n",
       "      <td>29.248700</td>\n",
       "      <td>23.704900</td>\n",
       "      <td>44.363400</td>\n",
       "      <td>12.997500</td>\n",
       "      <td>21.739200</td>\n",
       "      <td>22.786100</td>\n",
       "      <td>29.330300</td>\n",
       "      <td>4.034100</td>\n",
       "      <td>18.440900</td>\n",
       "      <td>16.716500</td>\n",
       "      <td>8.402400</td>\n",
       "      <td>18.281800</td>\n",
       "      <td>27.928800</td>\n",
       "      <td>4.272900</td>\n",
       "      <td>18.321500</td>\n",
       "      <td>12.000400</td>\n",
       "      <td>26.079100</td>\n",
       "      <td>28.500700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              target          var_0          var_1          var_2  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.100490      10.679914      -1.627622      10.715192   \n",
       "std         0.300653       3.040051       4.050044       2.640894   \n",
       "min         0.000000       0.408400     -15.043400       2.117100   \n",
       "25%         0.000000       8.453850      -4.740025       8.722475   \n",
       "50%         0.000000      10.524750      -1.608050      10.580000   \n",
       "75%         0.000000      12.758200       1.358625      12.516700   \n",
       "max         1.000000      20.315000      10.376800      19.353000   \n",
       "\n",
       "               var_3          var_4          var_5          var_6  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.796529      11.078333      -5.065317       5.408949   \n",
       "std         2.043319       1.623150       7.863267       0.866607   \n",
       "min        -0.040200       5.074800     -32.562600       2.347300   \n",
       "25%         5.254075       9.883175     -11.200350       4.767700   \n",
       "50%         6.825000      11.108250      -4.833150       5.385100   \n",
       "75%         8.324100      12.261125       0.924800       6.003000   \n",
       "max        13.188300      16.671400      17.251600       8.447700   \n",
       "\n",
       "               var_7          var_8          var_9         var_10  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       16.545850       0.284162       7.567236       0.394340   \n",
       "std         3.418076       3.332634       1.235070       5.500793   \n",
       "min         5.349700     -10.505500       3.970500     -20.731300   \n",
       "25%        13.943800      -2.317800       6.618800      -3.594950   \n",
       "50%        16.456800       0.393700       7.629600       0.487300   \n",
       "75%        19.102900       2.937900       8.584425       4.382925   \n",
       "max        27.691800      10.151300      11.150600      18.670200   \n",
       "\n",
       "              var_11         var_12         var_13         var_14  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -3.245596      14.023978       8.530232       7.537606   \n",
       "std         5.970253       0.190059       4.639536       2.247908   \n",
       "min       -26.095000      13.434600      -6.011100       1.013300   \n",
       "25%        -7.510600      13.894000       5.072800       5.781875   \n",
       "50%        -3.286950      14.025500       8.604250       7.520300   \n",
       "75%         0.852825      14.164200      12.274775       9.270425   \n",
       "max        17.188700      14.654500      22.331500      14.937700   \n",
       "\n",
       "              var_15         var_16         var_17         var_18  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       14.573126       9.333264      -5.696731      15.244013   \n",
       "std         0.411711       2.557421       6.712612       7.851370   \n",
       "min        13.076900       0.635100     -33.380200     -10.664200   \n",
       "25%        14.262800       7.452275     -10.476225       9.177950   \n",
       "50%        14.574100       9.232050      -5.666350      15.196250   \n",
       "75%        14.874500      11.055900      -0.810775      21.013325   \n",
       "max        15.863300      17.950600      19.025900      41.748000   \n",
       "\n",
       "              var_19         var_20         var_21         var_22  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       12.438567      13.290894      17.257883       4.305430   \n",
       "std         7.996694       5.876254       8.196564       2.847958   \n",
       "min       -12.402500      -5.432200     -10.089000      -5.322500   \n",
       "25%         6.276475       8.627800      11.551000       2.182400   \n",
       "50%        12.453900      13.196800      17.234250       4.275150   \n",
       "75%        18.433300      17.879400      23.089050       6.293200   \n",
       "max        35.183000      31.285900      49.044300      14.594500   \n",
       "\n",
       "              var_23         var_24         var_25         var_26  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        3.019540      10.584400      13.667496      -4.055133   \n",
       "std         0.526893       3.777245       0.285535       5.922210   \n",
       "min         1.209800      -0.678400      12.720000     -24.243100   \n",
       "25%         2.634100       7.613000      13.456400      -8.321725   \n",
       "50%         3.008650      10.380350      13.662500      -4.196900   \n",
       "75%         3.403800      13.479600      13.863700      -0.090200   \n",
       "max         4.875200      25.446000      14.654600      15.675100   \n",
       "\n",
       "              var_27         var_28         var_29         var_30  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -1.137908       5.532980       5.053874      -7.687740   \n",
       "std         1.523714       0.783367       2.615942       7.965198   \n",
       "min        -6.166800       2.089600      -4.787200     -34.798400   \n",
       "25%        -2.307900       4.992100       3.171700     -13.766175   \n",
       "50%        -1.132100       5.534850       4.950200      -7.411750   \n",
       "75%         0.015625       6.093700       6.798925      -1.443450   \n",
       "max         3.243100       8.787400      13.143100      15.651500   \n",
       "\n",
       "              var_31         var_32         var_33         var_34  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       10.393046      -0.512886      14.774147      11.434250   \n",
       "std         2.159891       2.587830       4.322325       0.541614   \n",
       "min         2.140600      -8.986100       1.508500       9.816900   \n",
       "25%         8.870000      -2.500875      11.456300      11.032300   \n",
       "50%        10.365650      -0.497650      14.576000      11.435200   \n",
       "75%        11.885000       1.469100      18.097125      11.844400   \n",
       "max        20.171900       6.787100      29.546600      13.287800   \n",
       "\n",
       "              var_35         var_36         var_37         var_38  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        3.842499       2.187230       5.868899      10.642131   \n",
       "std         5.179559       3.119978       2.249730       4.278903   \n",
       "min       -16.513600      -8.095100      -1.183400      -6.337100   \n",
       "25%         0.116975      -0.007125       4.125475       7.591050   \n",
       "50%         3.917750       2.198000       5.900650      10.562700   \n",
       "75%         7.487725       4.460400       7.542400      13.598925   \n",
       "max        21.528900      14.245600      11.863800      29.823500   \n",
       "\n",
       "              var_39         var_40         var_41         var_42  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.662956      -6.725505       9.299858      11.222356   \n",
       "std         4.068845       8.279259       5.938088       0.695991   \n",
       "min       -14.545700     -35.211700      -8.535900       8.859000   \n",
       "25%        -2.199500     -12.831825       4.519575      10.713200   \n",
       "50%         0.672300      -6.617450       9.162650      11.243400   \n",
       "75%         3.637825      -0.880875      13.754800      11.756900   \n",
       "max        15.322300      18.105600      26.165800      13.469600   \n",
       "\n",
       "              var_43         var_44         var_45         var_46  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.569954       8.948289     -12.699667      11.326488   \n",
       "std         0.309599       5.903073      21.404912       2.860511   \n",
       "min        10.652800      -9.939600     -90.252500       1.206200   \n",
       "25%        11.343800       5.313650     -28.730700       9.248750   \n",
       "50%        11.565000       9.437200     -12.547200      11.310750   \n",
       "75%        11.804600      13.087300       3.150525      13.318300   \n",
       "max        12.577900      34.196100      62.084400      21.293900   \n",
       "\n",
       "              var_47         var_48         var_49         var_50  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean      -12.471737      14.704713      16.682499      12.740986   \n",
       "std        10.579862      11.384332       7.855762       0.691709   \n",
       "min       -47.686200     -23.902200      -8.070700      10.385500   \n",
       "25%       -20.654525       6.351975      10.653475      12.269000   \n",
       "50%       -12.482400      14.559200      16.672400      12.745600   \n",
       "75%        -4.244525      23.028650      22.549050      13.234500   \n",
       "max        20.685400      54.273800      41.153000      15.317200   \n",
       "\n",
       "              var_51         var_52         var_53         var_54  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       13.428912      -2.528816       6.008569       1.137117   \n",
       "std         8.187306       4.985532       0.764753       8.414241   \n",
       "min       -15.046200     -24.721400       3.344900     -26.778600   \n",
       "25%         7.267625      -6.065025       5.435600      -5.147625   \n",
       "50%        13.444400      -2.502450       6.027800       1.274050   \n",
       "75%        19.385650       0.944350       6.542900       7.401825   \n",
       "max        40.689000      17.096800       8.231500      28.572400   \n",
       "\n",
       "              var_55         var_56         var_57         var_58  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       12.745852      16.629165       6.272014       3.177633   \n",
       "std         5.690072       3.540174       0.795026       4.296686   \n",
       "min        -3.782600       2.761800       3.442300     -12.600900   \n",
       "25%         8.163900      14.097875       5.687500       0.183500   \n",
       "50%        12.594100      16.648150       6.262500       3.170100   \n",
       "75%        17.086625      19.289700       6.845000       6.209700   \n",
       "max        29.092100      29.074100       9.160900      20.483300   \n",
       "\n",
       "              var_59         var_60         var_61         var_62  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        8.931124      12.155618     -11.946744       0.874170   \n",
       "std         0.854798       4.222389      11.622948       2.026238   \n",
       "min         6.184000      -2.100600     -48.802700      -6.328900   \n",
       "25%         8.312400       8.912750     -20.901725      -0.572400   \n",
       "50%         8.901000      12.064350     -11.892000       0.794700   \n",
       "75%         9.566525      15.116500      -3.225450       2.228200   \n",
       "max        11.986700      25.195500      27.102900       7.753600   \n",
       "\n",
       "              var_63         var_64         var_65         var_66  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.661173       6.369157       0.982891       5.794039   \n",
       "std         3.113089       1.485854       3.786493       1.121366   \n",
       "min       -10.554400       1.611700     -14.088800       1.336800   \n",
       "25%        -1.588700       5.293500      -1.702800       4.973800   \n",
       "50%         0.681700       6.377700       1.021350       5.782000   \n",
       "75%         3.020300       7.490600       3.739200       6.586200   \n",
       "max        11.231700      11.153700      15.731300       9.713200   \n",
       "\n",
       "              var_67         var_68         var_69         var_70  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.943223       5.018893      -3.331515      24.446811   \n",
       "std         7.365115       0.007186       3.955723      11.951742   \n",
       "min       -19.544300       4.993800     -16.309400     -17.027500   \n",
       "25%         6.753200       5.014000      -6.336625      15.256625   \n",
       "50%        11.922000       5.019100      -3.325500      24.445000   \n",
       "75%        17.037650       5.024100      -0.498875      33.633150   \n",
       "max        39.396800       5.046900       8.547300      64.464400   \n",
       "\n",
       "              var_71         var_72         var_73         var_74  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.669756       0.640553      19.610888      19.518846   \n",
       "std         0.266696       3.944703       7.466303      14.112591   \n",
       "min        -0.224000     -12.383400      -1.665800     -34.101500   \n",
       "25%         0.472300      -2.197100      14.097275       9.595975   \n",
       "50%         0.668400       0.646450      19.309750      19.536650   \n",
       "75%         0.864400       3.510700      25.207125      29.620700   \n",
       "max         1.571900      14.150000      44.536100      70.272000   \n",
       "\n",
       "              var_75         var_76         var_77         var_78  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       16.853732       6.050871      19.066993       5.349479   \n",
       "std         6.055322       7.938351       3.817292       1.993792   \n",
       "min        -1.293600     -21.633300       7.425700      -1.818300   \n",
       "25%        12.480975       0.596300      16.014700       3.817275   \n",
       "50%        16.844200       6.297800      18.967850       5.440050   \n",
       "75%        21.432225      11.818800      22.041100       6.867200   \n",
       "max        36.156700      34.435200      30.956900      11.350700   \n",
       "\n",
       "              var_79         var_80         var_81         var_82  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       14.402136       5.795044      14.719024      -3.471273   \n",
       "std         1.309055       7.436737       2.299567       8.479255   \n",
       "min        10.445400     -18.042200       7.586500     -30.026600   \n",
       "25%        13.375400       0.694475      13.214775     -10.004950   \n",
       "50%        14.388850       6.061750      14.844500      -3.284450   \n",
       "75%        15.383100      11.449125      16.340800       3.101725   \n",
       "max        18.225600      30.476900      23.132400      21.893400   \n",
       "\n",
       "              var_83         var_84         var_85         var_86  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.025817      -2.590209      18.362721       5.621058   \n",
       "std         8.297229       6.225305       3.908536       7.751142   \n",
       "min       -24.220100     -24.439800       7.023000     -19.272200   \n",
       "25%        -5.106400      -7.216125      15.338575       0.407550   \n",
       "50%         1.069700      -2.517950      18.296450       6.006700   \n",
       "75%         7.449900       1.986700      21.358850      11.158375   \n",
       "max        27.714300      17.742400      32.901100      34.563700   \n",
       "\n",
       "              var_87         var_88         var_89         var_90  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.351483       8.702924       3.725208     -16.548147   \n",
       "std         5.661867       2.491460       3.560554      13.152810   \n",
       "min        -8.481600       1.350200      -9.601400     -61.718000   \n",
       "25%         7.247175       6.918775       1.140500     -26.665600   \n",
       "50%        11.288000       8.616200       3.642550     -16.482600   \n",
       "75%        15.433225      10.567025       6.146200      -6.409375   \n",
       "max        33.354100      17.459400      15.481600      27.271300   \n",
       "\n",
       "              var_91         var_92         var_93         var_94  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.987541      12.739578      10.556740      10.999162   \n",
       "std         0.152641       4.186252       0.543341       2.768099   \n",
       "min         6.521800      -1.018500       8.491600       2.819000   \n",
       "25%         6.869900       9.670300      10.195600       8.828000   \n",
       "50%         6.986500      12.673500      10.582200      10.983850   \n",
       "75%         7.101400      15.840225      10.944900      13.089100   \n",
       "max         7.489500      26.997600      12.534300      18.975000   \n",
       "\n",
       "              var_95         var_96         var_97         var_98  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.084344      14.400433      18.539645       1.752012   \n",
       "std         0.621125       8.525400      12.642382       0.715836   \n",
       "min        -2.432400     -12.158400     -21.740000      -0.603500   \n",
       "25%        -0.527400       7.796950       8.919525       1.267675   \n",
       "50%        -0.098600      14.369900      18.502150       1.768300   \n",
       "75%         0.329100      20.819375      28.158975       2.260900   \n",
       "max         1.804000      40.880600      58.287900       4.502800   \n",
       "\n",
       "              var_99        var_100        var_101        var_102  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.746296      -6.600518      13.413526      22.294908   \n",
       "std         1.862550       9.181683       4.950537       8.628179   \n",
       "min        -7.280600     -39.179100       0.075700      -7.382900   \n",
       "25%        -2.106200     -13.198700       9.639800      16.047975   \n",
       "50%        -0.771300      -6.401500      13.380850      22.306850   \n",
       "75%         0.528500       0.132100      17.250225      28.682225   \n",
       "max         5.076400      25.140900      28.459400      51.326500   \n",
       "\n",
       "             var_103        var_104        var_105        var_106  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.568393      11.509834       4.244744       8.617657   \n",
       "std         0.185020       1.970520       0.855698       1.894899   \n",
       "min         0.979300       4.084600       0.715300       0.942400   \n",
       "25%         1.428900      10.097900       3.639600       7.282300   \n",
       "50%         1.566000      11.497950       4.224500       8.605150   \n",
       "75%         1.705400      12.902100       4.822200       9.928900   \n",
       "max         2.188700      19.020600       7.169200      15.307400   \n",
       "\n",
       "             var_107        var_108        var_109        var_110  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       17.796266      14.224435      18.458001       5.513238   \n",
       "std         7.604723       0.171091       4.355031       3.823253   \n",
       "min        -5.898000      13.729000       5.769700      -9.239800   \n",
       "25%        12.168075      14.098900      15.107175       2.817475   \n",
       "50%        17.573200      14.226600      18.281350       5.394300   \n",
       "75%        23.348600      14.361800      21.852900       8.104325   \n",
       "max        46.379500      14.743000      32.059100      19.519300   \n",
       "\n",
       "             var_111        var_112        var_113        var_114  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.312603       3.317843       8.136542       3.081191   \n",
       "std         1.082404       1.591170       4.459077       0.985396   \n",
       "min         2.194200      -2.030200      -5.513900      -0.050500   \n",
       "25%         5.510100       2.092675       4.803250       2.388775   \n",
       "50%         6.340100       3.408400       8.148550       3.083800   \n",
       "75%         7.080300       4.577400      11.596200       3.811900   \n",
       "max         9.800200       8.431700      21.542100       6.585000   \n",
       "\n",
       "             var_115        var_116        var_117        var_118  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        2.213717       2.402570      16.102233      -5.305132   \n",
       "std         2.621851       1.650912      13.297662       8.799268   \n",
       "min        -6.858600      -3.163000     -31.836900     -37.527700   \n",
       "25%         0.399700       1.171875       6.373500     -11.587850   \n",
       "50%         2.249850       2.456300      15.944850      -5.189500   \n",
       "75%         4.121500       3.665100      25.780825       0.971800   \n",
       "max        11.950400       8.120700      64.810900      25.263500   \n",
       "\n",
       "             var_119        var_120        var_121        var_122  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        3.032849      24.521078      11.310591       1.192984   \n",
       "std         4.182796      12.121016       1.714416       5.168479   \n",
       "min        -9.774200     -18.696200       6.305200     -15.194000   \n",
       "25%        -0.161975      15.696275       9.996400      -2.565200   \n",
       "50%         3.023950      24.354700      11.239700       1.200700   \n",
       "75%         6.098400      33.105275      12.619425       5.091700   \n",
       "max        15.688500      74.032100      17.307400      18.471400   \n",
       "\n",
       "             var_123        var_124        var_125        var_126  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        7.076254       4.272740      12.489165      13.202326   \n",
       "std         6.147345       2.736821       0.318100       0.776056   \n",
       "min       -12.405900      -7.053800      11.486100      11.265400   \n",
       "25%         2.817050       2.353600      12.245400      12.608400   \n",
       "50%         7.234300       4.302100      12.486300      13.166800   \n",
       "75%        11.734750       6.192200      12.718100      13.811700   \n",
       "max        26.874900      14.991500      13.664200      15.515600   \n",
       "\n",
       "             var_127        var_128        var_129        var_130  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.851507      -1.127952      15.460314      12.257151   \n",
       "std         3.137684       3.238043       4.136453       0.832199   \n",
       "min        -8.876900     -11.755900       2.186300       9.528300   \n",
       "25%        -1.502325      -3.580725      12.514475      11.619300   \n",
       "50%         0.925000      -1.101750      15.426800      12.264650   \n",
       "75%         3.293000       1.351700      18.480400      12.876700   \n",
       "max        10.597600       9.809600      31.203600      14.989500   \n",
       "\n",
       "             var_131        var_132        var_133        var_134  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.544674       7.799676       6.813270      -4.826053   \n",
       "std         0.456280       1.456486       0.375603       6.166126   \n",
       "min        -0.954800       2.890000       5.359300     -24.254600   \n",
       "25%         0.207800       6.724375       6.543500      -9.625700   \n",
       "50%         0.556600       7.809100       6.806700      -4.704250   \n",
       "75%         0.901000       8.911425       7.070800      -0.178800   \n",
       "max         2.192300      12.465000       8.309100      12.723600   \n",
       "\n",
       "             var_135        var_136        var_137        var_138  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -4.259472      22.968602      17.613651       1.210792   \n",
       "std         7.617732      10.382235       8.890516       4.551750   \n",
       "min       -31.380800      -9.949300      -9.851000     -16.468400   \n",
       "25%        -9.957100      14.933900      10.656550      -2.011825   \n",
       "50%        -4.111900      22.948300      17.257250       1.211750   \n",
       "75%         1.125950      31.042425      24.426025       4.391225   \n",
       "max        21.412800      54.579400      44.437600      18.818700   \n",
       "\n",
       "             var_139        var_140        var_141        var_142  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        7.760193       3.423636       2.897596      11.983489   \n",
       "std         7.686433       4.896325       6.715637       5.691936   \n",
       "min       -21.274300     -15.459500     -16.693700      -7.108000   \n",
       "25%         2.387575      -0.121700      -2.153725       7.900000   \n",
       "50%         8.066250       3.564700       2.975500      11.855900   \n",
       "75%        13.232525       7.078525       8.192425      16.073925   \n",
       "max        36.097100      21.121900      23.965800      32.891100   \n",
       "\n",
       "             var_143        var_144        var_145        var_146  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       12.333698       8.647632       4.841328      10.341178   \n",
       "std         2.934706       0.922469       3.899281       2.518883   \n",
       "min         2.806800       5.444300      -8.273400       0.427400   \n",
       "25%        10.311200       7.968075       1.885875       8.646900   \n",
       "50%        12.356350       8.651850       4.904700      10.395600   \n",
       "75%        14.461050       9.315000       7.676925      12.113225   \n",
       "max        22.691600      11.810100      16.008300      20.437300   \n",
       "\n",
       "             var_147        var_148        var_149        var_150  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -3.300779       3.990726       5.296237      16.817671   \n",
       "std         7.413301       0.199192      10.385133       2.464157   \n",
       "min       -29.984000       3.320500     -41.168300       9.242000   \n",
       "25%        -8.751450       3.853600      -1.903200      14.952200   \n",
       "50%        -3.178700       3.996000       5.283250      16.736950   \n",
       "75%         2.028275       4.131600      12.688225      18.682500   \n",
       "max        22.149400       4.752800      48.424000      25.435700   \n",
       "\n",
       "             var_151        var_152        var_153        var_154  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       10.141542       7.633199      16.727902       6.974955   \n",
       "std         3.962426       3.005373       2.014200       4.961678   \n",
       "min        -2.191500      -2.880000      11.030800      -8.196600   \n",
       "25%         7.064600       5.567900      15.233000       3.339900   \n",
       "50%        10.127900       7.673700      16.649750       6.994050   \n",
       "75%        13.057600       9.817300      18.263900      10.766350   \n",
       "max        21.124500      18.384600      24.007500      23.242800   \n",
       "\n",
       "             var_155        var_156        var_157        var_158  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -2.074128      13.209272      -4.813552      17.914591   \n",
       "std         5.771261       0.955140       5.570272       7.885579   \n",
       "min       -21.840900       9.996500     -22.990400      -4.554400   \n",
       "25%        -6.266025      12.475100      -8.939950      12.109200   \n",
       "50%        -2.066100      13.184300      -4.868400      17.630450   \n",
       "75%         1.891750      13.929300      -0.988575      23.875325   \n",
       "max        16.831600      16.497000      11.972100      44.779500   \n",
       "\n",
       "             var_159        var_160        var_161        var_162  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       10.223282      24.259300       5.633293       5.362896   \n",
       "std         4.122912      10.880263       0.217938       1.419612   \n",
       "min        -4.641600      -7.452200       4.852600       0.623100   \n",
       "25%         7.243525      15.696125       5.470500       4.326100   \n",
       "50%        10.217550      23.864500       5.633500       5.359700   \n",
       "75%        13.094525      32.622850       5.792000       6.371200   \n",
       "max        25.120000      58.394200       6.309900      10.134400   \n",
       "\n",
       "             var_163        var_164        var_165        var_166  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.002170      -2.871906      19.315753       2.963335   \n",
       "std         5.262056       5.457784       5.024182       0.369684   \n",
       "min        -6.531700     -19.997700       3.816700       1.851200   \n",
       "25%         7.029600      -7.094025      15.744550       2.699000   \n",
       "50%        10.788700      -2.637800      19.270800       2.960200   \n",
       "75%        14.623900       1.323600      23.024025       3.241500   \n",
       "max        27.564800      12.119300      38.332200       4.220400   \n",
       "\n",
       "             var_167        var_168        var_169        var_170  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -4.151155       4.937124       5.636008      -0.004962   \n",
       "std         7.798020       3.105986       0.369437       4.424621   \n",
       "min       -35.969500      -5.250200       4.258800     -14.506000   \n",
       "25%        -9.643100       2.703200       5.374600      -3.258500   \n",
       "50%        -4.011600       4.761600       5.634300       0.002800   \n",
       "75%         1.318725       7.020025       5.905400       3.096400   \n",
       "max        21.276600      14.886100       7.089000      16.731900   \n",
       "\n",
       "             var_171        var_172        var_173        var_174  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.831777      19.817094      -0.677967      20.210677   \n",
       "std         5.378008       8.674171       5.966674       7.136427   \n",
       "min       -22.479300     -11.453300     -22.748700      -2.995300   \n",
       "25%        -4.720350      13.731775      -5.009525      15.064600   \n",
       "50%        -0.807350      19.748000      -0.569750      20.206100   \n",
       "75%         2.956800      25.907725       3.619900      25.641225   \n",
       "max        17.917300      53.591900      18.855400      43.546800   \n",
       "\n",
       "             var_175        var_176        var_177        var_178  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       11.640613      -2.799585      11.882933      -1.014064   \n",
       "std         2.892167       7.513939       2.628895       8.579810   \n",
       "min         3.241500     -29.116500       4.952100     -29.273400   \n",
       "25%         9.371600      -8.386500       9.808675      -7.395700   \n",
       "50%        11.679800      -2.538450      11.737250      -0.942050   \n",
       "75%        13.745500       2.704400      13.931300       5.338750   \n",
       "max        20.854800      20.245200      20.596500      29.841300   \n",
       "\n",
       "             var_179        var_180        var_181        var_182  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        2.591444      -2.741666      10.085518       0.719109   \n",
       "std         2.798956       5.261243       1.371862       8.963434   \n",
       "min        -7.856100     -22.037400       5.416500     -26.001100   \n",
       "25%         0.625575      -6.673900       9.084700      -6.064425   \n",
       "50%         2.512300      -2.688800      10.036050       0.720200   \n",
       "75%         4.391125       0.996200      11.011300       7.499175   \n",
       "max        13.448700      12.750500      14.393900      29.248700   \n",
       "\n",
       "             var_183        var_184        var_185        var_186  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        8.769088      12.756676      -3.983261       8.970274   \n",
       "std         4.474924       9.318280       4.725167       3.189759   \n",
       "min        -4.808200     -18.489700     -22.583300      -3.022300   \n",
       "25%         5.423100       5.663300      -7.360000       6.715200   \n",
       "50%         8.600000      12.521000      -3.946950       8.902150   \n",
       "75%        12.127425      19.456150      -0.590650      11.193800   \n",
       "max        23.704900      44.363400      12.997500      21.739200   \n",
       "\n",
       "             var_187        var_188        var_189        var_190  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean      -10.335043      15.377174       0.746072       3.234440   \n",
       "std        11.574708       3.944604       0.976348       4.559922   \n",
       "min       -47.753600       4.412300      -2.554300     -14.093300   \n",
       "25%       -19.205125      12.501550       0.014900      -0.058825   \n",
       "50%       -10.209750      15.239450       0.742600       3.203600   \n",
       "75%        -1.466000      18.345225       1.482900       6.406200   \n",
       "max        22.786100      29.330300       4.034100      18.440900   \n",
       "\n",
       "             var_191        var_192        var_193        var_194  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        7.438408       1.927839       3.331774      17.993784   \n",
       "std         3.023272       1.478423       3.992030       3.135162   \n",
       "min        -2.691700      -3.814500     -11.783400       8.694400   \n",
       "25%         5.157400       0.889775       0.584600      15.629800   \n",
       "50%         7.347750       1.901300       3.396350      17.957950   \n",
       "75%         9.512525       2.949500       6.205800      20.396525   \n",
       "max        16.716500       8.402400      18.281800      27.928800   \n",
       "\n",
       "             var_195        var_196        var_197        var_198  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.142088       2.303335       8.908158      15.870720   \n",
       "std         1.429372       5.454369       0.921625       3.010945   \n",
       "min        -5.261000     -14.209600       5.960600       6.299300   \n",
       "25%        -1.170700      -1.946925       8.252800      13.829700   \n",
       "50%        -0.172700       2.408900       8.888200      15.934050   \n",
       "75%         0.829600       6.556725       9.593300      18.064725   \n",
       "max         4.272900      18.321500      12.000400      26.079100   \n",
       "\n",
       "             var_199  \n",
       "count  200000.000000  \n",
       "mean       -3.326537  \n",
       "std        10.438015  \n",
       "min       -38.852800  \n",
       "25%       -11.208475  \n",
       "50%        -2.819550  \n",
       "75%         4.836800  \n",
       "max        28.500700  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c172e30",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>2.1743</td>\n",
       "      <td>-3.4132</td>\n",
       "      <td>9.4763</td>\n",
       "      <td>13.3102</td>\n",
       "      <td>26.5376</td>\n",
       "      <td>1.4403</td>\n",
       "      <td>14.7100</td>\n",
       "      <td>6.0454</td>\n",
       "      <td>9.5426</td>\n",
       "      <td>17.1554</td>\n",
       "      <td>14.1104</td>\n",
       "      <td>24.3627</td>\n",
       "      <td>2.0323</td>\n",
       "      <td>6.7602</td>\n",
       "      <td>3.9141</td>\n",
       "      <td>-0.4851</td>\n",
       "      <td>2.5240</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>2.5516</td>\n",
       "      <td>15.5752</td>\n",
       "      <td>-13.4221</td>\n",
       "      <td>7.2739</td>\n",
       "      <td>16.0094</td>\n",
       "      <td>9.7268</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>12.0039</td>\n",
       "      <td>13.8571</td>\n",
       "      <td>-0.7338</td>\n",
       "      <td>-1.9245</td>\n",
       "      <td>15.4462</td>\n",
       "      <td>12.8287</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>9.6508</td>\n",
       "      <td>6.5674</td>\n",
       "      <td>5.1726</td>\n",
       "      <td>3.1345</td>\n",
       "      <td>29.4547</td>\n",
       "      <td>31.4045</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>15.6599</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>-5.6011</td>\n",
       "      <td>19.0614</td>\n",
       "      <td>11.2663</td>\n",
       "      <td>8.6989</td>\n",
       "      <td>8.3694</td>\n",
       "      <td>11.5659</td>\n",
       "      <td>-16.4727</td>\n",
       "      <td>4.0288</td>\n",
       "      <td>17.9244</td>\n",
       "      <td>18.5177</td>\n",
       "      <td>10.7800</td>\n",
       "      <td>9.0056</td>\n",
       "      <td>16.6964</td>\n",
       "      <td>10.4838</td>\n",
       "      <td>1.6573</td>\n",
       "      <td>12.1749</td>\n",
       "      <td>-13.1324</td>\n",
       "      <td>17.6054</td>\n",
       "      <td>11.5423</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>1.8944</td>\n",
       "      <td>0.6939</td>\n",
       "      <td>-13.6950</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>35.4734</td>\n",
       "      <td>1.7093</td>\n",
       "      <td>15.1866</td>\n",
       "      <td>2.6227</td>\n",
       "      <td>7.3412</td>\n",
       "      <td>32.0888</td>\n",
       "      <td>13.9550</td>\n",
       "      <td>13.0858</td>\n",
       "      <td>6.6203</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>5.3523</td>\n",
       "      <td>8.5426</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>4.1569</td>\n",
       "      <td>3.0454</td>\n",
       "      <td>7.8522</td>\n",
       "      <td>-11.5100</td>\n",
       "      <td>7.5109</td>\n",
       "      <td>31.5899</td>\n",
       "      <td>9.5018</td>\n",
       "      <td>8.2736</td>\n",
       "      <td>10.1633</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>12.5942</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>2.4354</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>16.5346</td>\n",
       "      <td>12.4205</td>\n",
       "      <td>-0.1780</td>\n",
       "      <td>5.7582</td>\n",
       "      <td>7.0513</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>-8.9921</td>\n",
       "      <td>9.7797</td>\n",
       "      <td>18.1577</td>\n",
       "      <td>-1.9721</td>\n",
       "      <td>16.1622</td>\n",
       "      <td>3.6937</td>\n",
       "      <td>6.6803</td>\n",
       "      <td>-0.3243</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>8.6086</td>\n",
       "      <td>11.0738</td>\n",
       "      <td>8.9231</td>\n",
       "      <td>11.7700</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>-4.4223</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>14.8743</td>\n",
       "      <td>9.4317</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>-0.5687</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>-9.6953</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>10.6261</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>-13.5174</td>\n",
       "      <td>2.2439</td>\n",
       "      <td>11.5283</td>\n",
       "      <td>12.0406</td>\n",
       "      <td>4.1006</td>\n",
       "      <td>-7.9078</td>\n",
       "      <td>11.1405</td>\n",
       "      <td>-5.7864</td>\n",
       "      <td>20.7477</td>\n",
       "      <td>6.8874</td>\n",
       "      <td>12.9143</td>\n",
       "      <td>19.5856</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>6.4059</td>\n",
       "      <td>9.3124</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>15.6372</td>\n",
       "      <td>5.8200</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>9.1854</td>\n",
       "      <td>12.5963</td>\n",
       "      <td>-10.3734</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>3.7163</td>\n",
       "      <td>-1.1016</td>\n",
       "      <td>7.3667</td>\n",
       "      <td>9.8565</td>\n",
       "      <td>5.0228</td>\n",
       "      <td>-5.7828</td>\n",
       "      <td>2.3612</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>12.1719</td>\n",
       "      <td>19.7312</td>\n",
       "      <td>19.4465</td>\n",
       "      <td>4.5048</td>\n",
       "      <td>23.2378</td>\n",
       "      <td>6.3191</td>\n",
       "      <td>12.8046</td>\n",
       "      <td>7.4729</td>\n",
       "      <td>15.7811</td>\n",
       "      <td>13.3529</td>\n",
       "      <td>10.1852</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>19.0773</td>\n",
       "      <td>-4.4577</td>\n",
       "      <td>9.5413</td>\n",
       "      <td>11.9052</td>\n",
       "      <td>2.1447</td>\n",
       "      <td>-22.4038</td>\n",
       "      <td>7.0883</td>\n",
       "      <td>14.1613</td>\n",
       "      <td>10.5080</td>\n",
       "      <td>14.2621</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>20.4031</td>\n",
       "      <td>17.0360</td>\n",
       "      <td>1.6981</td>\n",
       "      <td>-0.0269</td>\n",
       "      <td>-0.3939</td>\n",
       "      <td>12.6317</td>\n",
       "      <td>14.8863</td>\n",
       "      <td>1.3854</td>\n",
       "      <td>15.0284</td>\n",
       "      <td>3.9995</td>\n",
       "      <td>5.3683</td>\n",
       "      <td>8.6273</td>\n",
       "      <td>14.1963</td>\n",
       "      <td>20.3882</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>5.7033</td>\n",
       "      <td>4.5255</td>\n",
       "      <td>2.1929</td>\n",
       "      <td>3.1290</td>\n",
       "      <td>2.9044</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>28.7632</td>\n",
       "      <td>-17.2738</td>\n",
       "      <td>2.1056</td>\n",
       "      <td>21.1613</td>\n",
       "      <td>8.9573</td>\n",
       "      <td>2.7768</td>\n",
       "      <td>-2.1746</td>\n",
       "      <td>3.6932</td>\n",
       "      <td>12.4653</td>\n",
       "      <td>14.1978</td>\n",
       "      <td>-2.5511</td>\n",
       "      <td>-0.9479</td>\n",
       "      <td>17.1092</td>\n",
       "      <td>11.5419</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>8.8186</td>\n",
       "      <td>6.6231</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>-11.7218</td>\n",
       "      <td>24.5437</td>\n",
       "      <td>15.5827</td>\n",
       "      <td>3.8212</td>\n",
       "      <td>8.6674</td>\n",
       "      <td>7.3834</td>\n",
       "      <td>-2.4438</td>\n",
       "      <td>10.2158</td>\n",
       "      <td>7.4844</td>\n",
       "      <td>9.1104</td>\n",
       "      <td>4.3649</td>\n",
       "      <td>11.4934</td>\n",
       "      <td>1.7624</td>\n",
       "      <td>4.0714</td>\n",
       "      <td>-1.2681</td>\n",
       "      <td>14.3330</td>\n",
       "      <td>8.0088</td>\n",
       "      <td>4.4015</td>\n",
       "      <td>14.1479</td>\n",
       "      <td>-5.1747</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>14.5362</td>\n",
       "      <td>-1.7624</td>\n",
       "      <td>33.8820</td>\n",
       "      <td>11.6041</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>5.8442</td>\n",
       "      <td>4.7086</td>\n",
       "      <td>5.7141</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>20.5092</td>\n",
       "      <td>3.2790</td>\n",
       "      <td>-5.5952</td>\n",
       "      <td>7.3176</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>-7.0927</td>\n",
       "      <td>-3.9116</td>\n",
       "      <td>7.2569</td>\n",
       "      <td>-5.8234</td>\n",
       "      <td>25.6820</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-9.7009</td>\n",
       "      <td>2.4013</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>9.3908</td>\n",
       "      <td>-13.2648</td>\n",
       "      <td>3.1545</td>\n",
       "      <td>23.0866</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>5.3745</td>\n",
       "      <td>-6.2660</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-0.8417</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>9.1738</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>-3.8294</td>\n",
       "      <td>-1.0370</td>\n",
       "      <td>11.7770</td>\n",
       "      <td>11.2834</td>\n",
       "      <td>8.0485</td>\n",
       "      <td>-24.6840</td>\n",
       "      <td>12.7404</td>\n",
       "      <td>-35.1659</td>\n",
       "      <td>0.7613</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>12.6832</td>\n",
       "      <td>9.5503</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>5.2091</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>12.3972</td>\n",
       "      <td>14.4698</td>\n",
       "      <td>6.5850</td>\n",
       "      <td>3.3164</td>\n",
       "      <td>9.4638</td>\n",
       "      <td>15.7820</td>\n",
       "      <td>-25.0222</td>\n",
       "      <td>3.4418</td>\n",
       "      <td>-4.3923</td>\n",
       "      <td>8.6464</td>\n",
       "      <td>6.3072</td>\n",
       "      <td>5.6221</td>\n",
       "      <td>23.6143</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>-3.9989</td>\n",
       "      <td>4.0462</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>24.4187</td>\n",
       "      <td>4.5290</td>\n",
       "      <td>15.4235</td>\n",
       "      <td>11.6875</td>\n",
       "      <td>23.6273</td>\n",
       "      <td>4.0806</td>\n",
       "      <td>15.2733</td>\n",
       "      <td>0.7839</td>\n",
       "      <td>10.5404</td>\n",
       "      <td>1.6212</td>\n",
       "      <td>-5.2896</td>\n",
       "      <td>1.6027</td>\n",
       "      <td>17.9762</td>\n",
       "      <td>-2.3174</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>4.5474</td>\n",
       "      <td>7.5509</td>\n",
       "      <td>-7.5866</td>\n",
       "      <td>7.0364</td>\n",
       "      <td>14.4027</td>\n",
       "      <td>10.7795</td>\n",
       "      <td>7.2887</td>\n",
       "      <td>-1.0930</td>\n",
       "      <td>11.3596</td>\n",
       "      <td>18.1486</td>\n",
       "      <td>2.8344</td>\n",
       "      <td>1.9480</td>\n",
       "      <td>-19.8592</td>\n",
       "      <td>22.5316</td>\n",
       "      <td>18.6129</td>\n",
       "      <td>1.3512</td>\n",
       "      <td>9.3291</td>\n",
       "      <td>4.2835</td>\n",
       "      <td>10.3907</td>\n",
       "      <td>7.0874</td>\n",
       "      <td>14.3256</td>\n",
       "      <td>14.4135</td>\n",
       "      <td>4.2827</td>\n",
       "      <td>6.9750</td>\n",
       "      <td>1.6480</td>\n",
       "      <td>11.6896</td>\n",
       "      <td>2.5762</td>\n",
       "      <td>-2.5459</td>\n",
       "      <td>5.3446</td>\n",
       "      <td>38.1015</td>\n",
       "      <td>3.5732</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>30.5644</td>\n",
       "      <td>11.3025</td>\n",
       "      <td>3.9618</td>\n",
       "      <td>-8.2464</td>\n",
       "      <td>2.7038</td>\n",
       "      <td>12.3441</td>\n",
       "      <td>12.5431</td>\n",
       "      <td>-1.3683</td>\n",
       "      <td>3.5974</td>\n",
       "      <td>13.9761</td>\n",
       "      <td>14.3003</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>8.9500</td>\n",
       "      <td>7.1954</td>\n",
       "      <td>-1.1984</td>\n",
       "      <td>1.9586</td>\n",
       "      <td>27.5609</td>\n",
       "      <td>24.6065</td>\n",
       "      <td>-2.8233</td>\n",
       "      <td>8.9821</td>\n",
       "      <td>3.8873</td>\n",
       "      <td>15.9638</td>\n",
       "      <td>10.0142</td>\n",
       "      <td>7.8388</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>2.9253</td>\n",
       "      <td>10.4994</td>\n",
       "      <td>4.1622</td>\n",
       "      <td>3.7613</td>\n",
       "      <td>2.3701</td>\n",
       "      <td>18.0984</td>\n",
       "      <td>17.1765</td>\n",
       "      <td>7.6508</td>\n",
       "      <td>18.2452</td>\n",
       "      <td>17.0336</td>\n",
       "      <td>-10.9370</td>\n",
       "      <td>12.0500</td>\n",
       "      <td>-1.2155</td>\n",
       "      <td>19.9750</td>\n",
       "      <td>12.3892</td>\n",
       "      <td>31.8833</td>\n",
       "      <td>5.9684</td>\n",
       "      <td>7.2084</td>\n",
       "      <td>3.8899</td>\n",
       "      <td>-11.0882</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>2.5881</td>\n",
       "      <td>-2.7018</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>-7.1541</td>\n",
       "      <td>-6.1920</td>\n",
       "      <td>18.2366</td>\n",
       "      <td>11.7134</td>\n",
       "      <td>14.7483</td>\n",
       "      <td>8.1013</td>\n",
       "      <td>11.8771</td>\n",
       "      <td>13.9552</td>\n",
       "      <td>-10.4701</td>\n",
       "      <td>5.6961</td>\n",
       "      <td>-3.7546</td>\n",
       "      <td>8.4117</td>\n",
       "      <td>1.8986</td>\n",
       "      <td>7.2601</td>\n",
       "      <td>-0.4639</td>\n",
       "      <td>-0.0498</td>\n",
       "      <td>7.9336</td>\n",
       "      <td>-12.8279</td>\n",
       "      <td>12.4124</td>\n",
       "      <td>1.8489</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>19.3038</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>-5.5963</td>\n",
       "      <td>14.0685</td>\n",
       "      <td>11.9171</td>\n",
       "      <td>11.5111</td>\n",
       "      <td>6.9087</td>\n",
       "      <td>-65.4863</td>\n",
       "      <td>13.8657</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>-0.1346</td>\n",
       "      <td>14.4268</td>\n",
       "      <td>13.3273</td>\n",
       "      <td>10.4857</td>\n",
       "      <td>-1.4367</td>\n",
       "      <td>5.7555</td>\n",
       "      <td>-8.5414</td>\n",
       "      <td>14.1482</td>\n",
       "      <td>16.9840</td>\n",
       "      <td>6.1812</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>9.2048</td>\n",
       "      <td>8.6591</td>\n",
       "      <td>-27.7439</td>\n",
       "      <td>-0.4952</td>\n",
       "      <td>-1.7839</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>-4.3205</td>\n",
       "      <td>6.9860</td>\n",
       "      <td>1.6184</td>\n",
       "      <td>5.0301</td>\n",
       "      <td>-3.2431</td>\n",
       "      <td>40.1236</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>-0.7264</td>\n",
       "      <td>4.5886</td>\n",
       "      <td>-4.5346</td>\n",
       "      <td>23.3521</td>\n",
       "      <td>1.0273</td>\n",
       "      <td>19.1600</td>\n",
       "      <td>7.1734</td>\n",
       "      <td>14.3937</td>\n",
       "      <td>2.9598</td>\n",
       "      <td>13.3317</td>\n",
       "      <td>-9.2587</td>\n",
       "      <td>-6.7075</td>\n",
       "      <td>7.8984</td>\n",
       "      <td>14.5265</td>\n",
       "      <td>7.0799</td>\n",
       "      <td>20.1670</td>\n",
       "      <td>8.0053</td>\n",
       "      <td>3.7954</td>\n",
       "      <td>-39.7997</td>\n",
       "      <td>7.0065</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>14.0553</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>14.7246</td>\n",
       "      <td>35.2988</td>\n",
       "      <td>1.6844</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>-22.9264</td>\n",
       "      <td>12.3562</td>\n",
       "      <td>17.3410</td>\n",
       "      <td>1.6940</td>\n",
       "      <td>7.1179</td>\n",
       "      <td>5.1934</td>\n",
       "      <td>8.8230</td>\n",
       "      <td>10.6617</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>28.2749</td>\n",
       "      <td>-0.1937</td>\n",
       "      <td>5.9654</td>\n",
       "      <td>1.0719</td>\n",
       "      <td>7.9923</td>\n",
       "      <td>2.9138</td>\n",
       "      <td>-3.6135</td>\n",
       "      <td>1.4684</td>\n",
       "      <td>25.6795</td>\n",
       "      <td>13.8224</td>\n",
       "      <td>4.7478</td>\n",
       "      <td>41.1037</td>\n",
       "      <td>12.7140</td>\n",
       "      <td>5.2964</td>\n",
       "      <td>9.7289</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>12.1316</td>\n",
       "      <td>12.5815</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>5.6518</td>\n",
       "      <td>10.9346</td>\n",
       "      <td>11.4266</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>7.7532</td>\n",
       "      <td>6.6173</td>\n",
       "      <td>-6.8304</td>\n",
       "      <td>6.4730</td>\n",
       "      <td>17.1728</td>\n",
       "      <td>25.8128</td>\n",
       "      <td>2.6791</td>\n",
       "      <td>13.9547</td>\n",
       "      <td>6.6289</td>\n",
       "      <td>-4.3965</td>\n",
       "      <td>11.7159</td>\n",
       "      <td>16.1080</td>\n",
       "      <td>7.6874</td>\n",
       "      <td>9.1570</td>\n",
       "      <td>11.5670</td>\n",
       "      <td>-12.7047</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>9.9110</td>\n",
       "      <td>20.1461</td>\n",
       "      <td>1.2995</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>19.8234</td>\n",
       "      <td>4.7022</td>\n",
       "      <td>10.6101</td>\n",
       "      <td>13.0021</td>\n",
       "      <td>-12.6068</td>\n",
       "      <td>27.0846</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>33.5107</td>\n",
       "      <td>5.6953</td>\n",
       "      <td>5.4663</td>\n",
       "      <td>18.2201</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>21.2607</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>-1.7759</td>\n",
       "      <td>3.1283</td>\n",
       "      <td>5.5518</td>\n",
       "      <td>1.4493</td>\n",
       "      <td>-2.6627</td>\n",
       "      <td>19.8056</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>-3.3456</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>1.7189</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>9.7685</td>\n",
       "      <td>4.8910</td>\n",
       "      <td>12.2198</td>\n",
       "      <td>11.8503</td>\n",
       "      <td>-7.8931</td>\n",
       "      <td>6.4209</td>\n",
       "      <td>5.9270</td>\n",
       "      <td>16.0201</td>\n",
       "      <td>-0.2829</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0</td>\n",
       "      <td>11.4880</td>\n",
       "      <td>-0.4956</td>\n",
       "      <td>8.2622</td>\n",
       "      <td>3.5142</td>\n",
       "      <td>10.3404</td>\n",
       "      <td>11.6081</td>\n",
       "      <td>5.6709</td>\n",
       "      <td>15.1516</td>\n",
       "      <td>-0.6209</td>\n",
       "      <td>5.6669</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>-9.5348</td>\n",
       "      <td>13.9860</td>\n",
       "      <td>5.2982</td>\n",
       "      <td>8.2705</td>\n",
       "      <td>14.1527</td>\n",
       "      <td>7.4540</td>\n",
       "      <td>-5.0105</td>\n",
       "      <td>12.0465</td>\n",
       "      <td>8.6349</td>\n",
       "      <td>9.9137</td>\n",
       "      <td>25.1376</td>\n",
       "      <td>1.0914</td>\n",
       "      <td>3.2326</td>\n",
       "      <td>7.7802</td>\n",
       "      <td>13.9939</td>\n",
       "      <td>2.9085</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>4.2369</td>\n",
       "      <td>7.5665</td>\n",
       "      <td>-9.2149</td>\n",
       "      <td>9.5746</td>\n",
       "      <td>1.4012</td>\n",
       "      <td>7.4211</td>\n",
       "      <td>11.0075</td>\n",
       "      <td>7.8080</td>\n",
       "      <td>4.5567</td>\n",
       "      <td>4.9861</td>\n",
       "      <td>9.7471</td>\n",
       "      <td>0.0722</td>\n",
       "      <td>5.9053</td>\n",
       "      <td>8.1743</td>\n",
       "      <td>10.8800</td>\n",
       "      <td>11.1665</td>\n",
       "      <td>4.2600</td>\n",
       "      <td>-2.1296</td>\n",
       "      <td>8.7833</td>\n",
       "      <td>-15.5727</td>\n",
       "      <td>-8.4916</td>\n",
       "      <td>22.1905</td>\n",
       "      <td>12.4110</td>\n",
       "      <td>15.1168</td>\n",
       "      <td>1.6041</td>\n",
       "      <td>6.1868</td>\n",
       "      <td>10.9576</td>\n",
       "      <td>18.7371</td>\n",
       "      <td>15.2986</td>\n",
       "      <td>5.7322</td>\n",
       "      <td>5.1244</td>\n",
       "      <td>9.8225</td>\n",
       "      <td>14.0315</td>\n",
       "      <td>-23.6064</td>\n",
       "      <td>-1.3403</td>\n",
       "      <td>-2.5577</td>\n",
       "      <td>6.3582</td>\n",
       "      <td>-5.4557</td>\n",
       "      <td>5.6063</td>\n",
       "      <td>7.0054</td>\n",
       "      <td>5.0171</td>\n",
       "      <td>-5.0055</td>\n",
       "      <td>28.9502</td>\n",
       "      <td>1.2297</td>\n",
       "      <td>4.4918</td>\n",
       "      <td>19.5568</td>\n",
       "      <td>20.8357</td>\n",
       "      <td>19.2136</td>\n",
       "      <td>17.6422</td>\n",
       "      <td>17.9836</td>\n",
       "      <td>4.0395</td>\n",
       "      <td>14.0761</td>\n",
       "      <td>-5.7878</td>\n",
       "      <td>16.3870</td>\n",
       "      <td>-14.1721</td>\n",
       "      <td>-13.0269</td>\n",
       "      <td>-2.5955</td>\n",
       "      <td>21.4526</td>\n",
       "      <td>15.6163</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>8.2110</td>\n",
       "      <td>-0.8553</td>\n",
       "      <td>-12.1682</td>\n",
       "      <td>6.7779</td>\n",
       "      <td>7.3895</td>\n",
       "      <td>10.5084</td>\n",
       "      <td>15.5057</td>\n",
       "      <td>-0.6812</td>\n",
       "      <td>5.8999</td>\n",
       "      <td>6.1825</td>\n",
       "      <td>3.1038</td>\n",
       "      <td>-1.6930</td>\n",
       "      <td>-18.8473</td>\n",
       "      <td>9.9358</td>\n",
       "      <td>25.3359</td>\n",
       "      <td>1.3647</td>\n",
       "      <td>11.8509</td>\n",
       "      <td>5.0357</td>\n",
       "      <td>6.4630</td>\n",
       "      <td>18.4008</td>\n",
       "      <td>14.3787</td>\n",
       "      <td>19.0369</td>\n",
       "      <td>-0.6364</td>\n",
       "      <td>6.9155</td>\n",
       "      <td>3.6763</td>\n",
       "      <td>3.1460</td>\n",
       "      <td>4.9442</td>\n",
       "      <td>-1.8289</td>\n",
       "      <td>1.3521</td>\n",
       "      <td>34.6265</td>\n",
       "      <td>-0.6869</td>\n",
       "      <td>-5.3781</td>\n",
       "      <td>20.5030</td>\n",
       "      <td>10.9614</td>\n",
       "      <td>4.9677</td>\n",
       "      <td>6.1408</td>\n",
       "      <td>2.2575</td>\n",
       "      <td>12.8757</td>\n",
       "      <td>14.2253</td>\n",
       "      <td>-1.2868</td>\n",
       "      <td>0.2212</td>\n",
       "      <td>16.8661</td>\n",
       "      <td>12.7663</td>\n",
       "      <td>1.2414</td>\n",
       "      <td>7.1304</td>\n",
       "      <td>7.4108</td>\n",
       "      <td>-6.3369</td>\n",
       "      <td>3.0760</td>\n",
       "      <td>24.9796</td>\n",
       "      <td>20.3410</td>\n",
       "      <td>5.3312</td>\n",
       "      <td>23.7116</td>\n",
       "      <td>2.4745</td>\n",
       "      <td>11.2013</td>\n",
       "      <td>17.8165</td>\n",
       "      <td>13.0057</td>\n",
       "      <td>9.5506</td>\n",
       "      <td>5.3589</td>\n",
       "      <td>13.2491</td>\n",
       "      <td>-3.3068</td>\n",
       "      <td>3.6998</td>\n",
       "      <td>2.5927</td>\n",
       "      <td>14.3025</td>\n",
       "      <td>8.1596</td>\n",
       "      <td>7.9609</td>\n",
       "      <td>18.3343</td>\n",
       "      <td>4.3086</td>\n",
       "      <td>1.3546</td>\n",
       "      <td>12.4158</td>\n",
       "      <td>-5.3985</td>\n",
       "      <td>16.3683</td>\n",
       "      <td>10.4522</td>\n",
       "      <td>35.4923</td>\n",
       "      <td>5.5477</td>\n",
       "      <td>7.4244</td>\n",
       "      <td>12.5459</td>\n",
       "      <td>-6.7840</td>\n",
       "      <td>31.1895</td>\n",
       "      <td>2.6529</td>\n",
       "      <td>-11.1867</td>\n",
       "      <td>9.8865</td>\n",
       "      <td>5.4730</td>\n",
       "      <td>-5.3880</td>\n",
       "      <td>-0.4698</td>\n",
       "      <td>24.4025</td>\n",
       "      <td>-5.4493</td>\n",
       "      <td>11.3529</td>\n",
       "      <td>7.7075</td>\n",
       "      <td>-5.0491</td>\n",
       "      <td>13.0756</td>\n",
       "      <td>15.8271</td>\n",
       "      <td>3.3580</td>\n",
       "      <td>-14.3371</td>\n",
       "      <td>10.4421</td>\n",
       "      <td>7.6530</td>\n",
       "      <td>9.4585</td>\n",
       "      <td>22.7783</td>\n",
       "      <td>-4.0305</td>\n",
       "      <td>4.2233</td>\n",
       "      <td>-6.3906</td>\n",
       "      <td>13.5058</td>\n",
       "      <td>-0.4594</td>\n",
       "      <td>6.1415</td>\n",
       "      <td>13.2305</td>\n",
       "      <td>3.9901</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>18.0249</td>\n",
       "      <td>-1.7939</td>\n",
       "      <td>2.1661</td>\n",
       "      <td>8.5326</td>\n",
       "      <td>16.6660</td>\n",
       "      <td>-17.8661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0</td>\n",
       "      <td>4.9149</td>\n",
       "      <td>-2.4484</td>\n",
       "      <td>16.7052</td>\n",
       "      <td>6.6345</td>\n",
       "      <td>8.3096</td>\n",
       "      <td>-10.5628</td>\n",
       "      <td>5.8802</td>\n",
       "      <td>21.5940</td>\n",
       "      <td>-3.6797</td>\n",
       "      <td>6.0019</td>\n",
       "      <td>6.5576</td>\n",
       "      <td>-11.8776</td>\n",
       "      <td>14.4131</td>\n",
       "      <td>3.3087</td>\n",
       "      <td>3.5800</td>\n",
       "      <td>14.1597</td>\n",
       "      <td>7.5191</td>\n",
       "      <td>-8.8715</td>\n",
       "      <td>17.9467</td>\n",
       "      <td>17.0237</td>\n",
       "      <td>6.6459</td>\n",
       "      <td>18.2345</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>2.2532</td>\n",
       "      <td>15.4977</td>\n",
       "      <td>13.3282</td>\n",
       "      <td>5.2281</td>\n",
       "      <td>-3.7424</td>\n",
       "      <td>5.5144</td>\n",
       "      <td>5.7148</td>\n",
       "      <td>-13.7470</td>\n",
       "      <td>7.4369</td>\n",
       "      <td>1.3041</td>\n",
       "      <td>12.7552</td>\n",
       "      <td>12.5362</td>\n",
       "      <td>-1.1002</td>\n",
       "      <td>2.4370</td>\n",
       "      <td>6.2631</td>\n",
       "      <td>14.8565</td>\n",
       "      <td>-2.9862</td>\n",
       "      <td>-7.8820</td>\n",
       "      <td>7.1320</td>\n",
       "      <td>11.8869</td>\n",
       "      <td>11.4218</td>\n",
       "      <td>8.9282</td>\n",
       "      <td>-27.2007</td>\n",
       "      <td>14.5962</td>\n",
       "      <td>-19.8502</td>\n",
       "      <td>26.0775</td>\n",
       "      <td>24.3915</td>\n",
       "      <td>12.6910</td>\n",
       "      <td>10.2453</td>\n",
       "      <td>6.8173</td>\n",
       "      <td>4.5666</td>\n",
       "      <td>-9.5685</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.9534</td>\n",
       "      <td>7.3660</td>\n",
       "      <td>4.7038</td>\n",
       "      <td>9.4559</td>\n",
       "      <td>6.0037</td>\n",
       "      <td>-10.8728</td>\n",
       "      <td>0.7859</td>\n",
       "      <td>4.7000</td>\n",
       "      <td>7.8077</td>\n",
       "      <td>-1.7926</td>\n",
       "      <td>6.1534</td>\n",
       "      <td>12.9087</td>\n",
       "      <td>5.0398</td>\n",
       "      <td>-0.4247</td>\n",
       "      <td>22.6256</td>\n",
       "      <td>0.7166</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>13.5821</td>\n",
       "      <td>20.3267</td>\n",
       "      <td>25.5380</td>\n",
       "      <td>14.0155</td>\n",
       "      <td>17.3326</td>\n",
       "      <td>4.2046</td>\n",
       "      <td>14.0195</td>\n",
       "      <td>11.4812</td>\n",
       "      <td>17.9954</td>\n",
       "      <td>-18.3549</td>\n",
       "      <td>-3.4537</td>\n",
       "      <td>1.1233</td>\n",
       "      <td>22.3135</td>\n",
       "      <td>1.9795</td>\n",
       "      <td>16.0239</td>\n",
       "      <td>4.7492</td>\n",
       "      <td>0.2446</td>\n",
       "      <td>-39.6406</td>\n",
       "      <td>6.9473</td>\n",
       "      <td>9.9392</td>\n",
       "      <td>11.1977</td>\n",
       "      <td>14.1006</td>\n",
       "      <td>-0.8012</td>\n",
       "      <td>18.8214</td>\n",
       "      <td>32.9827</td>\n",
       "      <td>1.7989</td>\n",
       "      <td>-0.2476</td>\n",
       "      <td>-15.5294</td>\n",
       "      <td>9.5501</td>\n",
       "      <td>11.8548</td>\n",
       "      <td>1.5127</td>\n",
       "      <td>11.3998</td>\n",
       "      <td>4.2304</td>\n",
       "      <td>6.6777</td>\n",
       "      <td>11.3434</td>\n",
       "      <td>14.2993</td>\n",
       "      <td>13.1205</td>\n",
       "      <td>13.3224</td>\n",
       "      <td>7.3143</td>\n",
       "      <td>3.6817</td>\n",
       "      <td>9.7780</td>\n",
       "      <td>4.0491</td>\n",
       "      <td>2.7221</td>\n",
       "      <td>4.4344</td>\n",
       "      <td>3.7648</td>\n",
       "      <td>2.1927</td>\n",
       "      <td>-2.9197</td>\n",
       "      <td>23.0679</td>\n",
       "      <td>12.2112</td>\n",
       "      <td>3.7517</td>\n",
       "      <td>6.7907</td>\n",
       "      <td>6.5622</td>\n",
       "      <td>13.0283</td>\n",
       "      <td>12.2389</td>\n",
       "      <td>4.0627</td>\n",
       "      <td>-1.2406</td>\n",
       "      <td>13.9757</td>\n",
       "      <td>12.6133</td>\n",
       "      <td>0.6524</td>\n",
       "      <td>8.3929</td>\n",
       "      <td>6.9125</td>\n",
       "      <td>-6.0942</td>\n",
       "      <td>-6.3209</td>\n",
       "      <td>38.8105</td>\n",
       "      <td>17.6153</td>\n",
       "      <td>-2.9070</td>\n",
       "      <td>0.8270</td>\n",
       "      <td>2.0615</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>6.5953</td>\n",
       "      <td>17.2099</td>\n",
       "      <td>9.3960</td>\n",
       "      <td>9.9801</td>\n",
       "      <td>3.7881</td>\n",
       "      <td>2.9866</td>\n",
       "      <td>3.8695</td>\n",
       "      <td>17.8068</td>\n",
       "      <td>18.7807</td>\n",
       "      <td>9.4546</td>\n",
       "      <td>4.4657</td>\n",
       "      <td>17.8085</td>\n",
       "      <td>13.3077</td>\n",
       "      <td>-1.3209</td>\n",
       "      <td>12.7288</td>\n",
       "      <td>-12.3625</td>\n",
       "      <td>15.3500</td>\n",
       "      <td>11.1798</td>\n",
       "      <td>35.1445</td>\n",
       "      <td>5.5375</td>\n",
       "      <td>5.6397</td>\n",
       "      <td>17.0598</td>\n",
       "      <td>-9.7142</td>\n",
       "      <td>15.5117</td>\n",
       "      <td>3.3696</td>\n",
       "      <td>-17.1855</td>\n",
       "      <td>2.8292</td>\n",
       "      <td>5.2606</td>\n",
       "      <td>2.6836</td>\n",
       "      <td>5.8767</td>\n",
       "      <td>25.1262</td>\n",
       "      <td>7.3478</td>\n",
       "      <td>27.1264</td>\n",
       "      <td>11.8542</td>\n",
       "      <td>9.7999</td>\n",
       "      <td>11.1395</td>\n",
       "      <td>-3.2870</td>\n",
       "      <td>0.4285</td>\n",
       "      <td>2.5058</td>\n",
       "      <td>10.0339</td>\n",
       "      <td>9.1610</td>\n",
       "      <td>9.4318</td>\n",
       "      <td>13.4913</td>\n",
       "      <td>4.6247</td>\n",
       "      <td>6.2906</td>\n",
       "      <td>-17.8522</td>\n",
       "      <td>18.6751</td>\n",
       "      <td>-0.1162</td>\n",
       "      <td>4.9611</td>\n",
       "      <td>4.6549</td>\n",
       "      <td>0.6998</td>\n",
       "      <td>1.8341</td>\n",
       "      <td>22.2717</td>\n",
       "      <td>1.7337</td>\n",
       "      <td>-2.1651</td>\n",
       "      <td>6.7419</td>\n",
       "      <td>15.9054</td>\n",
       "      <td>0.3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0</td>\n",
       "      <td>11.2232</td>\n",
       "      <td>-5.0518</td>\n",
       "      <td>10.5127</td>\n",
       "      <td>5.6456</td>\n",
       "      <td>9.3410</td>\n",
       "      <td>-5.4086</td>\n",
       "      <td>4.5555</td>\n",
       "      <td>21.5571</td>\n",
       "      <td>0.1202</td>\n",
       "      <td>6.1629</td>\n",
       "      <td>4.4004</td>\n",
       "      <td>-0.4651</td>\n",
       "      <td>13.8775</td>\n",
       "      <td>9.7414</td>\n",
       "      <td>10.9044</td>\n",
       "      <td>14.5597</td>\n",
       "      <td>9.6214</td>\n",
       "      <td>-1.6429</td>\n",
       "      <td>23.1127</td>\n",
       "      <td>12.1517</td>\n",
       "      <td>16.2577</td>\n",
       "      <td>3.1453</td>\n",
       "      <td>3.1008</td>\n",
       "      <td>2.1497</td>\n",
       "      <td>10.2715</td>\n",
       "      <td>13.5637</td>\n",
       "      <td>4.9473</td>\n",
       "      <td>-0.9905</td>\n",
       "      <td>6.2801</td>\n",
       "      <td>9.4902</td>\n",
       "      <td>-12.8549</td>\n",
       "      <td>11.0403</td>\n",
       "      <td>1.4306</td>\n",
       "      <td>13.8533</td>\n",
       "      <td>11.7484</td>\n",
       "      <td>6.8969</td>\n",
       "      <td>6.4162</td>\n",
       "      <td>3.4246</td>\n",
       "      <td>12.1170</td>\n",
       "      <td>3.4096</td>\n",
       "      <td>-8.8763</td>\n",
       "      <td>9.5230</td>\n",
       "      <td>11.2566</td>\n",
       "      <td>11.4025</td>\n",
       "      <td>11.8492</td>\n",
       "      <td>-49.5007</td>\n",
       "      <td>7.4376</td>\n",
       "      <td>-21.2946</td>\n",
       "      <td>16.5701</td>\n",
       "      <td>15.9192</td>\n",
       "      <td>11.4688</td>\n",
       "      <td>16.3800</td>\n",
       "      <td>-5.7152</td>\n",
       "      <td>6.0771</td>\n",
       "      <td>7.5194</td>\n",
       "      <td>9.6364</td>\n",
       "      <td>15.3166</td>\n",
       "      <td>5.4830</td>\n",
       "      <td>0.6006</td>\n",
       "      <td>9.5466</td>\n",
       "      <td>22.0960</td>\n",
       "      <td>-6.7813</td>\n",
       "      <td>3.6870</td>\n",
       "      <td>-4.0387</td>\n",
       "      <td>5.8101</td>\n",
       "      <td>3.7793</td>\n",
       "      <td>5.7782</td>\n",
       "      <td>14.5730</td>\n",
       "      <td>5.0075</td>\n",
       "      <td>-1.0104</td>\n",
       "      <td>25.6050</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>3.3822</td>\n",
       "      <td>13.4685</td>\n",
       "      <td>10.8834</td>\n",
       "      <td>9.2657</td>\n",
       "      <td>-4.1948</td>\n",
       "      <td>12.1229</td>\n",
       "      <td>7.5949</td>\n",
       "      <td>11.9158</td>\n",
       "      <td>11.9537</td>\n",
       "      <td>16.9399</td>\n",
       "      <td>-2.2643</td>\n",
       "      <td>-3.3658</td>\n",
       "      <td>6.4020</td>\n",
       "      <td>18.2095</td>\n",
       "      <td>17.4710</td>\n",
       "      <td>6.3349</td>\n",
       "      <td>7.4740</td>\n",
       "      <td>4.8024</td>\n",
       "      <td>-0.3345</td>\n",
       "      <td>7.0295</td>\n",
       "      <td>16.5425</td>\n",
       "      <td>10.5645</td>\n",
       "      <td>12.7330</td>\n",
       "      <td>-0.9946</td>\n",
       "      <td>23.7210</td>\n",
       "      <td>11.2390</td>\n",
       "      <td>1.0012</td>\n",
       "      <td>-1.1083</td>\n",
       "      <td>-8.0574</td>\n",
       "      <td>10.0606</td>\n",
       "      <td>25.2535</td>\n",
       "      <td>1.8019</td>\n",
       "      <td>10.4973</td>\n",
       "      <td>4.2183</td>\n",
       "      <td>9.1158</td>\n",
       "      <td>10.1525</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>15.2503</td>\n",
       "      <td>3.4797</td>\n",
       "      <td>8.7901</td>\n",
       "      <td>2.9000</td>\n",
       "      <td>0.6471</td>\n",
       "      <td>2.3316</td>\n",
       "      <td>1.5084</td>\n",
       "      <td>0.2888</td>\n",
       "      <td>43.0307</td>\n",
       "      <td>-4.4543</td>\n",
       "      <td>3.2765</td>\n",
       "      <td>28.2664</td>\n",
       "      <td>12.1189</td>\n",
       "      <td>3.1526</td>\n",
       "      <td>14.2214</td>\n",
       "      <td>3.3878</td>\n",
       "      <td>13.2410</td>\n",
       "      <td>12.9788</td>\n",
       "      <td>4.5766</td>\n",
       "      <td>-4.8512</td>\n",
       "      <td>16.6344</td>\n",
       "      <td>12.3827</td>\n",
       "      <td>0.5293</td>\n",
       "      <td>8.0588</td>\n",
       "      <td>7.1081</td>\n",
       "      <td>-9.2317</td>\n",
       "      <td>-11.9277</td>\n",
       "      <td>20.5706</td>\n",
       "      <td>22.5568</td>\n",
       "      <td>3.0665</td>\n",
       "      <td>1.0527</td>\n",
       "      <td>7.4011</td>\n",
       "      <td>4.3367</td>\n",
       "      <td>1.4242</td>\n",
       "      <td>11.3654</td>\n",
       "      <td>9.1812</td>\n",
       "      <td>2.7627</td>\n",
       "      <td>12.2434</td>\n",
       "      <td>-0.2420</td>\n",
       "      <td>4.1575</td>\n",
       "      <td>4.7996</td>\n",
       "      <td>20.6307</td>\n",
       "      <td>10.2890</td>\n",
       "      <td>5.6890</td>\n",
       "      <td>13.4601</td>\n",
       "      <td>-0.9774</td>\n",
       "      <td>2.3728</td>\n",
       "      <td>11.7245</td>\n",
       "      <td>-9.6385</td>\n",
       "      <td>17.3101</td>\n",
       "      <td>14.0422</td>\n",
       "      <td>19.9293</td>\n",
       "      <td>5.3427</td>\n",
       "      <td>5.4776</td>\n",
       "      <td>13.1202</td>\n",
       "      <td>5.3500</td>\n",
       "      <td>31.7346</td>\n",
       "      <td>3.1693</td>\n",
       "      <td>-19.4779</td>\n",
       "      <td>6.8053</td>\n",
       "      <td>5.6281</td>\n",
       "      <td>-0.8774</td>\n",
       "      <td>-8.9508</td>\n",
       "      <td>17.4931</td>\n",
       "      <td>-1.6530</td>\n",
       "      <td>32.0032</td>\n",
       "      <td>12.5749</td>\n",
       "      <td>5.8756</td>\n",
       "      <td>8.8059</td>\n",
       "      <td>-10.6367</td>\n",
       "      <td>5.4401</td>\n",
       "      <td>-12.7967</td>\n",
       "      <td>8.7990</td>\n",
       "      <td>0.7021</td>\n",
       "      <td>14.9744</td>\n",
       "      <td>18.9211</td>\n",
       "      <td>0.3016</td>\n",
       "      <td>11.2869</td>\n",
       "      <td>-6.3741</td>\n",
       "      <td>12.9726</td>\n",
       "      <td>2.3425</td>\n",
       "      <td>4.0651</td>\n",
       "      <td>5.4414</td>\n",
       "      <td>3.1032</td>\n",
       "      <td>4.8793</td>\n",
       "      <td>23.5311</td>\n",
       "      <td>-1.5736</td>\n",
       "      <td>1.2832</td>\n",
       "      <td>8.7155</td>\n",
       "      <td>13.8329</td>\n",
       "      <td>4.1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>9.7148</td>\n",
       "      <td>-8.6098</td>\n",
       "      <td>13.6104</td>\n",
       "      <td>5.7930</td>\n",
       "      <td>12.5173</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>6.0479</td>\n",
       "      <td>17.0152</td>\n",
       "      <td>-2.1926</td>\n",
       "      <td>8.7542</td>\n",
       "      <td>1.4245</td>\n",
       "      <td>0.7086</td>\n",
       "      <td>14.2110</td>\n",
       "      <td>6.5641</td>\n",
       "      <td>7.6177</td>\n",
       "      <td>13.8771</td>\n",
       "      <td>9.0479</td>\n",
       "      <td>-11.8164</td>\n",
       "      <td>14.0831</td>\n",
       "      <td>-2.0345</td>\n",
       "      <td>18.3863</td>\n",
       "      <td>3.0911</td>\n",
       "      <td>5.5803</td>\n",
       "      <td>3.7091</td>\n",
       "      <td>12.8219</td>\n",
       "      <td>13.8866</td>\n",
       "      <td>-3.3859</td>\n",
       "      <td>-0.4440</td>\n",
       "      <td>5.4817</td>\n",
       "      <td>4.0902</td>\n",
       "      <td>-7.7085</td>\n",
       "      <td>10.3952</td>\n",
       "      <td>2.5739</td>\n",
       "      <td>17.8529</td>\n",
       "      <td>11.3433</td>\n",
       "      <td>5.0534</td>\n",
       "      <td>-3.0055</td>\n",
       "      <td>3.9433</td>\n",
       "      <td>11.0759</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>-11.7669</td>\n",
       "      <td>11.8626</td>\n",
       "      <td>10.7766</td>\n",
       "      <td>11.6900</td>\n",
       "      <td>12.9929</td>\n",
       "      <td>-42.9704</td>\n",
       "      <td>12.7881</td>\n",
       "      <td>4.4044</td>\n",
       "      <td>27.0880</td>\n",
       "      <td>14.0471</td>\n",
       "      <td>13.4318</td>\n",
       "      <td>9.4325</td>\n",
       "      <td>1.0213</td>\n",
       "      <td>6.2404</td>\n",
       "      <td>-8.1836</td>\n",
       "      <td>4.1057</td>\n",
       "      <td>10.7941</td>\n",
       "      <td>5.9704</td>\n",
       "      <td>-4.6315</td>\n",
       "      <td>9.9272</td>\n",
       "      <td>14.4322</td>\n",
       "      <td>-13.8557</td>\n",
       "      <td>-1.8803</td>\n",
       "      <td>1.8243</td>\n",
       "      <td>4.8059</td>\n",
       "      <td>-1.6255</td>\n",
       "      <td>5.1595</td>\n",
       "      <td>-2.8395</td>\n",
       "      <td>5.0116</td>\n",
       "      <td>2.4464</td>\n",
       "      <td>24.0896</td>\n",
       "      <td>0.8953</td>\n",
       "      <td>-2.6184</td>\n",
       "      <td>27.7040</td>\n",
       "      <td>43.5092</td>\n",
       "      <td>16.4079</td>\n",
       "      <td>14.4559</td>\n",
       "      <td>27.7355</td>\n",
       "      <td>5.5360</td>\n",
       "      <td>16.7484</td>\n",
       "      <td>9.6956</td>\n",
       "      <td>21.4391</td>\n",
       "      <td>-5.1839</td>\n",
       "      <td>6.8296</td>\n",
       "      <td>-9.0318</td>\n",
       "      <td>24.2122</td>\n",
       "      <td>-7.5779</td>\n",
       "      <td>5.6786</td>\n",
       "      <td>13.1278</td>\n",
       "      <td>7.0086</td>\n",
       "      <td>-32.3247</td>\n",
       "      <td>7.0141</td>\n",
       "      <td>6.9451</td>\n",
       "      <td>10.0272</td>\n",
       "      <td>10.0716</td>\n",
       "      <td>-0.3385</td>\n",
       "      <td>19.4605</td>\n",
       "      <td>26.9480</td>\n",
       "      <td>1.7079</td>\n",
       "      <td>-4.8882</td>\n",
       "      <td>-2.3891</td>\n",
       "      <td>24.6626</td>\n",
       "      <td>19.7783</td>\n",
       "      <td>1.5780</td>\n",
       "      <td>14.3962</td>\n",
       "      <td>4.8206</td>\n",
       "      <td>12.2354</td>\n",
       "      <td>33.9267</td>\n",
       "      <td>14.2625</td>\n",
       "      <td>26.2407</td>\n",
       "      <td>2.9091</td>\n",
       "      <td>6.4540</td>\n",
       "      <td>5.3290</td>\n",
       "      <td>10.6131</td>\n",
       "      <td>3.4212</td>\n",
       "      <td>-1.8915</td>\n",
       "      <td>2.1376</td>\n",
       "      <td>46.4915</td>\n",
       "      <td>1.0591</td>\n",
       "      <td>3.3543</td>\n",
       "      <td>18.1250</td>\n",
       "      <td>10.0102</td>\n",
       "      <td>9.3483</td>\n",
       "      <td>11.0467</td>\n",
       "      <td>2.3866</td>\n",
       "      <td>12.2352</td>\n",
       "      <td>13.5462</td>\n",
       "      <td>3.0043</td>\n",
       "      <td>5.3751</td>\n",
       "      <td>17.1567</td>\n",
       "      <td>11.6873</td>\n",
       "      <td>0.6677</td>\n",
       "      <td>8.3511</td>\n",
       "      <td>6.5834</td>\n",
       "      <td>1.6146</td>\n",
       "      <td>4.8462</td>\n",
       "      <td>15.2331</td>\n",
       "      <td>3.8390</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>2.2357</td>\n",
       "      <td>14.8203</td>\n",
       "      <td>5.8648</td>\n",
       "      <td>8.7190</td>\n",
       "      <td>15.1468</td>\n",
       "      <td>9.9930</td>\n",
       "      <td>10.4543</td>\n",
       "      <td>10.9535</td>\n",
       "      <td>-10.3405</td>\n",
       "      <td>3.6463</td>\n",
       "      <td>-16.8622</td>\n",
       "      <td>18.8580</td>\n",
       "      <td>8.2192</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>16.7224</td>\n",
       "      <td>8.8882</td>\n",
       "      <td>-3.2567</td>\n",
       "      <td>12.9142</td>\n",
       "      <td>-8.5421</td>\n",
       "      <td>15.9319</td>\n",
       "      <td>5.8348</td>\n",
       "      <td>40.3378</td>\n",
       "      <td>5.5357</td>\n",
       "      <td>4.6151</td>\n",
       "      <td>8.5910</td>\n",
       "      <td>-12.6998</td>\n",
       "      <td>25.8578</td>\n",
       "      <td>2.2346</td>\n",
       "      <td>-6.4988</td>\n",
       "      <td>2.6702</td>\n",
       "      <td>5.3868</td>\n",
       "      <td>-7.1875</td>\n",
       "      <td>8.1477</td>\n",
       "      <td>22.4362</td>\n",
       "      <td>-2.5914</td>\n",
       "      <td>8.8704</td>\n",
       "      <td>11.6621</td>\n",
       "      <td>7.4904</td>\n",
       "      <td>8.1808</td>\n",
       "      <td>-11.4177</td>\n",
       "      <td>2.8379</td>\n",
       "      <td>3.8748</td>\n",
       "      <td>8.7410</td>\n",
       "      <td>8.9998</td>\n",
       "      <td>16.4058</td>\n",
       "      <td>11.3244</td>\n",
       "      <td>-2.1751</td>\n",
       "      <td>12.4735</td>\n",
       "      <td>-18.3932</td>\n",
       "      <td>12.6337</td>\n",
       "      <td>0.3243</td>\n",
       "      <td>2.6840</td>\n",
       "      <td>8.6587</td>\n",
       "      <td>2.7337</td>\n",
       "      <td>11.1178</td>\n",
       "      <td>20.4158</td>\n",
       "      <td>-0.0786</td>\n",
       "      <td>6.7980</td>\n",
       "      <td>10.0342</td>\n",
       "      <td>15.5289</td>\n",
       "      <td>-13.9001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0</td>\n",
       "      <td>10.8762</td>\n",
       "      <td>-5.7105</td>\n",
       "      <td>12.1183</td>\n",
       "      <td>8.0328</td>\n",
       "      <td>11.5577</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>5.2839</td>\n",
       "      <td>15.2058</td>\n",
       "      <td>-0.4541</td>\n",
       "      <td>9.3688</td>\n",
       "      <td>-7.3826</td>\n",
       "      <td>-8.7049</td>\n",
       "      <td>14.2486</td>\n",
       "      <td>15.0849</td>\n",
       "      <td>5.2313</td>\n",
       "      <td>14.3572</td>\n",
       "      <td>12.5523</td>\n",
       "      <td>-6.5066</td>\n",
       "      <td>11.3592</td>\n",
       "      <td>11.4779</td>\n",
       "      <td>15.4997</td>\n",
       "      <td>3.8474</td>\n",
       "      <td>2.4381</td>\n",
       "      <td>2.8295</td>\n",
       "      <td>10.6681</td>\n",
       "      <td>13.7167</td>\n",
       "      <td>-7.7771</td>\n",
       "      <td>-2.7798</td>\n",
       "      <td>6.2885</td>\n",
       "      <td>6.0089</td>\n",
       "      <td>2.1547</td>\n",
       "      <td>10.8181</td>\n",
       "      <td>-0.2712</td>\n",
       "      <td>12.5254</td>\n",
       "      <td>11.6304</td>\n",
       "      <td>-1.4949</td>\n",
       "      <td>7.9509</td>\n",
       "      <td>2.2480</td>\n",
       "      <td>8.1459</td>\n",
       "      <td>0.7928</td>\n",
       "      <td>-7.9028</td>\n",
       "      <td>7.4223</td>\n",
       "      <td>11.4249</td>\n",
       "      <td>11.9103</td>\n",
       "      <td>8.7002</td>\n",
       "      <td>-6.6883</td>\n",
       "      <td>10.5219</td>\n",
       "      <td>-25.9933</td>\n",
       "      <td>11.6241</td>\n",
       "      <td>13.4670</td>\n",
       "      <td>12.3563</td>\n",
       "      <td>3.4031</td>\n",
       "      <td>-12.9247</td>\n",
       "      <td>6.2607</td>\n",
       "      <td>11.8525</td>\n",
       "      <td>8.8581</td>\n",
       "      <td>20.6438</td>\n",
       "      <td>6.5641</td>\n",
       "      <td>0.5322</td>\n",
       "      <td>10.0740</td>\n",
       "      <td>11.2477</td>\n",
       "      <td>-19.5169</td>\n",
       "      <td>-1.6499</td>\n",
       "      <td>5.3036</td>\n",
       "      <td>5.6244</td>\n",
       "      <td>1.2976</td>\n",
       "      <td>5.4680</td>\n",
       "      <td>10.3979</td>\n",
       "      <td>5.0209</td>\n",
       "      <td>-6.9248</td>\n",
       "      <td>32.4865</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>4.3880</td>\n",
       "      <td>16.1819</td>\n",
       "      <td>11.5080</td>\n",
       "      <td>11.9092</td>\n",
       "      <td>6.3494</td>\n",
       "      <td>23.0598</td>\n",
       "      <td>2.4466</td>\n",
       "      <td>15.6721</td>\n",
       "      <td>9.3809</td>\n",
       "      <td>14.7593</td>\n",
       "      <td>-12.8156</td>\n",
       "      <td>3.4928</td>\n",
       "      <td>-3.1634</td>\n",
       "      <td>21.5742</td>\n",
       "      <td>9.7015</td>\n",
       "      <td>22.4258</td>\n",
       "      <td>7.1213</td>\n",
       "      <td>2.4050</td>\n",
       "      <td>-3.1107</td>\n",
       "      <td>7.1529</td>\n",
       "      <td>16.2315</td>\n",
       "      <td>11.5051</td>\n",
       "      <td>16.5967</td>\n",
       "      <td>0.6444</td>\n",
       "      <td>21.0773</td>\n",
       "      <td>25.7270</td>\n",
       "      <td>2.4916</td>\n",
       "      <td>-3.0062</td>\n",
       "      <td>0.9636</td>\n",
       "      <td>13.4966</td>\n",
       "      <td>31.3629</td>\n",
       "      <td>1.5517</td>\n",
       "      <td>12.1898</td>\n",
       "      <td>3.5216</td>\n",
       "      <td>6.8915</td>\n",
       "      <td>9.0475</td>\n",
       "      <td>14.1260</td>\n",
       "      <td>15.9937</td>\n",
       "      <td>6.7106</td>\n",
       "      <td>8.1964</td>\n",
       "      <td>6.5520</td>\n",
       "      <td>6.6900</td>\n",
       "      <td>2.4643</td>\n",
       "      <td>4.5841</td>\n",
       "      <td>0.7221</td>\n",
       "      <td>23.4020</td>\n",
       "      <td>-10.3157</td>\n",
       "      <td>6.4857</td>\n",
       "      <td>33.4300</td>\n",
       "      <td>12.2439</td>\n",
       "      <td>4.3416</td>\n",
       "      <td>10.3869</td>\n",
       "      <td>1.3913</td>\n",
       "      <td>12.7127</td>\n",
       "      <td>13.8530</td>\n",
       "      <td>4.6685</td>\n",
       "      <td>-1.6082</td>\n",
       "      <td>10.1564</td>\n",
       "      <td>11.7936</td>\n",
       "      <td>0.2316</td>\n",
       "      <td>8.1760</td>\n",
       "      <td>7.6166</td>\n",
       "      <td>-18.3865</td>\n",
       "      <td>-7.3542</td>\n",
       "      <td>32.6663</td>\n",
       "      <td>15.5464</td>\n",
       "      <td>-0.9083</td>\n",
       "      <td>1.0662</td>\n",
       "      <td>5.3922</td>\n",
       "      <td>9.7708</td>\n",
       "      <td>11.4687</td>\n",
       "      <td>8.6980</td>\n",
       "      <td>8.0106</td>\n",
       "      <td>13.1911</td>\n",
       "      <td>12.3484</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>3.8811</td>\n",
       "      <td>-9.4762</td>\n",
       "      <td>11.6612</td>\n",
       "      <td>13.1571</td>\n",
       "      <td>8.5043</td>\n",
       "      <td>17.0369</td>\n",
       "      <td>7.1124</td>\n",
       "      <td>-13.1967</td>\n",
       "      <td>13.9404</td>\n",
       "      <td>-8.3303</td>\n",
       "      <td>29.0140</td>\n",
       "      <td>9.6174</td>\n",
       "      <td>15.9041</td>\n",
       "      <td>5.3187</td>\n",
       "      <td>6.2987</td>\n",
       "      <td>13.0729</td>\n",
       "      <td>-4.2045</td>\n",
       "      <td>19.2141</td>\n",
       "      <td>3.2902</td>\n",
       "      <td>-1.2175</td>\n",
       "      <td>4.1583</td>\n",
       "      <td>5.7675</td>\n",
       "      <td>5.7719</td>\n",
       "      <td>-1.2139</td>\n",
       "      <td>21.8496</td>\n",
       "      <td>-3.5368</td>\n",
       "      <td>25.9094</td>\n",
       "      <td>11.7673</td>\n",
       "      <td>1.9765</td>\n",
       "      <td>15.9218</td>\n",
       "      <td>3.9350</td>\n",
       "      <td>4.3993</td>\n",
       "      <td>-10.3268</td>\n",
       "      <td>10.5200</td>\n",
       "      <td>9.9587</td>\n",
       "      <td>11.9242</td>\n",
       "      <td>7.0626</td>\n",
       "      <td>-6.5429</td>\n",
       "      <td>10.5947</td>\n",
       "      <td>-3.8827</td>\n",
       "      <td>16.3552</td>\n",
       "      <td>1.7535</td>\n",
       "      <td>8.9842</td>\n",
       "      <td>1.6893</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.3766</td>\n",
       "      <td>15.2101</td>\n",
       "      <td>-2.4907</td>\n",
       "      <td>-2.2342</td>\n",
       "      <td>8.1857</td>\n",
       "      <td>12.1284</td>\n",
       "      <td>0.1385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows  201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target    var_0   var_1    var_2   var_3    var_4    var_5   var_6  \\\n",
       "0            0   8.9255 -6.7863  11.9081  5.0930  11.4607  -9.2834  5.1187   \n",
       "1            0  11.5006 -4.1473  13.8588  5.3890  12.3622   7.0433  5.6208   \n",
       "2            0   8.6093 -2.7457  12.0805  7.8928  10.5825  -9.0837  6.9427   \n",
       "3            0  11.0604 -2.1518   8.9522  7.1957  12.5846  -1.8361  5.8428   \n",
       "4            0   9.8369 -1.4834  12.8746  6.6375  12.2772   2.4486  5.9405   \n",
       "...        ...      ...     ...      ...     ...      ...      ...     ...   \n",
       "199995       0  11.4880 -0.4956   8.2622  3.5142  10.3404  11.6081  5.6709   \n",
       "199996       0   4.9149 -2.4484  16.7052  6.6345   8.3096 -10.5628  5.8802   \n",
       "199997       0  11.2232 -5.0518  10.5127  5.6456   9.3410  -5.4086  4.5555   \n",
       "199998       0   9.7148 -8.6098  13.6104  5.7930  12.5173   0.5339  6.0479   \n",
       "199999       0  10.8762 -5.7105  12.1183  8.0328  11.5577   0.3488  5.2839   \n",
       "\n",
       "          var_7   var_8   var_9  var_10   var_11   var_12   var_13   var_14  \\\n",
       "0       18.6266 -4.9200  5.7470  2.9252   3.1821  14.0137   0.5745   8.7989   \n",
       "1       16.5338  3.1468  8.0851 -0.4032   8.0585  14.0239   8.4135   5.4345   \n",
       "2       14.6155 -4.9193  5.9525 -0.3249 -11.2648  14.1929   7.3124   7.5244   \n",
       "3       14.9250 -5.8609  8.2450  2.3061   2.8102  13.8463  11.9704   6.4569   \n",
       "4       19.2514  6.2654  7.6784 -9.4458 -12.1419  13.8481   7.8895   7.7894   \n",
       "...         ...     ...     ...     ...      ...      ...      ...      ...   \n",
       "199995  15.1516 -0.6209  5.6669  3.7574  -9.5348  13.9860   5.2982   8.2705   \n",
       "199996  21.5940 -3.6797  6.0019  6.5576 -11.8776  14.4131   3.3087   3.5800   \n",
       "199997  21.5571  0.1202  6.1629  4.4004  -0.4651  13.8775   9.7414  10.9044   \n",
       "199998  17.0152 -2.1926  8.7542  1.4245   0.7086  14.2110   6.5641   7.6177   \n",
       "199999  15.2058 -0.4541  9.3688 -7.3826  -8.7049  14.2486  15.0849   5.2313   \n",
       "\n",
       "         var_15   var_16   var_17   var_18   var_19   var_20   var_21  \\\n",
       "0       14.5691   5.7487  -7.2393   4.2840  30.7133  10.5350  16.2191   \n",
       "1       13.7003  13.8275 -15.5849   7.8000  28.5708   3.4287   2.7407   \n",
       "2       14.6472   7.6782  -1.7395   4.7011  20.4775  17.7559  18.1377   \n",
       "3       14.8372  10.7430  -0.4299  15.9426  13.7257  20.3010  12.5579   \n",
       "4       15.0553   8.4871  -3.0680   6.5263  11.3152  21.4246  18.9608   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  14.1527   7.4540  -5.0105  12.0465   8.6349   9.9137  25.1376   \n",
       "199996  14.1597   7.5191  -8.8715  17.9467  17.0237   6.6459  18.2345   \n",
       "199997  14.5597   9.6214  -1.6429  23.1127  12.1517  16.2577   3.1453   \n",
       "199998  13.8771   9.0479 -11.8164  14.0831  -2.0345  18.3863   3.0911   \n",
       "199999  14.3572  12.5523  -6.5066  11.3592  11.4779  15.4997   3.8474   \n",
       "\n",
       "         var_22  var_23   var_24   var_25   var_26  var_27  var_28  var_29  \\\n",
       "0        2.5791  2.4716  14.3831  13.4325  -5.1488 -0.4073  4.9306  5.9965   \n",
       "1        8.5524  3.3716   6.9779  13.8910 -11.7684 -2.5586  5.0464  0.5481   \n",
       "2        1.2145  3.5137   5.6777  13.2177  -7.9940 -2.9029  5.8463  6.1439   \n",
       "3        6.8202  2.7229  12.1354  13.7367   0.8135 -0.9059  5.9070  2.8407   \n",
       "4       10.1102  2.7142  14.2080  13.5433   3.1736 -3.3423  5.9015  7.9352   \n",
       "...         ...     ...      ...      ...      ...     ...     ...     ...   \n",
       "199995   1.0914  3.2326   7.7802  13.9939   2.9085  0.1005  4.2369  7.5665   \n",
       "199996   0.8982  2.2532  15.4977  13.3282   5.2281 -3.7424  5.5144  5.7148   \n",
       "199997   3.1008  2.1497  10.2715  13.5637   4.9473 -0.9905  6.2801  9.4902   \n",
       "199998   5.5803  3.7091  12.8219  13.8866  -3.3859 -0.4440  5.4817  4.0902   \n",
       "199999   2.4381  2.8295  10.6681  13.7167  -7.7771 -2.7798  6.2885  6.0089   \n",
       "\n",
       "         var_30   var_31  var_32   var_33   var_34   var_35  var_36  var_37  \\\n",
       "0       -0.3085  12.9041 -3.8766  16.8911  11.1920  10.5785  0.6764  7.8871   \n",
       "1       -9.2987   7.8755  1.2859  19.3710  11.3702   0.7399  2.7995  5.8434   \n",
       "2      -11.1025  12.4858 -2.2871  19.0422  11.0449   4.1087  4.6974  6.9346   \n",
       "3      -15.2398  10.4407 -2.5731   6.1796  10.6093  -5.9158  8.1723  2.8521   \n",
       "4       -3.1582   9.4668 -0.0083  19.3239  12.4057   0.6329  2.7922  5.8184   \n",
       "...         ...      ...     ...      ...      ...      ...     ...     ...   \n",
       "199995  -9.2149   9.5746  1.4012   7.4211  11.0075   7.8080  4.5567  4.9861   \n",
       "199996 -13.7470   7.4369  1.3041  12.7552  12.5362  -1.1002  2.4370  6.2631   \n",
       "199997 -12.8549  11.0403  1.4306  13.8533  11.7484   6.8969  6.4162  3.4246   \n",
       "199998  -7.7085  10.3952  2.5739  17.8529  11.3433   5.0534 -3.0055  3.9433   \n",
       "199999   2.1547  10.8181 -0.2712  12.5254  11.6304  -1.4949  7.9509  2.2480   \n",
       "\n",
       "         var_38  var_39   var_40   var_41   var_42   var_43   var_44   var_45  \\\n",
       "0        4.6667  3.8743  -5.2387   7.3746  11.5767  12.0446  11.6418  -7.0170   \n",
       "1       10.8160  3.6783 -11.1147   1.8730   9.8775  11.7842   1.2444 -47.3797   \n",
       "2       10.8917  0.9003 -13.5174   2.2439  11.5283  12.0406   4.1006  -7.9078   \n",
       "3        9.1738  0.6665  -3.8294  -1.0370  11.7770  11.2834   8.0485 -24.6840   \n",
       "4       19.3038  1.4450  -5.5963  14.0685  11.9171  11.5111   6.9087 -65.4863   \n",
       "...         ...     ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   9.7471  0.0722   5.9053   8.1743  10.8800  11.1665   4.2600  -2.1296   \n",
       "199996  14.8565 -2.9862  -7.8820   7.1320  11.8869  11.4218   8.9282 -27.2007   \n",
       "199997  12.1170  3.4096  -8.8763   9.5230  11.2566  11.4025  11.8492 -49.5007   \n",
       "199998  11.0759  1.2173 -11.7669  11.8626  10.7766  11.6900  12.9929 -42.9704   \n",
       "199999   8.1459  0.7928  -7.9028   7.4223  11.4249  11.9103   8.7002  -6.6883   \n",
       "\n",
       "         var_46   var_47   var_48   var_49   var_50   var_51   var_52  var_53  \\\n",
       "0        5.9226 -14.2136  16.0283   5.3253  12.9194  29.0460  -0.6940  5.1736   \n",
       "1        7.3718   0.1948  34.4014  25.7037  11.8343  13.2256  -4.1083  6.6885   \n",
       "2       11.1405  -5.7864  20.7477   6.8874  12.9143  19.5856   0.7268  6.4059   \n",
       "3       12.7404 -35.1659   0.7613   8.3838  12.6832   9.5503   1.7895  5.2091   \n",
       "4       13.8657   0.0444  -0.1346  14.4268  13.3273  10.4857  -1.4367  5.7555   \n",
       "...         ...      ...      ...      ...      ...      ...      ...     ...   \n",
       "199995   8.7833 -15.5727  -8.4916  22.1905  12.4110  15.1168   1.6041  6.1868   \n",
       "199996  14.5962 -19.8502  26.0775  24.3915  12.6910  10.2453   6.8173  4.5666   \n",
       "199997   7.4376 -21.2946  16.5701  15.9192  11.4688  16.3800  -5.7152  6.0771   \n",
       "199998  12.7881   4.4044  27.0880  14.0471  13.4318   9.4325   1.0213  6.2404   \n",
       "199999  10.5219 -25.9933  11.6241  13.4670  12.3563   3.4031 -12.9247  6.2607   \n",
       "\n",
       "         var_54   var_55   var_56  var_57  var_58   var_59   var_60   var_61  \\\n",
       "0       -0.7474  14.8322  11.2668  5.3822  2.0183  10.1166  16.1828   4.9590   \n",
       "1       -8.0946  18.5995  19.3219  7.0118  1.9210   8.8682   8.0109  -7.2417   \n",
       "2        9.3124   6.2846  15.6372  5.8200  1.1000   9.1854  12.5963 -10.3734   \n",
       "3        8.0913  12.3972  14.4698  6.5850  3.3164   9.4638  15.7820 -25.0222   \n",
       "4       -8.5414  14.1482  16.9840  6.1812  1.9548   9.2048   8.6591 -27.7439   \n",
       "...         ...      ...      ...     ...     ...      ...      ...      ...   \n",
       "199995  10.9576  18.7371  15.2986  5.7322  5.1244   9.8225  14.0315 -23.6064   \n",
       "199996  -9.5685  18.4685  16.9534  7.3660  4.7038   9.4559   6.0037 -10.8728   \n",
       "199997   7.5194   9.6364  15.3166  5.4830  0.6006   9.5466  22.0960  -6.7813   \n",
       "199998  -8.1836   4.1057  10.7941  5.9704 -4.6315   9.9272  14.4322 -13.8557   \n",
       "199999  11.8525   8.8581  20.6438  6.5641  0.5322  10.0740  11.2477 -19.5169   \n",
       "\n",
       "        var_62  var_63  var_64  var_65  var_66   var_67  var_68  var_69  \\\n",
       "0       2.0771 -0.2154  8.6748  9.5319  5.8056  22.4321  5.0109 -4.7010   \n",
       "1       1.7944 -1.3147  8.1042  1.5365  5.4007   7.9344  5.0220  2.2302   \n",
       "2       0.8748  5.8042  3.7163 -1.1016  7.3667   9.8565  5.0228 -5.7828   \n",
       "3       3.4418 -4.3923  8.6464  6.3072  5.6221  23.6143  5.0220 -3.9989   \n",
       "4      -0.4952 -1.7839  5.2670 -4.3205  6.9860   1.6184  5.0301 -3.2431   \n",
       "...        ...     ...     ...     ...     ...      ...     ...     ...   \n",
       "199995 -1.3403 -2.5577  6.3582 -5.4557  5.6063   7.0054  5.0171 -5.0055   \n",
       "199996  0.7859  4.7000  7.8077 -1.7926  6.1534  12.9087  5.0398 -0.4247   \n",
       "199997  3.6870 -4.0387  5.8101  3.7793  5.7782  14.5730  5.0075 -1.0104   \n",
       "199998 -1.8803  1.8243  4.8059 -1.6255  5.1595  -2.8395  5.0116  2.4464   \n",
       "199999 -1.6499  5.3036  5.6244  1.2976  5.4680  10.3979  5.0209 -6.9248   \n",
       "\n",
       "         var_70  var_71  var_72   var_73   var_74   var_75   var_76   var_77  \\\n",
       "0       21.6374  0.5663  5.1999   8.8600  43.1127  18.3816  -2.3440  23.4104   \n",
       "1       40.5632  0.5134  3.1701  20.1068   7.7841   7.0529   3.2709  23.4822   \n",
       "2        2.3612  0.8520  6.3577  12.1719  19.7312  19.4465   4.5048  23.2378   \n",
       "3        4.0462  0.2500  1.2516  24.4187   4.5290  15.4235  11.6875  23.6273   \n",
       "4       40.1236  0.7737 -0.7264   4.5886  -4.5346  23.3521   1.0273  19.1600   \n",
       "...         ...     ...     ...      ...      ...      ...      ...      ...   \n",
       "199995  28.9502  1.2297  4.4918  19.5568  20.8357  19.2136  17.6422  17.9836   \n",
       "199996  22.6256  0.7166  0.6533  13.5821  20.3267  25.5380  14.0155  17.3326   \n",
       "199997  25.6050  0.2655  3.3822  13.4685  10.8834   9.2657  -4.1948  12.1229   \n",
       "199998  24.0896  0.8953 -2.6184  27.7040  43.5092  16.4079  14.4559  27.7355   \n",
       "199999  32.4865  0.8271  4.3880  16.1819  11.5080  11.9092   6.3494  23.0598   \n",
       "\n",
       "        var_78   var_79   var_80   var_81   var_82   var_83  var_84   var_85  \\\n",
       "0       6.5199  12.1983  13.6468  13.8372   1.3675   2.9423 -4.5213  21.4669   \n",
       "1       5.5075  13.7814   2.5462  18.1782   0.3683  -4.8210 -5.4850  13.7867   \n",
       "2       6.3191  12.8046   7.4729  15.7811  13.3529  10.1852  5.4604  19.0773   \n",
       "3       4.0806  15.2733   0.7839  10.5404   1.6212  -5.2896  1.6027  17.9762   \n",
       "4       7.1734  14.3937   2.9598  13.3317  -9.2587  -6.7075  7.8984  14.5265   \n",
       "...        ...      ...      ...      ...      ...      ...     ...      ...   \n",
       "199995  4.0395  14.0761  -5.7878  16.3870 -14.1721 -13.0269 -2.5955  21.4526   \n",
       "199996  4.2046  14.0195  11.4812  17.9954 -18.3549  -3.4537  1.1233  22.3135   \n",
       "199997  7.5949  11.9158  11.9537  16.9399  -2.2643  -3.3658  6.4020  18.2095   \n",
       "199998  5.5360  16.7484   9.6956  21.4391  -5.1839   6.8296 -9.0318  24.2122   \n",
       "199999  2.4466  15.6721   9.3809  14.7593 -12.8156   3.4928 -3.1634  21.5742   \n",
       "\n",
       "         var_86   var_87   var_88   var_89   var_90  var_91   var_92   var_93  \\\n",
       "0        9.3225  16.4597   7.9984  -1.7069 -21.4494  6.7806  11.0924   9.9913   \n",
       "1      -13.5901  11.0993   7.9022  12.2301   0.4768  6.8852   8.0905  10.9631   \n",
       "2       -4.4577   9.5413  11.9052   2.1447 -22.4038  7.0883  14.1613  10.5080   \n",
       "3       -2.3174  15.6298   4.5474   7.5509  -7.5866  7.0364  14.4027  10.7795   \n",
       "4        7.0799  20.1670   8.0053   3.7954 -39.7997  7.0065   9.3627  10.4316   \n",
       "...         ...      ...      ...      ...      ...     ...      ...      ...   \n",
       "199995  15.6163   0.9845   8.2110  -0.8553 -12.1682  6.7779   7.3895  10.5084   \n",
       "199996   1.9795  16.0239   4.7492   0.2446 -39.6406  6.9473   9.9392  11.1977   \n",
       "199997  17.4710   6.3349   7.4740   4.8024  -0.3345  7.0295  16.5425  10.5645   \n",
       "199998  -7.5779   5.6786  13.1278   7.0086 -32.3247  7.0141   6.9451  10.0272   \n",
       "199999   9.7015  22.4258   7.1213   2.4050  -3.1107  7.1529  16.2315  11.5051   \n",
       "\n",
       "         var_94  var_95   var_96   var_97  var_98  var_99  var_100  var_101  \\\n",
       "0       14.8421  0.1812   8.9642  16.2572  2.1743 -3.4132   9.4763  13.3102   \n",
       "1       11.7569 -1.2722  24.7876  26.6881  1.8944  0.6939 -13.6950   8.4068   \n",
       "2       14.2621  0.2647  20.4031  17.0360  1.6981 -0.0269  -0.3939  12.6317   \n",
       "3        7.2887 -1.0930  11.3596  18.1486  2.8344  1.9480 -19.8592  22.5316   \n",
       "4       14.0553  0.0213  14.7246  35.2988  1.6844  0.6715 -22.9264  12.3562   \n",
       "...         ...     ...      ...      ...     ...     ...      ...      ...   \n",
       "199995  15.5057 -0.6812   5.8999   6.1825  3.1038 -1.6930 -18.8473   9.9358   \n",
       "199996  14.1006 -0.8012  18.8214  32.9827  1.7989 -0.2476 -15.5294   9.5501   \n",
       "199997  12.7330 -0.9946  23.7210  11.2390  1.0012 -1.1083  -8.0574  10.0606   \n",
       "199998  10.0716 -0.3385  19.4605  26.9480  1.7079 -4.8882  -2.3891  24.6626   \n",
       "199999  16.5967  0.6444  21.0773  25.7270  2.4916 -3.0062   0.9636  13.4966   \n",
       "\n",
       "        var_102  var_103  var_104  var_105  var_106  var_107  var_108  \\\n",
       "0       26.5376   1.4403  14.7100   6.0454   9.5426  17.1554  14.1104   \n",
       "1       35.4734   1.7093  15.1866   2.6227   7.3412  32.0888  13.9550   \n",
       "2       14.8863   1.3854  15.0284   3.9995   5.3683   8.6273  14.1963   \n",
       "3       18.6129   1.3512   9.3291   4.2835  10.3907   7.0874  14.3256   \n",
       "4       17.3410   1.6940   7.1179   5.1934   8.8230  10.6617  14.0837   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  25.3359   1.3647  11.8509   5.0357   6.4630  18.4008  14.3787   \n",
       "199996  11.8548   1.5127  11.3998   4.2304   6.6777  11.3434  14.2993   \n",
       "199997  25.2535   1.8019  10.4973   4.2183   9.1158  10.1525  14.0837   \n",
       "199998  19.7783   1.5780  14.3962   4.8206  12.2354  33.9267  14.2625   \n",
       "199999  31.3629   1.5517  12.1898   3.5216   6.8915   9.0475  14.1260   \n",
       "\n",
       "        var_109  var_110  var_111  var_112  var_113  var_114  var_115  \\\n",
       "0       24.3627   2.0323   6.7602   3.9141  -0.4851   2.5240   1.5093   \n",
       "1       13.0858   6.6203   7.1051   5.3523   8.5426   3.6159   4.1569   \n",
       "2       20.3882   3.2304   5.7033   4.5255   2.1929   3.1290   2.9044   \n",
       "3       14.4135   4.2827   6.9750   1.6480  11.6896   2.5762  -2.5459   \n",
       "4       28.2749  -0.1937   5.9654   1.0719   7.9923   2.9138  -3.6135   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  19.0369  -0.6364   6.9155   3.6763   3.1460   4.9442  -1.8289   \n",
       "199996  13.1205  13.3224   7.3143   3.6817   9.7780   4.0491   2.7221   \n",
       "199997  15.2503   3.4797   8.7901   2.9000   0.6471   2.3316   1.5084   \n",
       "199998  26.2407   2.9091   6.4540   5.3290  10.6131   3.4212  -1.8915   \n",
       "199999  15.9937   6.7106   8.1964   6.5520   6.6900   2.4643   4.5841   \n",
       "\n",
       "        var_116  var_117  var_118  var_119  var_120  var_121  var_122  \\\n",
       "0        2.5516  15.5752 -13.4221   7.2739  16.0094   9.7268   0.8897   \n",
       "1        3.0454   7.8522 -11.5100   7.5109  31.5899   9.5018   8.2736   \n",
       "2        1.1696  28.7632 -17.2738   2.1056  21.1613   8.9573   2.7768   \n",
       "3        5.3446  38.1015   3.5732   5.0988  30.5644  11.3025   3.9618   \n",
       "4        1.4684  25.6795  13.8224   4.7478  41.1037  12.7140   5.2964   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   1.3521  34.6265  -0.6869  -5.3781  20.5030  10.9614   4.9677   \n",
       "199996   4.4344   3.7648   2.1927  -2.9197  23.0679  12.2112   3.7517   \n",
       "199997   0.2888  43.0307  -4.4543   3.2765  28.2664  12.1189   3.1526   \n",
       "199998   2.1376  46.4915   1.0591   3.3543  18.1250  10.0102   9.3483   \n",
       "199999   0.7221  23.4020 -10.3157   6.4857  33.4300  12.2439   4.3416   \n",
       "\n",
       "        var_123  var_124  var_125  var_126  var_127  var_128  var_129  \\\n",
       "0        0.7754   4.2218  12.0039  13.8571  -0.7338  -1.9245  15.4462   \n",
       "1       10.1633   0.1225  12.5942  14.5697   2.4354   0.8194  16.5346   \n",
       "2       -2.1746   3.6932  12.4653  14.1978  -2.5511  -0.9479  17.1092   \n",
       "3       -8.2464   2.7038  12.3441  12.5431  -1.3683   3.5974  13.9761   \n",
       "4        9.7289   3.9370  12.1316  12.5815   7.0642   5.6518  10.9346   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   6.1408   2.2575  12.8757  14.2253  -1.2868   0.2212  16.8661   \n",
       "199996   6.7907   6.5622  13.0283  12.2389   4.0627  -1.2406  13.9757   \n",
       "199997  14.2214   3.3878  13.2410  12.9788   4.5766  -4.8512  16.6344   \n",
       "199998  11.0467   2.3866  12.2352  13.5462   3.0043   5.3751  17.1567   \n",
       "199999  10.3869   1.3913  12.7127  13.8530   4.6685  -1.6082  10.1564   \n",
       "\n",
       "        var_130  var_131  var_132  var_133  var_134  var_135  var_136  \\\n",
       "0       12.8287   0.3587   9.6508   6.5674   5.1726   3.1345  29.4547   \n",
       "1       12.4205  -0.1780   5.7582   7.0513   1.9568  -8.9921   9.7797   \n",
       "2       11.5419   0.0975   8.8186   6.6231   3.9358 -11.7218  24.5437   \n",
       "3       14.3003   1.0486   8.9500   7.1954  -1.1984   1.9586  27.5609   \n",
       "4       11.4266   0.9442   7.7532   6.6173  -6.8304   6.4730  17.1728   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  12.7663   1.2414   7.1304   7.4108  -6.3369   3.0760  24.9796   \n",
       "199996  12.6133   0.6524   8.3929   6.9125  -6.0942  -6.3209  38.8105   \n",
       "199997  12.3827   0.5293   8.0588   7.1081  -9.2317 -11.9277  20.5706   \n",
       "199998  11.6873   0.6677   8.3511   6.5834   1.6146   4.8462  15.2331   \n",
       "199999  11.7936   0.2316   8.1760   7.6166 -18.3865  -7.3542  32.6663   \n",
       "\n",
       "        var_137  var_138  var_139  var_140  var_141  var_142  var_143  \\\n",
       "0       31.4045   2.8279  15.6599   8.3307  -5.6011  19.0614  11.2663   \n",
       "1       18.1577  -1.9721  16.1622   3.6937   6.6803  -0.3243  12.2806   \n",
       "2       15.5827   3.8212   8.6674   7.3834  -2.4438  10.2158   7.4844   \n",
       "3       24.6065  -2.8233   8.9821   3.8873  15.9638  10.0142   7.8388   \n",
       "4       25.8128   2.6791  13.9547   6.6289  -4.3965  11.7159  16.1080   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  20.3410   5.3312  23.7116   2.4745  11.2013  17.8165  13.0057   \n",
       "199996  17.6153  -2.9070   0.8270   2.0615   0.9315   6.5953  17.2099   \n",
       "199997  22.5568   3.0665   1.0527   7.4011   4.3367   1.4242  11.3654   \n",
       "199998   3.8390   0.6656   2.2357  14.8203   5.8648   8.7190  15.1468   \n",
       "199999  15.5464  -0.9083   1.0662   5.3922   9.7708  11.4687   8.6980   \n",
       "\n",
       "        var_144  var_145  var_146  var_147  var_148  var_149  var_150  \\\n",
       "0        8.6989   8.3694  11.5659 -16.4727   4.0288  17.9244  18.5177   \n",
       "1        8.6086  11.0738   8.9231  11.7700   4.2578  -4.4223  20.6294   \n",
       "2        9.1104   4.3649  11.4934   1.7624   4.0714  -1.2681  14.3330   \n",
       "3        9.9718   2.9253  10.4994   4.1622   3.7613   2.3701  18.0984   \n",
       "4        7.6874   9.1570  11.5670 -12.7047   3.7574   9.9110  20.1461   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   9.5506   5.3589  13.2491  -3.3068   3.6998   2.5927  14.3025   \n",
       "199996   9.3960   9.9801   3.7881   2.9866   3.8695  17.8068  18.7807   \n",
       "199997   9.1812   2.7627  12.2434  -0.2420   4.1575   4.7996  20.6307   \n",
       "199998   9.9930  10.4543  10.9535 -10.3405   3.6463 -16.8622  18.8580   \n",
       "199999   8.0106  13.1911  12.3484   0.2655   3.8811  -9.4762  11.6612   \n",
       "\n",
       "        var_151  var_152  var_153  var_154  var_155  var_156  var_157  \\\n",
       "0       10.7800   9.0056  16.6964  10.4838   1.6573  12.1749 -13.1324   \n",
       "1       14.8743   9.4317  16.7242  -0.5687   0.1898  12.2419  -9.6953   \n",
       "2        8.0088   4.4015  14.1479  -5.1747   0.5778  14.5362  -1.7624   \n",
       "3       17.1765   7.6508  18.2452  17.0336 -10.9370  12.0500  -1.2155   \n",
       "4        1.2995   5.8493  19.8234   4.7022  10.6101  13.0021 -12.6068   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   8.1596   7.9609  18.3343   4.3086   1.3546  12.4158  -5.3985   \n",
       "199996   9.4546   4.4657  17.8085  13.3077  -1.3209  12.7288 -12.3625   \n",
       "199997  10.2890   5.6890  13.4601  -0.9774   2.3728  11.7245  -9.6385   \n",
       "199998   8.2192  -0.4073  16.7224   8.8882  -3.2567  12.9142  -8.5421   \n",
       "199999  13.1571   8.5043  17.0369   7.1124 -13.1967  13.9404  -8.3303   \n",
       "\n",
       "        var_158  var_159  var_160  var_161  var_162  var_163  var_164  \\\n",
       "0       17.6054  11.5423  15.4576   5.3133   3.6159   5.0384   6.6760   \n",
       "1       22.3949  10.6261  29.4846   5.8683   3.8208  15.8348  -5.0121   \n",
       "2       33.8820  11.6041  13.2070   5.8442   4.7086   5.7141  -1.0410   \n",
       "3       19.9750  12.3892  31.8833   5.9684   7.2084   3.8899 -11.0882   \n",
       "4       27.0846   8.0913  33.5107   5.6953   5.4663  18.2201   6.5769   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  16.3683  10.4522  35.4923   5.5477   7.4244  12.5459  -6.7840   \n",
       "199996  15.3500  11.1798  35.1445   5.5375   5.6397  17.0598  -9.7142   \n",
       "199997  17.3101  14.0422  19.9293   5.3427   5.4776  13.1202   5.3500   \n",
       "199998  15.9319   5.8348  40.3378   5.5357   4.6151   8.5910 -12.6998   \n",
       "199999  29.0140   9.6174  15.9041   5.3187   6.2987  13.0729  -4.2045   \n",
       "\n",
       "        var_165  var_166  var_167  var_168  var_169  var_170  var_171  \\\n",
       "0       12.6644   2.7004  -0.6975   9.5981   5.4879  -4.7645  -8.4254   \n",
       "1       15.1345   3.2003   9.3192   3.8821   5.7999   5.5378   5.0988   \n",
       "2       20.5092   3.2790  -5.5952   7.3176   5.7690  -7.0927  -3.9116   \n",
       "3       17.2502   2.5881  -2.7018   0.5641   5.3430  -7.1541  -6.1920   \n",
       "4       21.2607   3.2304  -1.7759   3.1283   5.5518   1.4493  -2.6627   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  31.1895   2.6529 -11.1867   9.8865   5.4730  -5.3880  -0.4698   \n",
       "199996  15.5117   3.3696 -17.1855   2.8292   5.2606   2.6836   5.8767   \n",
       "199997  31.7346   3.1693 -19.4779   6.8053   5.6281  -0.8774  -8.9508   \n",
       "199998  25.8578   2.2346  -6.4988   2.6702   5.3868  -7.1875   8.1477   \n",
       "199999  19.2141   3.2902  -1.2175   4.1583   5.7675   5.7719  -1.2139   \n",
       "\n",
       "        var_172  var_173  var_174  var_175  var_176  var_177  var_178  \\\n",
       "0       20.8773   3.1531  18.5618   7.7423 -10.1245  13.7241  -3.5189   \n",
       "1       22.0330   5.5134  30.2645  10.4968  -7.2352  16.5721  -7.3477   \n",
       "2        7.2569  -5.8234  25.6820  10.9202  -0.3104   8.8438  -9.7009   \n",
       "3       18.2366  11.7134  14.7483   8.1013  11.8771  13.9552 -10.4701   \n",
       "4       19.8056   2.3705  18.4685  16.3309  -3.3456  13.5261   1.7189   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995  24.4025  -5.4493  11.3529   7.7075  -5.0491  13.0756  15.8271   \n",
       "199996  25.1262   7.3478  27.1264  11.8542   9.7999  11.1395  -3.2870   \n",
       "199997  17.4931  -1.6530  32.0032  12.5749   5.8756   8.8059 -10.6367   \n",
       "199998  22.4362  -2.5914   8.8704  11.6621   7.4904   8.1808 -11.4177   \n",
       "199999  21.8496  -3.5368  25.9094  11.7673   1.9765  15.9218   3.9350   \n",
       "\n",
       "        var_179  var_180  var_181  var_182  var_183  var_184  var_185  \\\n",
       "0        1.7202  -8.4051   9.0164   3.0657  14.3691  25.8398   5.8764   \n",
       "1       11.0752  -5.5937   9.4878 -14.9100   9.4245  22.5441  -4.8622   \n",
       "2        2.4013  -4.2935   9.3908 -13.2648   3.1545  23.0866  -5.3000   \n",
       "3        5.6961  -3.7546   8.4117   1.8986   7.2601  -0.4639  -0.0498   \n",
       "4        5.1743  -7.6938   9.7685   4.8910  12.2198  11.8503  -7.8931   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   3.3580 -14.3371  10.4421   7.6530   9.4585  22.7783  -4.0305   \n",
       "199996   0.4285   2.5058  10.0339   9.1610   9.4318  13.4913   4.6247   \n",
       "199997   5.4401 -12.7967   8.7990   0.7021  14.9744  18.9211   0.3016   \n",
       "199998   2.8379   3.8748   8.7410   8.9998  16.4058  11.3244  -2.1751   \n",
       "199999   4.3993 -10.3268  10.5200   9.9587  11.9242   7.0626  -6.5429   \n",
       "\n",
       "        var_186  var_187  var_188  var_189  var_190  var_191  var_192  \\\n",
       "0       11.8411 -19.7159  17.5743   0.5857   4.4354   3.9642   3.1364   \n",
       "1        7.6543 -15.9319  13.3175  -0.3566   7.6421   7.7214   2.5837   \n",
       "2        5.3745  -6.2660  10.1934  -0.8417   2.9057   9.7905   1.6704   \n",
       "3        7.9336 -12.8279  12.4124   1.8489   4.4666   4.7433   0.7178   \n",
       "4        6.4209   5.9270  16.0201  -0.2829  -1.4905   9.5214  -0.1508   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "199995   4.2233  -6.3906  13.5058  -0.4594   6.1415  13.2305   3.9901   \n",
       "199996   6.2906 -17.8522  18.6751  -0.1162   4.9611   4.6549   0.6998   \n",
       "199997  11.2869  -6.3741  12.9726   2.3425   4.0651   5.4414   3.1032   \n",
       "199998  12.4735 -18.3932  12.6337   0.3243   2.6840   8.6587   2.7337   \n",
       "199999  10.5947  -3.8827  16.3552   1.7535   8.9842   1.6893   0.1276   \n",
       "\n",
       "        var_193  var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0        1.6910  18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "1       10.9516  15.4305   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "2        1.6858  21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "3        1.4214  23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4        9.1942  13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104  \n",
       "...         ...      ...      ...      ...      ...      ...      ...  \n",
       "199995   0.9388  18.0249  -1.7939   2.1661   8.5326  16.6660 -17.8661  \n",
       "199996   1.8341  22.2717   1.7337  -2.1651   6.7419  15.9054   0.3388  \n",
       "199997   4.8793  23.5311  -1.5736   1.2832   8.7155  13.8329   4.1995  \n",
       "199998  11.1178  20.4158  -0.0786   6.7980  10.0342  15.5289 -13.9001  \n",
       "199999   0.3766  15.2101  -2.4907  -2.2342   8.1857  12.1284   0.1385  \n",
       "\n",
       "[200000 rows x 201 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop('ID_code',axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85591d0",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223168be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3caa933a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target     0\n",
       "var_0      0\n",
       "var_1      0\n",
       "var_2      0\n",
       "var_3      0\n",
       "          ..\n",
       "var_195    0\n",
       "var_196    0\n",
       "var_197    0\n",
       "var_198    0\n",
       "var_199    0\n",
       "Length: 201, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3407a9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "target     0\n",
      "var_0      0\n",
      "var_1      0\n",
      "var_2      0\n",
      "var_3      0\n",
      "          ..\n",
      "var_195    0\n",
      "var_196    0\n",
      "var_197    0\n",
      "var_198    0\n",
      "var_199    0\n",
      "Length: 201, dtype: int64\n",
      "\n",
      "Missing Values After Handling:\n",
      "target     0\n",
      "var_0      0\n",
      "var_1      0\n",
      "var_2      0\n",
      "var_3      0\n",
      "          ..\n",
      "var_195    0\n",
      "var_196    0\n",
      "var_197    0\n",
      "var_198    0\n",
      "var_199    0\n",
      "Length: 201, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Handle missing values\n",
    "# For demonstration, let's impute missing values with the mean for numerical features\n",
    "# and with the most frequent category for categorical features\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'float64':\n",
    "        data[column].fillna(data[column].mean(), inplace=True)\n",
    "    elif data[column].dtype == 'object':\n",
    "        data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "\n",
    "# Alternatively, you can remove rows with missing values\n",
    "# data.dropna(inplace=True)\n",
    "data.dropna(axis=1, inplace=True)\n",
    "\n",
    "# Verify if missing values are handled\n",
    "missing_values_after_handling = data.isnull().sum()\n",
    "print(\"\\nMissing Values After Handling:\")\n",
    "print(missing_values_after_handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c6ad286",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for Numerical Features:\n",
      "              target          var_0          var_1          var_2  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        0.100490      10.679914      -1.627622      10.715192   \n",
      "std         0.300653       3.040051       4.050044       2.640894   \n",
      "min         0.000000       0.408400     -15.043400       2.117100   \n",
      "25%         0.000000       8.453850      -4.740025       8.722475   \n",
      "50%         0.000000      10.524750      -1.608050      10.580000   \n",
      "75%         0.000000      12.758200       1.358625      12.516700   \n",
      "max         1.000000      20.315000      10.376800      19.353000   \n",
      "\n",
      "               var_3          var_4          var_5          var_6  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        6.796529      11.078333      -5.065317       5.408949   \n",
      "std         2.043319       1.623150       7.863267       0.866607   \n",
      "min        -0.040200       5.074800     -32.562600       2.347300   \n",
      "25%         5.254075       9.883175     -11.200350       4.767700   \n",
      "50%         6.825000      11.108250      -4.833150       5.385100   \n",
      "75%         8.324100      12.261125       0.924800       6.003000   \n",
      "max        13.188300      16.671400      17.251600       8.447700   \n",
      "\n",
      "               var_7          var_8          var_9         var_10  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       16.545850       0.284162       7.567236       0.394340   \n",
      "std         3.418076       3.332634       1.235070       5.500793   \n",
      "min         5.349700     -10.505500       3.970500     -20.731300   \n",
      "25%        13.943800      -2.317800       6.618800      -3.594950   \n",
      "50%        16.456800       0.393700       7.629600       0.487300   \n",
      "75%        19.102900       2.937900       8.584425       4.382925   \n",
      "max        27.691800      10.151300      11.150600      18.670200   \n",
      "\n",
      "              var_11         var_12         var_13         var_14  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -3.245596      14.023978       8.530232       7.537606   \n",
      "std         5.970253       0.190059       4.639536       2.247908   \n",
      "min       -26.095000      13.434600      -6.011100       1.013300   \n",
      "25%        -7.510600      13.894000       5.072800       5.781875   \n",
      "50%        -3.286950      14.025500       8.604250       7.520300   \n",
      "75%         0.852825      14.164200      12.274775       9.270425   \n",
      "max        17.188700      14.654500      22.331500      14.937700   \n",
      "\n",
      "              var_15         var_16         var_17         var_18  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       14.573126       9.333264      -5.696731      15.244013   \n",
      "std         0.411711       2.557421       6.712612       7.851370   \n",
      "min        13.076900       0.635100     -33.380200     -10.664200   \n",
      "25%        14.262800       7.452275     -10.476225       9.177950   \n",
      "50%        14.574100       9.232050      -5.666350      15.196250   \n",
      "75%        14.874500      11.055900      -0.810775      21.013325   \n",
      "max        15.863300      17.950600      19.025900      41.748000   \n",
      "\n",
      "              var_19         var_20         var_21         var_22  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       12.438567      13.290894      17.257883       4.305430   \n",
      "std         7.996694       5.876254       8.196564       2.847958   \n",
      "min       -12.402500      -5.432200     -10.089000      -5.322500   \n",
      "25%         6.276475       8.627800      11.551000       2.182400   \n",
      "50%        12.453900      13.196800      17.234250       4.275150   \n",
      "75%        18.433300      17.879400      23.089050       6.293200   \n",
      "max        35.183000      31.285900      49.044300      14.594500   \n",
      "\n",
      "              var_23         var_24         var_25         var_26  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        3.019540      10.584400      13.667496      -4.055133   \n",
      "std         0.526893       3.777245       0.285535       5.922210   \n",
      "min         1.209800      -0.678400      12.720000     -24.243100   \n",
      "25%         2.634100       7.613000      13.456400      -8.321725   \n",
      "50%         3.008650      10.380350      13.662500      -4.196900   \n",
      "75%         3.403800      13.479600      13.863700      -0.090200   \n",
      "max         4.875200      25.446000      14.654600      15.675100   \n",
      "\n",
      "              var_27         var_28         var_29         var_30  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -1.137908       5.532980       5.053874      -7.687740   \n",
      "std         1.523714       0.783367       2.615942       7.965198   \n",
      "min        -6.166800       2.089600      -4.787200     -34.798400   \n",
      "25%        -2.307900       4.992100       3.171700     -13.766175   \n",
      "50%        -1.132100       5.534850       4.950200      -7.411750   \n",
      "75%         0.015625       6.093700       6.798925      -1.443450   \n",
      "max         3.243100       8.787400      13.143100      15.651500   \n",
      "\n",
      "              var_31         var_32         var_33         var_34  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       10.393046      -0.512886      14.774147      11.434250   \n",
      "std         2.159891       2.587830       4.322325       0.541614   \n",
      "min         2.140600      -8.986100       1.508500       9.816900   \n",
      "25%         8.870000      -2.500875      11.456300      11.032300   \n",
      "50%        10.365650      -0.497650      14.576000      11.435200   \n",
      "75%        11.885000       1.469100      18.097125      11.844400   \n",
      "max        20.171900       6.787100      29.546600      13.287800   \n",
      "\n",
      "              var_35         var_36         var_37         var_38  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        3.842499       2.187230       5.868899      10.642131   \n",
      "std         5.179559       3.119978       2.249730       4.278903   \n",
      "min       -16.513600      -8.095100      -1.183400      -6.337100   \n",
      "25%         0.116975      -0.007125       4.125475       7.591050   \n",
      "50%         3.917750       2.198000       5.900650      10.562700   \n",
      "75%         7.487725       4.460400       7.542400      13.598925   \n",
      "max        21.528900      14.245600      11.863800      29.823500   \n",
      "\n",
      "              var_39         var_40         var_41         var_42  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        0.662956      -6.725505       9.299858      11.222356   \n",
      "std         4.068845       8.279259       5.938088       0.695991   \n",
      "min       -14.545700     -35.211700      -8.535900       8.859000   \n",
      "25%        -2.199500     -12.831825       4.519575      10.713200   \n",
      "50%         0.672300      -6.617450       9.162650      11.243400   \n",
      "75%         3.637825      -0.880875      13.754800      11.756900   \n",
      "max        15.322300      18.105600      26.165800      13.469600   \n",
      "\n",
      "              var_43         var_44         var_45         var_46  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       11.569954       8.948289     -12.699667      11.326488   \n",
      "std         0.309599       5.903073      21.404912       2.860511   \n",
      "min        10.652800      -9.939600     -90.252500       1.206200   \n",
      "25%        11.343800       5.313650     -28.730700       9.248750   \n",
      "50%        11.565000       9.437200     -12.547200      11.310750   \n",
      "75%        11.804600      13.087300       3.150525      13.318300   \n",
      "max        12.577900      34.196100      62.084400      21.293900   \n",
      "\n",
      "              var_47         var_48         var_49         var_50  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean      -12.471737      14.704713      16.682499      12.740986   \n",
      "std        10.579862      11.384332       7.855762       0.691709   \n",
      "min       -47.686200     -23.902200      -8.070700      10.385500   \n",
      "25%       -20.654525       6.351975      10.653475      12.269000   \n",
      "50%       -12.482400      14.559200      16.672400      12.745600   \n",
      "75%        -4.244525      23.028650      22.549050      13.234500   \n",
      "max        20.685400      54.273800      41.153000      15.317200   \n",
      "\n",
      "              var_51         var_52         var_53         var_54  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       13.428912      -2.528816       6.008569       1.137117   \n",
      "std         8.187306       4.985532       0.764753       8.414241   \n",
      "min       -15.046200     -24.721400       3.344900     -26.778600   \n",
      "25%         7.267625      -6.065025       5.435600      -5.147625   \n",
      "50%        13.444400      -2.502450       6.027800       1.274050   \n",
      "75%        19.385650       0.944350       6.542900       7.401825   \n",
      "max        40.689000      17.096800       8.231500      28.572400   \n",
      "\n",
      "              var_55         var_56         var_57         var_58  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       12.745852      16.629165       6.272014       3.177633   \n",
      "std         5.690072       3.540174       0.795026       4.296686   \n",
      "min        -3.782600       2.761800       3.442300     -12.600900   \n",
      "25%         8.163900      14.097875       5.687500       0.183500   \n",
      "50%        12.594100      16.648150       6.262500       3.170100   \n",
      "75%        17.086625      19.289700       6.845000       6.209700   \n",
      "max        29.092100      29.074100       9.160900      20.483300   \n",
      "\n",
      "              var_59         var_60         var_61         var_62  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        8.931124      12.155618     -11.946744       0.874170   \n",
      "std         0.854798       4.222389      11.622948       2.026238   \n",
      "min         6.184000      -2.100600     -48.802700      -6.328900   \n",
      "25%         8.312400       8.912750     -20.901725      -0.572400   \n",
      "50%         8.901000      12.064350     -11.892000       0.794700   \n",
      "75%         9.566525      15.116500      -3.225450       2.228200   \n",
      "max        11.986700      25.195500      27.102900       7.753600   \n",
      "\n",
      "              var_63         var_64         var_65         var_66  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        0.661173       6.369157       0.982891       5.794039   \n",
      "std         3.113089       1.485854       3.786493       1.121366   \n",
      "min       -10.554400       1.611700     -14.088800       1.336800   \n",
      "25%        -1.588700       5.293500      -1.702800       4.973800   \n",
      "50%         0.681700       6.377700       1.021350       5.782000   \n",
      "75%         3.020300       7.490600       3.739200       6.586200   \n",
      "max        11.231700      11.153700      15.731300       9.713200   \n",
      "\n",
      "              var_67         var_68         var_69         var_70  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       11.943223       5.018893      -3.331515      24.446811   \n",
      "std         7.365115       0.007186       3.955723      11.951742   \n",
      "min       -19.544300       4.993800     -16.309400     -17.027500   \n",
      "25%         6.753200       5.014000      -6.336625      15.256625   \n",
      "50%        11.922000       5.019100      -3.325500      24.445000   \n",
      "75%        17.037650       5.024100      -0.498875      33.633150   \n",
      "max        39.396800       5.046900       8.547300      64.464400   \n",
      "\n",
      "              var_71         var_72         var_73         var_74  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        0.669756       0.640553      19.610888      19.518846   \n",
      "std         0.266696       3.944703       7.466303      14.112591   \n",
      "min        -0.224000     -12.383400      -1.665800     -34.101500   \n",
      "25%         0.472300      -2.197100      14.097275       9.595975   \n",
      "50%         0.668400       0.646450      19.309750      19.536650   \n",
      "75%         0.864400       3.510700      25.207125      29.620700   \n",
      "max         1.571900      14.150000      44.536100      70.272000   \n",
      "\n",
      "              var_75         var_76         var_77         var_78  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       16.853732       6.050871      19.066993       5.349479   \n",
      "std         6.055322       7.938351       3.817292       1.993792   \n",
      "min        -1.293600     -21.633300       7.425700      -1.818300   \n",
      "25%        12.480975       0.596300      16.014700       3.817275   \n",
      "50%        16.844200       6.297800      18.967850       5.440050   \n",
      "75%        21.432225      11.818800      22.041100       6.867200   \n",
      "max        36.156700      34.435200      30.956900      11.350700   \n",
      "\n",
      "              var_79         var_80         var_81         var_82  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       14.402136       5.795044      14.719024      -3.471273   \n",
      "std         1.309055       7.436737       2.299567       8.479255   \n",
      "min        10.445400     -18.042200       7.586500     -30.026600   \n",
      "25%        13.375400       0.694475      13.214775     -10.004950   \n",
      "50%        14.388850       6.061750      14.844500      -3.284450   \n",
      "75%        15.383100      11.449125      16.340800       3.101725   \n",
      "max        18.225600      30.476900      23.132400      21.893400   \n",
      "\n",
      "              var_83         var_84         var_85         var_86  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        1.025817      -2.590209      18.362721       5.621058   \n",
      "std         8.297229       6.225305       3.908536       7.751142   \n",
      "min       -24.220100     -24.439800       7.023000     -19.272200   \n",
      "25%        -5.106400      -7.216125      15.338575       0.407550   \n",
      "50%         1.069700      -2.517950      18.296450       6.006700   \n",
      "75%         7.449900       1.986700      21.358850      11.158375   \n",
      "max        27.714300      17.742400      32.901100      34.563700   \n",
      "\n",
      "              var_87         var_88         var_89         var_90  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       11.351483       8.702924       3.725208     -16.548147   \n",
      "std         5.661867       2.491460       3.560554      13.152810   \n",
      "min        -8.481600       1.350200      -9.601400     -61.718000   \n",
      "25%         7.247175       6.918775       1.140500     -26.665600   \n",
      "50%        11.288000       8.616200       3.642550     -16.482600   \n",
      "75%        15.433225      10.567025       6.146200      -6.409375   \n",
      "max        33.354100      17.459400      15.481600      27.271300   \n",
      "\n",
      "              var_91         var_92         var_93         var_94  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        6.987541      12.739578      10.556740      10.999162   \n",
      "std         0.152641       4.186252       0.543341       2.768099   \n",
      "min         6.521800      -1.018500       8.491600       2.819000   \n",
      "25%         6.869900       9.670300      10.195600       8.828000   \n",
      "50%         6.986500      12.673500      10.582200      10.983850   \n",
      "75%         7.101400      15.840225      10.944900      13.089100   \n",
      "max         7.489500      26.997600      12.534300      18.975000   \n",
      "\n",
      "              var_95         var_96         var_97         var_98  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -0.084344      14.400433      18.539645       1.752012   \n",
      "std         0.621125       8.525400      12.642382       0.715836   \n",
      "min        -2.432400     -12.158400     -21.740000      -0.603500   \n",
      "25%        -0.527400       7.796950       8.919525       1.267675   \n",
      "50%        -0.098600      14.369900      18.502150       1.768300   \n",
      "75%         0.329100      20.819375      28.158975       2.260900   \n",
      "max         1.804000      40.880600      58.287900       4.502800   \n",
      "\n",
      "              var_99        var_100        var_101        var_102  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -0.746296      -6.600518      13.413526      22.294908   \n",
      "std         1.862550       9.181683       4.950537       8.628179   \n",
      "min        -7.280600     -39.179100       0.075700      -7.382900   \n",
      "25%        -2.106200     -13.198700       9.639800      16.047975   \n",
      "50%        -0.771300      -6.401500      13.380850      22.306850   \n",
      "75%         0.528500       0.132100      17.250225      28.682225   \n",
      "max         5.076400      25.140900      28.459400      51.326500   \n",
      "\n",
      "             var_103        var_104        var_105        var_106  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        1.568393      11.509834       4.244744       8.617657   \n",
      "std         0.185020       1.970520       0.855698       1.894899   \n",
      "min         0.979300       4.084600       0.715300       0.942400   \n",
      "25%         1.428900      10.097900       3.639600       7.282300   \n",
      "50%         1.566000      11.497950       4.224500       8.605150   \n",
      "75%         1.705400      12.902100       4.822200       9.928900   \n",
      "max         2.188700      19.020600       7.169200      15.307400   \n",
      "\n",
      "             var_107        var_108        var_109        var_110  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       17.796266      14.224435      18.458001       5.513238   \n",
      "std         7.604723       0.171091       4.355031       3.823253   \n",
      "min        -5.898000      13.729000       5.769700      -9.239800   \n",
      "25%        12.168075      14.098900      15.107175       2.817475   \n",
      "50%        17.573200      14.226600      18.281350       5.394300   \n",
      "75%        23.348600      14.361800      21.852900       8.104325   \n",
      "max        46.379500      14.743000      32.059100      19.519300   \n",
      "\n",
      "             var_111        var_112        var_113        var_114  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        6.312603       3.317843       8.136542       3.081191   \n",
      "std         1.082404       1.591170       4.459077       0.985396   \n",
      "min         2.194200      -2.030200      -5.513900      -0.050500   \n",
      "25%         5.510100       2.092675       4.803250       2.388775   \n",
      "50%         6.340100       3.408400       8.148550       3.083800   \n",
      "75%         7.080300       4.577400      11.596200       3.811900   \n",
      "max         9.800200       8.431700      21.542100       6.585000   \n",
      "\n",
      "             var_115        var_116        var_117        var_118  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        2.213717       2.402570      16.102233      -5.305132   \n",
      "std         2.621851       1.650912      13.297662       8.799268   \n",
      "min        -6.858600      -3.163000     -31.836900     -37.527700   \n",
      "25%         0.399700       1.171875       6.373500     -11.587850   \n",
      "50%         2.249850       2.456300      15.944850      -5.189500   \n",
      "75%         4.121500       3.665100      25.780825       0.971800   \n",
      "max        11.950400       8.120700      64.810900      25.263500   \n",
      "\n",
      "             var_119        var_120        var_121        var_122  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        3.032849      24.521078      11.310591       1.192984   \n",
      "std         4.182796      12.121016       1.714416       5.168479   \n",
      "min        -9.774200     -18.696200       6.305200     -15.194000   \n",
      "25%        -0.161975      15.696275       9.996400      -2.565200   \n",
      "50%         3.023950      24.354700      11.239700       1.200700   \n",
      "75%         6.098400      33.105275      12.619425       5.091700   \n",
      "max        15.688500      74.032100      17.307400      18.471400   \n",
      "\n",
      "             var_123        var_124        var_125        var_126  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        7.076254       4.272740      12.489165      13.202326   \n",
      "std         6.147345       2.736821       0.318100       0.776056   \n",
      "min       -12.405900      -7.053800      11.486100      11.265400   \n",
      "25%         2.817050       2.353600      12.245400      12.608400   \n",
      "50%         7.234300       4.302100      12.486300      13.166800   \n",
      "75%        11.734750       6.192200      12.718100      13.811700   \n",
      "max        26.874900      14.991500      13.664200      15.515600   \n",
      "\n",
      "             var_127        var_128        var_129        var_130  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        0.851507      -1.127952      15.460314      12.257151   \n",
      "std         3.137684       3.238043       4.136453       0.832199   \n",
      "min        -8.876900     -11.755900       2.186300       9.528300   \n",
      "25%        -1.502325      -3.580725      12.514475      11.619300   \n",
      "50%         0.925000      -1.101750      15.426800      12.264650   \n",
      "75%         3.293000       1.351700      18.480400      12.876700   \n",
      "max        10.597600       9.809600      31.203600      14.989500   \n",
      "\n",
      "             var_131        var_132        var_133        var_134  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        0.544674       7.799676       6.813270      -4.826053   \n",
      "std         0.456280       1.456486       0.375603       6.166126   \n",
      "min        -0.954800       2.890000       5.359300     -24.254600   \n",
      "25%         0.207800       6.724375       6.543500      -9.625700   \n",
      "50%         0.556600       7.809100       6.806700      -4.704250   \n",
      "75%         0.901000       8.911425       7.070800      -0.178800   \n",
      "max         2.192300      12.465000       8.309100      12.723600   \n",
      "\n",
      "             var_135        var_136        var_137        var_138  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -4.259472      22.968602      17.613651       1.210792   \n",
      "std         7.617732      10.382235       8.890516       4.551750   \n",
      "min       -31.380800      -9.949300      -9.851000     -16.468400   \n",
      "25%        -9.957100      14.933900      10.656550      -2.011825   \n",
      "50%        -4.111900      22.948300      17.257250       1.211750   \n",
      "75%         1.125950      31.042425      24.426025       4.391225   \n",
      "max        21.412800      54.579400      44.437600      18.818700   \n",
      "\n",
      "             var_139        var_140        var_141        var_142  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        7.760193       3.423636       2.897596      11.983489   \n",
      "std         7.686433       4.896325       6.715637       5.691936   \n",
      "min       -21.274300     -15.459500     -16.693700      -7.108000   \n",
      "25%         2.387575      -0.121700      -2.153725       7.900000   \n",
      "50%         8.066250       3.564700       2.975500      11.855900   \n",
      "75%        13.232525       7.078525       8.192425      16.073925   \n",
      "max        36.097100      21.121900      23.965800      32.891100   \n",
      "\n",
      "             var_143        var_144        var_145        var_146  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       12.333698       8.647632       4.841328      10.341178   \n",
      "std         2.934706       0.922469       3.899281       2.518883   \n",
      "min         2.806800       5.444300      -8.273400       0.427400   \n",
      "25%        10.311200       7.968075       1.885875       8.646900   \n",
      "50%        12.356350       8.651850       4.904700      10.395600   \n",
      "75%        14.461050       9.315000       7.676925      12.113225   \n",
      "max        22.691600      11.810100      16.008300      20.437300   \n",
      "\n",
      "             var_147        var_148        var_149        var_150  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -3.300779       3.990726       5.296237      16.817671   \n",
      "std         7.413301       0.199192      10.385133       2.464157   \n",
      "min       -29.984000       3.320500     -41.168300       9.242000   \n",
      "25%        -8.751450       3.853600      -1.903200      14.952200   \n",
      "50%        -3.178700       3.996000       5.283250      16.736950   \n",
      "75%         2.028275       4.131600      12.688225      18.682500   \n",
      "max        22.149400       4.752800      48.424000      25.435700   \n",
      "\n",
      "             var_151        var_152        var_153        var_154  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       10.141542       7.633199      16.727902       6.974955   \n",
      "std         3.962426       3.005373       2.014200       4.961678   \n",
      "min        -2.191500      -2.880000      11.030800      -8.196600   \n",
      "25%         7.064600       5.567900      15.233000       3.339900   \n",
      "50%        10.127900       7.673700      16.649750       6.994050   \n",
      "75%        13.057600       9.817300      18.263900      10.766350   \n",
      "max        21.124500      18.384600      24.007500      23.242800   \n",
      "\n",
      "             var_155        var_156        var_157        var_158  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -2.074128      13.209272      -4.813552      17.914591   \n",
      "std         5.771261       0.955140       5.570272       7.885579   \n",
      "min       -21.840900       9.996500     -22.990400      -4.554400   \n",
      "25%        -6.266025      12.475100      -8.939950      12.109200   \n",
      "50%        -2.066100      13.184300      -4.868400      17.630450   \n",
      "75%         1.891750      13.929300      -0.988575      23.875325   \n",
      "max        16.831600      16.497000      11.972100      44.779500   \n",
      "\n",
      "             var_159        var_160        var_161        var_162  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       10.223282      24.259300       5.633293       5.362896   \n",
      "std         4.122912      10.880263       0.217938       1.419612   \n",
      "min        -4.641600      -7.452200       4.852600       0.623100   \n",
      "25%         7.243525      15.696125       5.470500       4.326100   \n",
      "50%        10.217550      23.864500       5.633500       5.359700   \n",
      "75%        13.094525      32.622850       5.792000       6.371200   \n",
      "max        25.120000      58.394200       6.309900      10.134400   \n",
      "\n",
      "             var_163        var_164        var_165        var_166  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       11.002170      -2.871906      19.315753       2.963335   \n",
      "std         5.262056       5.457784       5.024182       0.369684   \n",
      "min        -6.531700     -19.997700       3.816700       1.851200   \n",
      "25%         7.029600      -7.094025      15.744550       2.699000   \n",
      "50%        10.788700      -2.637800      19.270800       2.960200   \n",
      "75%        14.623900       1.323600      23.024025       3.241500   \n",
      "max        27.564800      12.119300      38.332200       4.220400   \n",
      "\n",
      "             var_167        var_168        var_169        var_170  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -4.151155       4.937124       5.636008      -0.004962   \n",
      "std         7.798020       3.105986       0.369437       4.424621   \n",
      "min       -35.969500      -5.250200       4.258800     -14.506000   \n",
      "25%        -9.643100       2.703200       5.374600      -3.258500   \n",
      "50%        -4.011600       4.761600       5.634300       0.002800   \n",
      "75%         1.318725       7.020025       5.905400       3.096400   \n",
      "max        21.276600      14.886100       7.089000      16.731900   \n",
      "\n",
      "             var_171        var_172        var_173        var_174  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -0.831777      19.817094      -0.677967      20.210677   \n",
      "std         5.378008       8.674171       5.966674       7.136427   \n",
      "min       -22.479300     -11.453300     -22.748700      -2.995300   \n",
      "25%        -4.720350      13.731775      -5.009525      15.064600   \n",
      "50%        -0.807350      19.748000      -0.569750      20.206100   \n",
      "75%         2.956800      25.907725       3.619900      25.641225   \n",
      "max        17.917300      53.591900      18.855400      43.546800   \n",
      "\n",
      "             var_175        var_176        var_177        var_178  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       11.640613      -2.799585      11.882933      -1.014064   \n",
      "std         2.892167       7.513939       2.628895       8.579810   \n",
      "min         3.241500     -29.116500       4.952100     -29.273400   \n",
      "25%         9.371600      -8.386500       9.808675      -7.395700   \n",
      "50%        11.679800      -2.538450      11.737250      -0.942050   \n",
      "75%        13.745500       2.704400      13.931300       5.338750   \n",
      "max        20.854800      20.245200      20.596500      29.841300   \n",
      "\n",
      "             var_179        var_180        var_181        var_182  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        2.591444      -2.741666      10.085518       0.719109   \n",
      "std         2.798956       5.261243       1.371862       8.963434   \n",
      "min        -7.856100     -22.037400       5.416500     -26.001100   \n",
      "25%         0.625575      -6.673900       9.084700      -6.064425   \n",
      "50%         2.512300      -2.688800      10.036050       0.720200   \n",
      "75%         4.391125       0.996200      11.011300       7.499175   \n",
      "max        13.448700      12.750500      14.393900      29.248700   \n",
      "\n",
      "             var_183        var_184        var_185        var_186  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        8.769088      12.756676      -3.983261       8.970274   \n",
      "std         4.474924       9.318280       4.725167       3.189759   \n",
      "min        -4.808200     -18.489700     -22.583300      -3.022300   \n",
      "25%         5.423100       5.663300      -7.360000       6.715200   \n",
      "50%         8.600000      12.521000      -3.946950       8.902150   \n",
      "75%        12.127425      19.456150      -0.590650      11.193800   \n",
      "max        23.704900      44.363400      12.997500      21.739200   \n",
      "\n",
      "             var_187        var_188        var_189        var_190  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean      -10.335043      15.377174       0.746072       3.234440   \n",
      "std        11.574708       3.944604       0.976348       4.559922   \n",
      "min       -47.753600       4.412300      -2.554300     -14.093300   \n",
      "25%       -19.205125      12.501550       0.014900      -0.058825   \n",
      "50%       -10.209750      15.239450       0.742600       3.203600   \n",
      "75%        -1.466000      18.345225       1.482900       6.406200   \n",
      "max        22.786100      29.330300       4.034100      18.440900   \n",
      "\n",
      "             var_191        var_192        var_193        var_194  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean        7.438408       1.927839       3.331774      17.993784   \n",
      "std         3.023272       1.478423       3.992030       3.135162   \n",
      "min        -2.691700      -3.814500     -11.783400       8.694400   \n",
      "25%         5.157400       0.889775       0.584600      15.629800   \n",
      "50%         7.347750       1.901300       3.396350      17.957950   \n",
      "75%         9.512525       2.949500       6.205800      20.396525   \n",
      "max        16.716500       8.402400      18.281800      27.928800   \n",
      "\n",
      "             var_195        var_196        var_197        var_198  \\\n",
      "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
      "mean       -0.142088       2.303335       8.908158      15.870720   \n",
      "std         1.429372       5.454369       0.921625       3.010945   \n",
      "min        -5.261000     -14.209600       5.960600       6.299300   \n",
      "25%        -1.170700      -1.946925       8.252800      13.829700   \n",
      "50%        -0.172700       2.408900       8.888200      15.934050   \n",
      "75%         0.829600       6.556725       9.593300      18.064725   \n",
      "max         4.272900      18.321500      12.000400      26.079100   \n",
      "\n",
      "             var_199  \n",
      "count  200000.000000  \n",
      "mean       -3.326537  \n",
      "std        10.438015  \n",
      "min       -38.852800  \n",
      "25%       -11.208475  \n",
      "50%        -2.819550  \n",
      "75%         4.836800  \n",
      "max        28.500700  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1000 with 210 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary statistics for numerical features\n",
    "numerical_features = data.select_dtypes(include=['float64', 'int64'])\n",
    "print(\"Summary Statistics for Numerical Features:\")\n",
    "print(numerical_features.describe())\n",
    "\n",
    "# Histograms for numerical features\n",
    "numerical_features.hist(figsize=(15, 10))\n",
    "plt.suptitle('Histograms of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "# Box plots for numerical features\n",
    "numerical_features.boxplot(figsize=(15, 10))\n",
    "plt.title('Box plots of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "# Bar plots for categorical features\n",
    "categorical_features = data.select_dtypes(include=['object'])\n",
    "for column in categorical_features.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    data[column].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Bar Plot of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d4ce75b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='var_199', ylabel='Count'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x = data.var_199)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0632a69",
   "metadata": {},
   "source": [
    "Skip The EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839dbed0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='var_0'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x = data.var_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1172a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c5fbb",
   "metadata": {},
   "source": [
    "We have outliers in the dataset, so we have to remove the ouliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5cc1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "data1 = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a0fc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data1.var_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "999f6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables if any\n",
    "# (not shown here assuming all variables are numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87af0d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    89.951\n",
      "1    10.049\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each class in the target variable\n",
    "class_counts = data['target'].value_counts()\n",
    "\n",
    "# Plot the class distribution\n",
    "plt.bar(class_counts.index, class_counts.values)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_counts.index)  # Ensure ticks are properly labeled\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage of instances in each class\n",
    "class_percentages = class_counts / len(data) * 100\n",
    "print(class_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c16e2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# Perform data augmentation by shuffling the dataset\n",
    "augmented_data = shuffle(data)\n",
    "\n",
    "# Optionally, you can concatenate the original and augmented data\n",
    "# to create a larger dataset\n",
    "combined_data = pd.concat([data, augmented_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71d1ecdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51590</th>\n",
       "      <td>0</td>\n",
       "      <td>10.3974</td>\n",
       "      <td>-5.9136</td>\n",
       "      <td>17.9117</td>\n",
       "      <td>3.8008</td>\n",
       "      <td>9.2937</td>\n",
       "      <td>7.5353</td>\n",
       "      <td>5.1777</td>\n",
       "      <td>17.5569</td>\n",
       "      <td>3.1515</td>\n",
       "      <td>5.4242</td>\n",
       "      <td>1.3896</td>\n",
       "      <td>-8.8148</td>\n",
       "      <td>14.0323</td>\n",
       "      <td>12.2651</td>\n",
       "      <td>8.2894</td>\n",
       "      <td>14.4554</td>\n",
       "      <td>8.7900</td>\n",
       "      <td>2.1192</td>\n",
       "      <td>9.3334</td>\n",
       "      <td>6.4745</td>\n",
       "      <td>13.8313</td>\n",
       "      <td>3.6496</td>\n",
       "      <td>3.2625</td>\n",
       "      <td>2.5559</td>\n",
       "      <td>7.4217</td>\n",
       "      <td>13.4497</td>\n",
       "      <td>-10.1505</td>\n",
       "      <td>-3.0011</td>\n",
       "      <td>4.2576</td>\n",
       "      <td>4.8010</td>\n",
       "      <td>-8.4010</td>\n",
       "      <td>10.6937</td>\n",
       "      <td>-2.8658</td>\n",
       "      <td>12.2040</td>\n",
       "      <td>11.2481</td>\n",
       "      <td>1.2608</td>\n",
       "      <td>2.6706</td>\n",
       "      <td>5.2132</td>\n",
       "      <td>6.5590</td>\n",
       "      <td>6.7980</td>\n",
       "      <td>-11.9985</td>\n",
       "      <td>18.5377</td>\n",
       "      <td>11.0236</td>\n",
       "      <td>11.1724</td>\n",
       "      <td>3.6655</td>\n",
       "      <td>-21.0209</td>\n",
       "      <td>15.5505</td>\n",
       "      <td>0.4496</td>\n",
       "      <td>18.3813</td>\n",
       "      <td>3.9219</td>\n",
       "      <td>13.0180</td>\n",
       "      <td>15.0474</td>\n",
       "      <td>-9.7847</td>\n",
       "      <td>6.2350</td>\n",
       "      <td>9.7629</td>\n",
       "      <td>8.7289</td>\n",
       "      <td>20.5470</td>\n",
       "      <td>5.8125</td>\n",
       "      <td>8.6421</td>\n",
       "      <td>7.0765</td>\n",
       "      <td>15.5821</td>\n",
       "      <td>-14.2911</td>\n",
       "      <td>2.1746</td>\n",
       "      <td>5.8409</td>\n",
       "      <td>5.4659</td>\n",
       "      <td>11.4184</td>\n",
       "      <td>3.9363</td>\n",
       "      <td>10.3117</td>\n",
       "      <td>5.0200</td>\n",
       "      <td>-6.4885</td>\n",
       "      <td>29.9411</td>\n",
       "      <td>1.0439</td>\n",
       "      <td>5.1866</td>\n",
       "      <td>23.1108</td>\n",
       "      <td>27.9835</td>\n",
       "      <td>18.3182</td>\n",
       "      <td>13.4834</td>\n",
       "      <td>20.9578</td>\n",
       "      <td>0.6799</td>\n",
       "      <td>13.3298</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>14.8175</td>\n",
       "      <td>-18.4265</td>\n",
       "      <td>1.1093</td>\n",
       "      <td>1.4114</td>\n",
       "      <td>15.1929</td>\n",
       "      <td>3.1095</td>\n",
       "      <td>1.1317</td>\n",
       "      <td>12.3433</td>\n",
       "      <td>2.4092</td>\n",
       "      <td>-47.7806</td>\n",
       "      <td>7.0021</td>\n",
       "      <td>12.0954</td>\n",
       "      <td>10.8004</td>\n",
       "      <td>13.6917</td>\n",
       "      <td>-1.2344</td>\n",
       "      <td>23.2131</td>\n",
       "      <td>6.5506</td>\n",
       "      <td>2.4426</td>\n",
       "      <td>-0.7730</td>\n",
       "      <td>-3.4700</td>\n",
       "      <td>7.4344</td>\n",
       "      <td>33.2912</td>\n",
       "      <td>1.6448</td>\n",
       "      <td>10.6332</td>\n",
       "      <td>3.8944</td>\n",
       "      <td>6.1798</td>\n",
       "      <td>26.9649</td>\n",
       "      <td>14.0846</td>\n",
       "      <td>11.3842</td>\n",
       "      <td>13.5426</td>\n",
       "      <td>5.9930</td>\n",
       "      <td>2.5665</td>\n",
       "      <td>1.3369</td>\n",
       "      <td>2.9007</td>\n",
       "      <td>3.2890</td>\n",
       "      <td>2.9834</td>\n",
       "      <td>6.8565</td>\n",
       "      <td>-15.4291</td>\n",
       "      <td>0.4685</td>\n",
       "      <td>8.0814</td>\n",
       "      <td>11.3221</td>\n",
       "      <td>3.0953</td>\n",
       "      <td>6.6758</td>\n",
       "      <td>9.0801</td>\n",
       "      <td>12.4159</td>\n",
       "      <td>12.2032</td>\n",
       "      <td>1.6486</td>\n",
       "      <td>-0.0429</td>\n",
       "      <td>15.4525</td>\n",
       "      <td>13.8214</td>\n",
       "      <td>0.8637</td>\n",
       "      <td>6.1031</td>\n",
       "      <td>6.4719</td>\n",
       "      <td>-7.0811</td>\n",
       "      <td>-7.3782</td>\n",
       "      <td>10.9045</td>\n",
       "      <td>17.5765</td>\n",
       "      <td>0.7724</td>\n",
       "      <td>9.7178</td>\n",
       "      <td>1.1914</td>\n",
       "      <td>-6.1055</td>\n",
       "      <td>15.9892</td>\n",
       "      <td>14.7951</td>\n",
       "      <td>8.6756</td>\n",
       "      <td>-3.5333</td>\n",
       "      <td>8.9492</td>\n",
       "      <td>-12.6419</td>\n",
       "      <td>3.8829</td>\n",
       "      <td>2.8378</td>\n",
       "      <td>15.1440</td>\n",
       "      <td>11.7746</td>\n",
       "      <td>8.9811</td>\n",
       "      <td>14.9659</td>\n",
       "      <td>-5.1464</td>\n",
       "      <td>0.5794</td>\n",
       "      <td>13.2541</td>\n",
       "      <td>-10.0531</td>\n",
       "      <td>16.2484</td>\n",
       "      <td>8.8786</td>\n",
       "      <td>29.6674</td>\n",
       "      <td>5.3186</td>\n",
       "      <td>7.2194</td>\n",
       "      <td>10.5803</td>\n",
       "      <td>-4.6134</td>\n",
       "      <td>27.9719</td>\n",
       "      <td>2.5454</td>\n",
       "      <td>-10.8715</td>\n",
       "      <td>5.5696</td>\n",
       "      <td>5.2786</td>\n",
       "      <td>4.1202</td>\n",
       "      <td>-12.4553</td>\n",
       "      <td>21.5342</td>\n",
       "      <td>1.8643</td>\n",
       "      <td>14.5976</td>\n",
       "      <td>10.1628</td>\n",
       "      <td>-2.0807</td>\n",
       "      <td>10.2684</td>\n",
       "      <td>-14.7208</td>\n",
       "      <td>2.6609</td>\n",
       "      <td>-12.6520</td>\n",
       "      <td>9.3089</td>\n",
       "      <td>3.3315</td>\n",
       "      <td>2.6496</td>\n",
       "      <td>16.0369</td>\n",
       "      <td>-4.5869</td>\n",
       "      <td>7.8993</td>\n",
       "      <td>-9.2009</td>\n",
       "      <td>19.8196</td>\n",
       "      <td>2.3064</td>\n",
       "      <td>-2.9676</td>\n",
       "      <td>4.9930</td>\n",
       "      <td>-0.8115</td>\n",
       "      <td>3.2842</td>\n",
       "      <td>23.4969</td>\n",
       "      <td>1.3758</td>\n",
       "      <td>0.6114</td>\n",
       "      <td>9.5197</td>\n",
       "      <td>18.5446</td>\n",
       "      <td>-12.9629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27699</th>\n",
       "      <td>0</td>\n",
       "      <td>10.8077</td>\n",
       "      <td>2.4330</td>\n",
       "      <td>9.6733</td>\n",
       "      <td>4.1676</td>\n",
       "      <td>9.6226</td>\n",
       "      <td>-1.2920</td>\n",
       "      <td>5.1978</td>\n",
       "      <td>16.2652</td>\n",
       "      <td>-1.8513</td>\n",
       "      <td>8.8580</td>\n",
       "      <td>5.1333</td>\n",
       "      <td>3.8998</td>\n",
       "      <td>14.0904</td>\n",
       "      <td>8.1751</td>\n",
       "      <td>4.3692</td>\n",
       "      <td>14.7130</td>\n",
       "      <td>11.7514</td>\n",
       "      <td>-4.5023</td>\n",
       "      <td>17.8784</td>\n",
       "      <td>5.1581</td>\n",
       "      <td>21.2157</td>\n",
       "      <td>15.0899</td>\n",
       "      <td>6.1326</td>\n",
       "      <td>2.0407</td>\n",
       "      <td>10.6846</td>\n",
       "      <td>13.9065</td>\n",
       "      <td>-6.7307</td>\n",
       "      <td>-1.2344</td>\n",
       "      <td>4.8528</td>\n",
       "      <td>5.5035</td>\n",
       "      <td>-24.4808</td>\n",
       "      <td>9.3302</td>\n",
       "      <td>-2.6454</td>\n",
       "      <td>17.6198</td>\n",
       "      <td>10.9933</td>\n",
       "      <td>5.2141</td>\n",
       "      <td>-3.3251</td>\n",
       "      <td>9.4923</td>\n",
       "      <td>8.8039</td>\n",
       "      <td>1.2540</td>\n",
       "      <td>-5.1073</td>\n",
       "      <td>12.9980</td>\n",
       "      <td>11.5622</td>\n",
       "      <td>11.7701</td>\n",
       "      <td>5.1420</td>\n",
       "      <td>-29.8639</td>\n",
       "      <td>12.0049</td>\n",
       "      <td>1.2001</td>\n",
       "      <td>20.2347</td>\n",
       "      <td>21.4472</td>\n",
       "      <td>13.0744</td>\n",
       "      <td>12.5868</td>\n",
       "      <td>-4.4248</td>\n",
       "      <td>5.1277</td>\n",
       "      <td>-8.5259</td>\n",
       "      <td>14.2198</td>\n",
       "      <td>21.5951</td>\n",
       "      <td>4.6118</td>\n",
       "      <td>1.4066</td>\n",
       "      <td>7.8775</td>\n",
       "      <td>14.0038</td>\n",
       "      <td>-15.8245</td>\n",
       "      <td>3.9117</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>5.4982</td>\n",
       "      <td>-0.3704</td>\n",
       "      <td>5.8907</td>\n",
       "      <td>6.6712</td>\n",
       "      <td>5.0176</td>\n",
       "      <td>-4.7609</td>\n",
       "      <td>34.1382</td>\n",
       "      <td>0.6152</td>\n",
       "      <td>4.8368</td>\n",
       "      <td>22.1731</td>\n",
       "      <td>0.8923</td>\n",
       "      <td>17.7268</td>\n",
       "      <td>8.1880</td>\n",
       "      <td>15.2038</td>\n",
       "      <td>4.2709</td>\n",
       "      <td>16.0487</td>\n",
       "      <td>12.3891</td>\n",
       "      <td>16.7520</td>\n",
       "      <td>-4.6216</td>\n",
       "      <td>2.5369</td>\n",
       "      <td>3.7545</td>\n",
       "      <td>23.4362</td>\n",
       "      <td>8.2782</td>\n",
       "      <td>8.1417</td>\n",
       "      <td>9.9846</td>\n",
       "      <td>0.8077</td>\n",
       "      <td>-8.6084</td>\n",
       "      <td>6.9590</td>\n",
       "      <td>13.2010</td>\n",
       "      <td>11.2578</td>\n",
       "      <td>14.3965</td>\n",
       "      <td>-0.6759</td>\n",
       "      <td>23.9976</td>\n",
       "      <td>15.3405</td>\n",
       "      <td>1.7837</td>\n",
       "      <td>-1.3212</td>\n",
       "      <td>-0.8734</td>\n",
       "      <td>18.7022</td>\n",
       "      <td>14.4519</td>\n",
       "      <td>1.7998</td>\n",
       "      <td>11.3719</td>\n",
       "      <td>5.8409</td>\n",
       "      <td>8.7993</td>\n",
       "      <td>5.7631</td>\n",
       "      <td>13.9356</td>\n",
       "      <td>21.0540</td>\n",
       "      <td>9.8195</td>\n",
       "      <td>6.9385</td>\n",
       "      <td>1.9997</td>\n",
       "      <td>13.4560</td>\n",
       "      <td>3.7082</td>\n",
       "      <td>-1.5049</td>\n",
       "      <td>0.4664</td>\n",
       "      <td>30.5513</td>\n",
       "      <td>-12.6981</td>\n",
       "      <td>1.9631</td>\n",
       "      <td>2.6705</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>-5.0672</td>\n",
       "      <td>2.0362</td>\n",
       "      <td>4.3461</td>\n",
       "      <td>12.8119</td>\n",
       "      <td>12.6473</td>\n",
       "      <td>5.4512</td>\n",
       "      <td>-1.7088</td>\n",
       "      <td>19.9176</td>\n",
       "      <td>11.8387</td>\n",
       "      <td>1.5735</td>\n",
       "      <td>9.4802</td>\n",
       "      <td>6.5240</td>\n",
       "      <td>-1.0628</td>\n",
       "      <td>1.4667</td>\n",
       "      <td>24.4042</td>\n",
       "      <td>6.7886</td>\n",
       "      <td>9.4675</td>\n",
       "      <td>4.8104</td>\n",
       "      <td>-3.8836</td>\n",
       "      <td>2.8022</td>\n",
       "      <td>14.0605</td>\n",
       "      <td>10.6581</td>\n",
       "      <td>8.9250</td>\n",
       "      <td>4.0323</td>\n",
       "      <td>7.0351</td>\n",
       "      <td>3.6557</td>\n",
       "      <td>4.2898</td>\n",
       "      <td>21.1085</td>\n",
       "      <td>19.0920</td>\n",
       "      <td>7.2070</td>\n",
       "      <td>7.5173</td>\n",
       "      <td>15.0867</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>-5.6385</td>\n",
       "      <td>13.3836</td>\n",
       "      <td>-0.9613</td>\n",
       "      <td>26.1073</td>\n",
       "      <td>14.1441</td>\n",
       "      <td>16.0268</td>\n",
       "      <td>5.5152</td>\n",
       "      <td>5.2663</td>\n",
       "      <td>8.4773</td>\n",
       "      <td>-3.6637</td>\n",
       "      <td>15.7517</td>\n",
       "      <td>3.2652</td>\n",
       "      <td>-8.4806</td>\n",
       "      <td>4.3225</td>\n",
       "      <td>5.5613</td>\n",
       "      <td>-3.5126</td>\n",
       "      <td>-5.4094</td>\n",
       "      <td>9.3867</td>\n",
       "      <td>8.4791</td>\n",
       "      <td>19.6515</td>\n",
       "      <td>12.9587</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>12.7234</td>\n",
       "      <td>3.4768</td>\n",
       "      <td>3.6308</td>\n",
       "      <td>0.8005</td>\n",
       "      <td>11.2543</td>\n",
       "      <td>14.0389</td>\n",
       "      <td>9.5484</td>\n",
       "      <td>3.1812</td>\n",
       "      <td>1.7877</td>\n",
       "      <td>13.0856</td>\n",
       "      <td>-22.5574</td>\n",
       "      <td>22.7242</td>\n",
       "      <td>-0.4419</td>\n",
       "      <td>5.7070</td>\n",
       "      <td>8.7324</td>\n",
       "      <td>2.2800</td>\n",
       "      <td>6.8707</td>\n",
       "      <td>19.6816</td>\n",
       "      <td>-0.3650</td>\n",
       "      <td>-0.4176</td>\n",
       "      <td>8.9947</td>\n",
       "      <td>16.9930</td>\n",
       "      <td>5.4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197832</th>\n",
       "      <td>0</td>\n",
       "      <td>8.4101</td>\n",
       "      <td>-6.1183</td>\n",
       "      <td>9.0142</td>\n",
       "      <td>4.5054</td>\n",
       "      <td>10.4665</td>\n",
       "      <td>4.7996</td>\n",
       "      <td>6.4269</td>\n",
       "      <td>18.4828</td>\n",
       "      <td>1.3070</td>\n",
       "      <td>8.3639</td>\n",
       "      <td>6.3789</td>\n",
       "      <td>-1.5832</td>\n",
       "      <td>13.8791</td>\n",
       "      <td>-0.2168</td>\n",
       "      <td>7.1908</td>\n",
       "      <td>14.7899</td>\n",
       "      <td>9.0505</td>\n",
       "      <td>3.1331</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>6.7446</td>\n",
       "      <td>5.7861</td>\n",
       "      <td>13.7175</td>\n",
       "      <td>2.3444</td>\n",
       "      <td>2.3788</td>\n",
       "      <td>6.2723</td>\n",
       "      <td>13.8015</td>\n",
       "      <td>1.6004</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>4.7560</td>\n",
       "      <td>2.6710</td>\n",
       "      <td>-2.7872</td>\n",
       "      <td>8.1213</td>\n",
       "      <td>3.9318</td>\n",
       "      <td>5.2983</td>\n",
       "      <td>12.0709</td>\n",
       "      <td>7.3224</td>\n",
       "      <td>9.6489</td>\n",
       "      <td>1.8804</td>\n",
       "      <td>12.6210</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>-0.3617</td>\n",
       "      <td>10.2813</td>\n",
       "      <td>10.4964</td>\n",
       "      <td>11.2196</td>\n",
       "      <td>7.9191</td>\n",
       "      <td>-2.1707</td>\n",
       "      <td>10.3119</td>\n",
       "      <td>-11.3343</td>\n",
       "      <td>0.5858</td>\n",
       "      <td>11.5365</td>\n",
       "      <td>12.7841</td>\n",
       "      <td>1.7925</td>\n",
       "      <td>-7.1104</td>\n",
       "      <td>5.4393</td>\n",
       "      <td>-6.1413</td>\n",
       "      <td>20.8171</td>\n",
       "      <td>15.2871</td>\n",
       "      <td>6.3777</td>\n",
       "      <td>-0.7626</td>\n",
       "      <td>10.3784</td>\n",
       "      <td>6.5166</td>\n",
       "      <td>-20.3622</td>\n",
       "      <td>-0.8036</td>\n",
       "      <td>3.0076</td>\n",
       "      <td>9.9220</td>\n",
       "      <td>-3.8246</td>\n",
       "      <td>5.9093</td>\n",
       "      <td>14.9093</td>\n",
       "      <td>5.0252</td>\n",
       "      <td>-3.0187</td>\n",
       "      <td>33.0230</td>\n",
       "      <td>0.6794</td>\n",
       "      <td>-5.1704</td>\n",
       "      <td>15.1073</td>\n",
       "      <td>10.6373</td>\n",
       "      <td>15.6992</td>\n",
       "      <td>3.8031</td>\n",
       "      <td>20.5601</td>\n",
       "      <td>6.2441</td>\n",
       "      <td>14.5273</td>\n",
       "      <td>2.6096</td>\n",
       "      <td>17.9988</td>\n",
       "      <td>-0.3455</td>\n",
       "      <td>-10.6491</td>\n",
       "      <td>0.3746</td>\n",
       "      <td>26.0284</td>\n",
       "      <td>-1.1477</td>\n",
       "      <td>9.3538</td>\n",
       "      <td>5.6864</td>\n",
       "      <td>7.5691</td>\n",
       "      <td>-23.0817</td>\n",
       "      <td>6.8478</td>\n",
       "      <td>11.8861</td>\n",
       "      <td>10.1202</td>\n",
       "      <td>12.5871</td>\n",
       "      <td>-0.4373</td>\n",
       "      <td>13.8322</td>\n",
       "      <td>0.6236</td>\n",
       "      <td>2.3191</td>\n",
       "      <td>-1.9994</td>\n",
       "      <td>-9.5843</td>\n",
       "      <td>16.2810</td>\n",
       "      <td>9.7850</td>\n",
       "      <td>1.7776</td>\n",
       "      <td>13.5545</td>\n",
       "      <td>5.4013</td>\n",
       "      <td>5.8620</td>\n",
       "      <td>31.5440</td>\n",
       "      <td>14.1263</td>\n",
       "      <td>20.8892</td>\n",
       "      <td>11.5224</td>\n",
       "      <td>5.4217</td>\n",
       "      <td>6.2807</td>\n",
       "      <td>15.0956</td>\n",
       "      <td>4.3287</td>\n",
       "      <td>-0.2405</td>\n",
       "      <td>-0.1866</td>\n",
       "      <td>-10.7498</td>\n",
       "      <td>-9.1729</td>\n",
       "      <td>-2.0486</td>\n",
       "      <td>26.2988</td>\n",
       "      <td>10.1711</td>\n",
       "      <td>1.4742</td>\n",
       "      <td>5.9040</td>\n",
       "      <td>1.8949</td>\n",
       "      <td>12.3401</td>\n",
       "      <td>13.5984</td>\n",
       "      <td>5.8305</td>\n",
       "      <td>0.5503</td>\n",
       "      <td>6.0700</td>\n",
       "      <td>12.0291</td>\n",
       "      <td>0.8113</td>\n",
       "      <td>9.2694</td>\n",
       "      <td>7.4065</td>\n",
       "      <td>-4.3575</td>\n",
       "      <td>-1.3944</td>\n",
       "      <td>37.9452</td>\n",
       "      <td>21.4039</td>\n",
       "      <td>-3.5463</td>\n",
       "      <td>16.0042</td>\n",
       "      <td>10.0060</td>\n",
       "      <td>10.0897</td>\n",
       "      <td>6.9294</td>\n",
       "      <td>11.3180</td>\n",
       "      <td>7.8723</td>\n",
       "      <td>8.7166</td>\n",
       "      <td>11.6819</td>\n",
       "      <td>7.9200</td>\n",
       "      <td>3.8769</td>\n",
       "      <td>14.7180</td>\n",
       "      <td>13.3159</td>\n",
       "      <td>12.9321</td>\n",
       "      <td>7.0719</td>\n",
       "      <td>19.9343</td>\n",
       "      <td>2.3284</td>\n",
       "      <td>-0.8067</td>\n",
       "      <td>13.6185</td>\n",
       "      <td>-2.6548</td>\n",
       "      <td>23.3571</td>\n",
       "      <td>16.7988</td>\n",
       "      <td>36.7339</td>\n",
       "      <td>5.7112</td>\n",
       "      <td>5.8650</td>\n",
       "      <td>13.4639</td>\n",
       "      <td>3.4726</td>\n",
       "      <td>19.7280</td>\n",
       "      <td>2.9546</td>\n",
       "      <td>-17.2973</td>\n",
       "      <td>5.3176</td>\n",
       "      <td>5.6618</td>\n",
       "      <td>8.9496</td>\n",
       "      <td>0.3537</td>\n",
       "      <td>29.1036</td>\n",
       "      <td>2.1857</td>\n",
       "      <td>30.0889</td>\n",
       "      <td>7.7824</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>8.8942</td>\n",
       "      <td>-13.6776</td>\n",
       "      <td>-3.6005</td>\n",
       "      <td>-5.5389</td>\n",
       "      <td>10.3645</td>\n",
       "      <td>3.7962</td>\n",
       "      <td>13.8262</td>\n",
       "      <td>-3.8803</td>\n",
       "      <td>-10.7126</td>\n",
       "      <td>9.9370</td>\n",
       "      <td>-15.4758</td>\n",
       "      <td>18.7759</td>\n",
       "      <td>-0.8889</td>\n",
       "      <td>1.8659</td>\n",
       "      <td>7.6620</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>2.9958</td>\n",
       "      <td>13.1156</td>\n",
       "      <td>-1.0957</td>\n",
       "      <td>-1.6878</td>\n",
       "      <td>9.5792</td>\n",
       "      <td>15.7510</td>\n",
       "      <td>-13.8635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165345</th>\n",
       "      <td>1</td>\n",
       "      <td>13.9492</td>\n",
       "      <td>-1.9923</td>\n",
       "      <td>10.1264</td>\n",
       "      <td>5.5747</td>\n",
       "      <td>12.1341</td>\n",
       "      <td>0.2001</td>\n",
       "      <td>4.9879</td>\n",
       "      <td>15.6888</td>\n",
       "      <td>-0.3861</td>\n",
       "      <td>7.2170</td>\n",
       "      <td>8.2519</td>\n",
       "      <td>1.1034</td>\n",
       "      <td>14.2549</td>\n",
       "      <td>5.5744</td>\n",
       "      <td>6.1776</td>\n",
       "      <td>15.0508</td>\n",
       "      <td>15.2053</td>\n",
       "      <td>-2.7194</td>\n",
       "      <td>34.9080</td>\n",
       "      <td>18.2220</td>\n",
       "      <td>11.5677</td>\n",
       "      <td>15.0358</td>\n",
       "      <td>9.4101</td>\n",
       "      <td>3.1746</td>\n",
       "      <td>14.2896</td>\n",
       "      <td>13.8907</td>\n",
       "      <td>1.2564</td>\n",
       "      <td>-0.2592</td>\n",
       "      <td>6.3966</td>\n",
       "      <td>5.4585</td>\n",
       "      <td>-3.6115</td>\n",
       "      <td>5.8313</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>13.3021</td>\n",
       "      <td>11.1677</td>\n",
       "      <td>1.5277</td>\n",
       "      <td>-0.4913</td>\n",
       "      <td>3.7415</td>\n",
       "      <td>4.9268</td>\n",
       "      <td>-3.8037</td>\n",
       "      <td>-6.8895</td>\n",
       "      <td>14.5932</td>\n",
       "      <td>11.7038</td>\n",
       "      <td>12.0420</td>\n",
       "      <td>-2.2053</td>\n",
       "      <td>-8.0283</td>\n",
       "      <td>13.5874</td>\n",
       "      <td>-5.6721</td>\n",
       "      <td>4.8935</td>\n",
       "      <td>18.3564</td>\n",
       "      <td>13.3420</td>\n",
       "      <td>2.4080</td>\n",
       "      <td>-13.0974</td>\n",
       "      <td>6.2421</td>\n",
       "      <td>-1.2747</td>\n",
       "      <td>8.5735</td>\n",
       "      <td>16.7074</td>\n",
       "      <td>7.7924</td>\n",
       "      <td>6.1628</td>\n",
       "      <td>9.1747</td>\n",
       "      <td>8.5397</td>\n",
       "      <td>-4.8144</td>\n",
       "      <td>1.2895</td>\n",
       "      <td>2.8638</td>\n",
       "      <td>7.9320</td>\n",
       "      <td>-5.4780</td>\n",
       "      <td>3.1890</td>\n",
       "      <td>-0.8125</td>\n",
       "      <td>5.0236</td>\n",
       "      <td>-3.4999</td>\n",
       "      <td>30.8681</td>\n",
       "      <td>0.8785</td>\n",
       "      <td>-1.7515</td>\n",
       "      <td>17.1378</td>\n",
       "      <td>49.0808</td>\n",
       "      <td>3.4056</td>\n",
       "      <td>-6.5321</td>\n",
       "      <td>18.9915</td>\n",
       "      <td>8.1503</td>\n",
       "      <td>15.1397</td>\n",
       "      <td>-7.2213</td>\n",
       "      <td>14.6778</td>\n",
       "      <td>-5.9811</td>\n",
       "      <td>-3.6889</td>\n",
       "      <td>-3.4124</td>\n",
       "      <td>21.2494</td>\n",
       "      <td>-8.4622</td>\n",
       "      <td>14.7261</td>\n",
       "      <td>10.9138</td>\n",
       "      <td>2.4000</td>\n",
       "      <td>-23.1283</td>\n",
       "      <td>6.8857</td>\n",
       "      <td>4.0456</td>\n",
       "      <td>10.4192</td>\n",
       "      <td>11.3547</td>\n",
       "      <td>0.4168</td>\n",
       "      <td>10.0579</td>\n",
       "      <td>20.4419</td>\n",
       "      <td>2.7041</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>-4.9686</td>\n",
       "      <td>18.5949</td>\n",
       "      <td>7.7888</td>\n",
       "      <td>1.1942</td>\n",
       "      <td>12.6430</td>\n",
       "      <td>5.3534</td>\n",
       "      <td>5.4759</td>\n",
       "      <td>9.0434</td>\n",
       "      <td>14.2632</td>\n",
       "      <td>14.1875</td>\n",
       "      <td>1.9956</td>\n",
       "      <td>8.3070</td>\n",
       "      <td>2.7901</td>\n",
       "      <td>14.7004</td>\n",
       "      <td>2.6474</td>\n",
       "      <td>-0.1101</td>\n",
       "      <td>5.3927</td>\n",
       "      <td>14.9807</td>\n",
       "      <td>-3.1040</td>\n",
       "      <td>8.0356</td>\n",
       "      <td>33.8359</td>\n",
       "      <td>12.1279</td>\n",
       "      <td>3.1443</td>\n",
       "      <td>9.8729</td>\n",
       "      <td>11.0123</td>\n",
       "      <td>12.6269</td>\n",
       "      <td>13.5159</td>\n",
       "      <td>5.1756</td>\n",
       "      <td>-2.6192</td>\n",
       "      <td>16.4620</td>\n",
       "      <td>11.0076</td>\n",
       "      <td>-0.1398</td>\n",
       "      <td>6.0427</td>\n",
       "      <td>7.3026</td>\n",
       "      <td>-3.9317</td>\n",
       "      <td>5.2346</td>\n",
       "      <td>15.9069</td>\n",
       "      <td>13.0027</td>\n",
       "      <td>-8.2954</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>10.8152</td>\n",
       "      <td>-2.5145</td>\n",
       "      <td>23.0875</td>\n",
       "      <td>8.8675</td>\n",
       "      <td>9.4659</td>\n",
       "      <td>8.6054</td>\n",
       "      <td>5.1493</td>\n",
       "      <td>-11.6052</td>\n",
       "      <td>4.2768</td>\n",
       "      <td>-15.9487</td>\n",
       "      <td>15.3167</td>\n",
       "      <td>13.1224</td>\n",
       "      <td>6.1052</td>\n",
       "      <td>15.9495</td>\n",
       "      <td>13.0445</td>\n",
       "      <td>2.9080</td>\n",
       "      <td>13.0394</td>\n",
       "      <td>-8.1259</td>\n",
       "      <td>21.7006</td>\n",
       "      <td>14.3895</td>\n",
       "      <td>24.0276</td>\n",
       "      <td>5.6229</td>\n",
       "      <td>7.2125</td>\n",
       "      <td>10.8350</td>\n",
       "      <td>-14.2928</td>\n",
       "      <td>17.6844</td>\n",
       "      <td>2.1648</td>\n",
       "      <td>-12.1571</td>\n",
       "      <td>11.4552</td>\n",
       "      <td>5.0698</td>\n",
       "      <td>4.4769</td>\n",
       "      <td>-7.4787</td>\n",
       "      <td>-4.3707</td>\n",
       "      <td>-5.9778</td>\n",
       "      <td>13.7006</td>\n",
       "      <td>9.6750</td>\n",
       "      <td>-3.3032</td>\n",
       "      <td>8.2330</td>\n",
       "      <td>4.4451</td>\n",
       "      <td>2.5123</td>\n",
       "      <td>-2.7197</td>\n",
       "      <td>10.6328</td>\n",
       "      <td>-13.3813</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>-1.5199</td>\n",
       "      <td>-3.3075</td>\n",
       "      <td>7.9794</td>\n",
       "      <td>-15.3099</td>\n",
       "      <td>18.4142</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>4.0636</td>\n",
       "      <td>4.7780</td>\n",
       "      <td>-0.8486</td>\n",
       "      <td>6.5988</td>\n",
       "      <td>22.5410</td>\n",
       "      <td>-0.9957</td>\n",
       "      <td>-7.3223</td>\n",
       "      <td>8.3877</td>\n",
       "      <td>17.9658</td>\n",
       "      <td>10.0786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193250</th>\n",
       "      <td>0</td>\n",
       "      <td>9.0401</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>8.6602</td>\n",
       "      <td>10.6992</td>\n",
       "      <td>10.3945</td>\n",
       "      <td>-6.3093</td>\n",
       "      <td>3.8334</td>\n",
       "      <td>20.7618</td>\n",
       "      <td>-4.1906</td>\n",
       "      <td>8.3753</td>\n",
       "      <td>8.8166</td>\n",
       "      <td>5.8952</td>\n",
       "      <td>13.6831</td>\n",
       "      <td>10.0875</td>\n",
       "      <td>5.9167</td>\n",
       "      <td>14.8246</td>\n",
       "      <td>6.7432</td>\n",
       "      <td>6.2375</td>\n",
       "      <td>12.3850</td>\n",
       "      <td>25.4931</td>\n",
       "      <td>3.6677</td>\n",
       "      <td>18.4586</td>\n",
       "      <td>-0.7245</td>\n",
       "      <td>3.5133</td>\n",
       "      <td>14.4356</td>\n",
       "      <td>13.4201</td>\n",
       "      <td>-9.4178</td>\n",
       "      <td>-0.3241</td>\n",
       "      <td>6.5649</td>\n",
       "      <td>8.5876</td>\n",
       "      <td>1.4522</td>\n",
       "      <td>11.6911</td>\n",
       "      <td>-3.9003</td>\n",
       "      <td>11.7968</td>\n",
       "      <td>11.1777</td>\n",
       "      <td>-8.4646</td>\n",
       "      <td>0.3705</td>\n",
       "      <td>4.2674</td>\n",
       "      <td>8.7305</td>\n",
       "      <td>2.7650</td>\n",
       "      <td>-19.4462</td>\n",
       "      <td>0.3172</td>\n",
       "      <td>10.4609</td>\n",
       "      <td>11.1559</td>\n",
       "      <td>-0.2677</td>\n",
       "      <td>-4.4435</td>\n",
       "      <td>7.1143</td>\n",
       "      <td>-8.8506</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>16.4792</td>\n",
       "      <td>11.7772</td>\n",
       "      <td>13.8041</td>\n",
       "      <td>-4.9998</td>\n",
       "      <td>5.4787</td>\n",
       "      <td>-6.6192</td>\n",
       "      <td>22.0729</td>\n",
       "      <td>18.1839</td>\n",
       "      <td>5.7545</td>\n",
       "      <td>-1.7124</td>\n",
       "      <td>10.2638</td>\n",
       "      <td>16.2137</td>\n",
       "      <td>-18.6633</td>\n",
       "      <td>3.6817</td>\n",
       "      <td>3.3001</td>\n",
       "      <td>6.3637</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>4.3216</td>\n",
       "      <td>3.3222</td>\n",
       "      <td>5.0265</td>\n",
       "      <td>4.9121</td>\n",
       "      <td>25.9164</td>\n",
       "      <td>0.5259</td>\n",
       "      <td>-0.3095</td>\n",
       "      <td>24.0295</td>\n",
       "      <td>42.7918</td>\n",
       "      <td>7.8532</td>\n",
       "      <td>3.8120</td>\n",
       "      <td>15.4269</td>\n",
       "      <td>6.2945</td>\n",
       "      <td>15.2070</td>\n",
       "      <td>12.4370</td>\n",
       "      <td>16.3856</td>\n",
       "      <td>-4.9115</td>\n",
       "      <td>7.0759</td>\n",
       "      <td>2.9776</td>\n",
       "      <td>13.8435</td>\n",
       "      <td>6.3386</td>\n",
       "      <td>10.2212</td>\n",
       "      <td>11.3405</td>\n",
       "      <td>3.9957</td>\n",
       "      <td>-8.4941</td>\n",
       "      <td>7.1574</td>\n",
       "      <td>17.1164</td>\n",
       "      <td>9.7088</td>\n",
       "      <td>12.6389</td>\n",
       "      <td>0.1626</td>\n",
       "      <td>10.1873</td>\n",
       "      <td>22.8263</td>\n",
       "      <td>1.0635</td>\n",
       "      <td>1.0045</td>\n",
       "      <td>-10.3631</td>\n",
       "      <td>16.6667</td>\n",
       "      <td>22.0601</td>\n",
       "      <td>1.6457</td>\n",
       "      <td>10.6608</td>\n",
       "      <td>3.4468</td>\n",
       "      <td>7.1010</td>\n",
       "      <td>12.5263</td>\n",
       "      <td>14.4222</td>\n",
       "      <td>14.8845</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>6.1599</td>\n",
       "      <td>1.1125</td>\n",
       "      <td>6.3257</td>\n",
       "      <td>2.0698</td>\n",
       "      <td>1.0220</td>\n",
       "      <td>2.0541</td>\n",
       "      <td>24.7152</td>\n",
       "      <td>-8.4836</td>\n",
       "      <td>-1.3805</td>\n",
       "      <td>34.7891</td>\n",
       "      <td>8.5580</td>\n",
       "      <td>-0.2724</td>\n",
       "      <td>3.2507</td>\n",
       "      <td>1.3492</td>\n",
       "      <td>12.1872</td>\n",
       "      <td>13.4285</td>\n",
       "      <td>3.9408</td>\n",
       "      <td>2.3803</td>\n",
       "      <td>20.2104</td>\n",
       "      <td>11.2887</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>8.9386</td>\n",
       "      <td>6.6793</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>-12.2984</td>\n",
       "      <td>8.3998</td>\n",
       "      <td>20.1393</td>\n",
       "      <td>7.5714</td>\n",
       "      <td>5.7452</td>\n",
       "      <td>0.1243</td>\n",
       "      <td>-2.0019</td>\n",
       "      <td>11.0512</td>\n",
       "      <td>10.9457</td>\n",
       "      <td>7.8604</td>\n",
       "      <td>2.9153</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-14.4886</td>\n",
       "      <td>4.1416</td>\n",
       "      <td>-4.0070</td>\n",
       "      <td>17.8525</td>\n",
       "      <td>13.7682</td>\n",
       "      <td>4.9644</td>\n",
       "      <td>16.3574</td>\n",
       "      <td>9.3638</td>\n",
       "      <td>-15.9545</td>\n",
       "      <td>14.1494</td>\n",
       "      <td>7.5398</td>\n",
       "      <td>12.7183</td>\n",
       "      <td>9.1931</td>\n",
       "      <td>30.0005</td>\n",
       "      <td>5.5811</td>\n",
       "      <td>5.7817</td>\n",
       "      <td>11.5418</td>\n",
       "      <td>-5.3090</td>\n",
       "      <td>8.4325</td>\n",
       "      <td>3.1602</td>\n",
       "      <td>-4.3750</td>\n",
       "      <td>6.4332</td>\n",
       "      <td>6.0029</td>\n",
       "      <td>-5.2903</td>\n",
       "      <td>2.6729</td>\n",
       "      <td>20.1646</td>\n",
       "      <td>9.0153</td>\n",
       "      <td>14.7169</td>\n",
       "      <td>13.8902</td>\n",
       "      <td>1.0081</td>\n",
       "      <td>11.3524</td>\n",
       "      <td>4.8887</td>\n",
       "      <td>-0.1036</td>\n",
       "      <td>8.1350</td>\n",
       "      <td>10.3610</td>\n",
       "      <td>5.4340</td>\n",
       "      <td>10.0519</td>\n",
       "      <td>4.1118</td>\n",
       "      <td>-3.7783</td>\n",
       "      <td>5.7147</td>\n",
       "      <td>-2.6823</td>\n",
       "      <td>21.2950</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>5.5215</td>\n",
       "      <td>9.1564</td>\n",
       "      <td>2.9410</td>\n",
       "      <td>2.5742</td>\n",
       "      <td>22.1022</td>\n",
       "      <td>-2.0655</td>\n",
       "      <td>7.5666</td>\n",
       "      <td>8.4366</td>\n",
       "      <td>17.7034</td>\n",
       "      <td>3.3621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54444</th>\n",
       "      <td>0</td>\n",
       "      <td>12.4904</td>\n",
       "      <td>4.0971</td>\n",
       "      <td>9.5729</td>\n",
       "      <td>7.1030</td>\n",
       "      <td>12.2411</td>\n",
       "      <td>-3.8714</td>\n",
       "      <td>5.3379</td>\n",
       "      <td>13.5599</td>\n",
       "      <td>-2.5057</td>\n",
       "      <td>6.5893</td>\n",
       "      <td>4.4358</td>\n",
       "      <td>-4.1894</td>\n",
       "      <td>13.9236</td>\n",
       "      <td>14.2479</td>\n",
       "      <td>9.2883</td>\n",
       "      <td>14.9302</td>\n",
       "      <td>9.6972</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>7.6976</td>\n",
       "      <td>17.0411</td>\n",
       "      <td>9.4789</td>\n",
       "      <td>9.0912</td>\n",
       "      <td>6.6730</td>\n",
       "      <td>2.7345</td>\n",
       "      <td>7.9862</td>\n",
       "      <td>13.7540</td>\n",
       "      <td>-13.0887</td>\n",
       "      <td>-3.1314</td>\n",
       "      <td>5.4377</td>\n",
       "      <td>6.7783</td>\n",
       "      <td>-10.7636</td>\n",
       "      <td>12.6498</td>\n",
       "      <td>0.8116</td>\n",
       "      <td>22.1980</td>\n",
       "      <td>11.4345</td>\n",
       "      <td>8.5225</td>\n",
       "      <td>0.0637</td>\n",
       "      <td>8.3442</td>\n",
       "      <td>9.6516</td>\n",
       "      <td>5.1408</td>\n",
       "      <td>-16.9031</td>\n",
       "      <td>14.7474</td>\n",
       "      <td>10.8899</td>\n",
       "      <td>10.8464</td>\n",
       "      <td>5.7639</td>\n",
       "      <td>21.4889</td>\n",
       "      <td>8.0400</td>\n",
       "      <td>-11.2252</td>\n",
       "      <td>15.7474</td>\n",
       "      <td>31.3351</td>\n",
       "      <td>13.8472</td>\n",
       "      <td>0.4772</td>\n",
       "      <td>-5.5649</td>\n",
       "      <td>6.5326</td>\n",
       "      <td>-8.2557</td>\n",
       "      <td>14.5900</td>\n",
       "      <td>17.2057</td>\n",
       "      <td>7.0540</td>\n",
       "      <td>1.0443</td>\n",
       "      <td>10.1205</td>\n",
       "      <td>10.3625</td>\n",
       "      <td>0.6524</td>\n",
       "      <td>2.4346</td>\n",
       "      <td>-1.7746</td>\n",
       "      <td>6.0212</td>\n",
       "      <td>2.7454</td>\n",
       "      <td>4.7968</td>\n",
       "      <td>4.3666</td>\n",
       "      <td>5.0092</td>\n",
       "      <td>-9.8259</td>\n",
       "      <td>16.0449</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>-1.7623</td>\n",
       "      <td>15.2569</td>\n",
       "      <td>23.0787</td>\n",
       "      <td>20.9913</td>\n",
       "      <td>-0.0052</td>\n",
       "      <td>15.3383</td>\n",
       "      <td>3.0695</td>\n",
       "      <td>13.5407</td>\n",
       "      <td>10.8407</td>\n",
       "      <td>19.1084</td>\n",
       "      <td>-3.0142</td>\n",
       "      <td>-2.1188</td>\n",
       "      <td>1.4255</td>\n",
       "      <td>16.1721</td>\n",
       "      <td>15.3741</td>\n",
       "      <td>13.3447</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>0.4362</td>\n",
       "      <td>-8.9597</td>\n",
       "      <td>7.1372</td>\n",
       "      <td>11.9309</td>\n",
       "      <td>10.6130</td>\n",
       "      <td>16.3536</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>21.5935</td>\n",
       "      <td>42.3055</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>-0.1292</td>\n",
       "      <td>-9.2950</td>\n",
       "      <td>14.3201</td>\n",
       "      <td>36.7479</td>\n",
       "      <td>1.6059</td>\n",
       "      <td>9.6548</td>\n",
       "      <td>5.1387</td>\n",
       "      <td>8.1292</td>\n",
       "      <td>16.3701</td>\n",
       "      <td>14.2756</td>\n",
       "      <td>19.7277</td>\n",
       "      <td>1.7365</td>\n",
       "      <td>7.2307</td>\n",
       "      <td>3.2307</td>\n",
       "      <td>12.3384</td>\n",
       "      <td>4.4210</td>\n",
       "      <td>4.2612</td>\n",
       "      <td>3.5299</td>\n",
       "      <td>22.4758</td>\n",
       "      <td>-14.5210</td>\n",
       "      <td>-0.6718</td>\n",
       "      <td>28.8213</td>\n",
       "      <td>11.1212</td>\n",
       "      <td>-0.3664</td>\n",
       "      <td>12.6570</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>12.7450</td>\n",
       "      <td>12.6463</td>\n",
       "      <td>-0.3941</td>\n",
       "      <td>-3.5594</td>\n",
       "      <td>18.6261</td>\n",
       "      <td>11.1020</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>9.8752</td>\n",
       "      <td>6.6927</td>\n",
       "      <td>-11.8125</td>\n",
       "      <td>-8.3717</td>\n",
       "      <td>26.3973</td>\n",
       "      <td>11.7138</td>\n",
       "      <td>-0.5914</td>\n",
       "      <td>9.3315</td>\n",
       "      <td>-2.3878</td>\n",
       "      <td>1.8984</td>\n",
       "      <td>5.8921</td>\n",
       "      <td>8.3873</td>\n",
       "      <td>7.6926</td>\n",
       "      <td>1.1354</td>\n",
       "      <td>8.9153</td>\n",
       "      <td>-7.3375</td>\n",
       "      <td>3.8037</td>\n",
       "      <td>-7.5865</td>\n",
       "      <td>19.0708</td>\n",
       "      <td>13.6887</td>\n",
       "      <td>7.0208</td>\n",
       "      <td>16.2498</td>\n",
       "      <td>10.5968</td>\n",
       "      <td>12.5921</td>\n",
       "      <td>13.3929</td>\n",
       "      <td>-13.8908</td>\n",
       "      <td>11.8437</td>\n",
       "      <td>11.2875</td>\n",
       "      <td>20.0500</td>\n",
       "      <td>5.8561</td>\n",
       "      <td>4.1490</td>\n",
       "      <td>11.6025</td>\n",
       "      <td>1.9091</td>\n",
       "      <td>18.4879</td>\n",
       "      <td>2.9838</td>\n",
       "      <td>-5.4682</td>\n",
       "      <td>4.4443</td>\n",
       "      <td>5.8550</td>\n",
       "      <td>4.3673</td>\n",
       "      <td>-3.1908</td>\n",
       "      <td>12.1283</td>\n",
       "      <td>2.3392</td>\n",
       "      <td>22.8965</td>\n",
       "      <td>13.7958</td>\n",
       "      <td>2.9110</td>\n",
       "      <td>11.7910</td>\n",
       "      <td>-8.3711</td>\n",
       "      <td>-0.9903</td>\n",
       "      <td>-8.7420</td>\n",
       "      <td>8.3954</td>\n",
       "      <td>6.7660</td>\n",
       "      <td>16.2471</td>\n",
       "      <td>14.3125</td>\n",
       "      <td>-4.0452</td>\n",
       "      <td>8.6593</td>\n",
       "      <td>-20.7593</td>\n",
       "      <td>15.1800</td>\n",
       "      <td>2.4887</td>\n",
       "      <td>11.4350</td>\n",
       "      <td>7.5056</td>\n",
       "      <td>0.6534</td>\n",
       "      <td>5.0310</td>\n",
       "      <td>19.6522</td>\n",
       "      <td>0.2626</td>\n",
       "      <td>-6.8085</td>\n",
       "      <td>8.2127</td>\n",
       "      <td>21.1710</td>\n",
       "      <td>-20.4727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77521</th>\n",
       "      <td>0</td>\n",
       "      <td>12.0756</td>\n",
       "      <td>0.6536</td>\n",
       "      <td>14.9369</td>\n",
       "      <td>5.3230</td>\n",
       "      <td>7.8097</td>\n",
       "      <td>2.3978</td>\n",
       "      <td>5.5501</td>\n",
       "      <td>23.0557</td>\n",
       "      <td>-4.4645</td>\n",
       "      <td>7.9690</td>\n",
       "      <td>3.1193</td>\n",
       "      <td>-1.5357</td>\n",
       "      <td>13.9197</td>\n",
       "      <td>9.7544</td>\n",
       "      <td>10.2130</td>\n",
       "      <td>14.6041</td>\n",
       "      <td>12.6836</td>\n",
       "      <td>-5.5208</td>\n",
       "      <td>4.5473</td>\n",
       "      <td>19.0755</td>\n",
       "      <td>24.6041</td>\n",
       "      <td>14.9972</td>\n",
       "      <td>5.8234</td>\n",
       "      <td>3.5070</td>\n",
       "      <td>9.9510</td>\n",
       "      <td>13.8415</td>\n",
       "      <td>-8.9524</td>\n",
       "      <td>-1.3360</td>\n",
       "      <td>4.4207</td>\n",
       "      <td>4.4859</td>\n",
       "      <td>-16.4447</td>\n",
       "      <td>9.8363</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>18.9750</td>\n",
       "      <td>12.4266</td>\n",
       "      <td>-0.7335</td>\n",
       "      <td>2.0133</td>\n",
       "      <td>6.0986</td>\n",
       "      <td>9.3941</td>\n",
       "      <td>-6.2155</td>\n",
       "      <td>-9.6860</td>\n",
       "      <td>16.2583</td>\n",
       "      <td>11.0130</td>\n",
       "      <td>11.5700</td>\n",
       "      <td>10.4327</td>\n",
       "      <td>-48.8558</td>\n",
       "      <td>13.8643</td>\n",
       "      <td>-11.1942</td>\n",
       "      <td>15.8437</td>\n",
       "      <td>15.1464</td>\n",
       "      <td>13.3754</td>\n",
       "      <td>1.5810</td>\n",
       "      <td>0.9192</td>\n",
       "      <td>6.5946</td>\n",
       "      <td>3.6180</td>\n",
       "      <td>17.3741</td>\n",
       "      <td>18.7799</td>\n",
       "      <td>6.2369</td>\n",
       "      <td>3.8269</td>\n",
       "      <td>8.4710</td>\n",
       "      <td>7.8216</td>\n",
       "      <td>-17.6335</td>\n",
       "      <td>-0.2381</td>\n",
       "      <td>-1.1925</td>\n",
       "      <td>5.4755</td>\n",
       "      <td>0.6752</td>\n",
       "      <td>5.2577</td>\n",
       "      <td>13.4306</td>\n",
       "      <td>5.0252</td>\n",
       "      <td>-0.5491</td>\n",
       "      <td>33.0498</td>\n",
       "      <td>0.6976</td>\n",
       "      <td>2.8715</td>\n",
       "      <td>6.3629</td>\n",
       "      <td>15.0014</td>\n",
       "      <td>5.6505</td>\n",
       "      <td>5.9231</td>\n",
       "      <td>14.8444</td>\n",
       "      <td>5.8406</td>\n",
       "      <td>13.9866</td>\n",
       "      <td>-5.2695</td>\n",
       "      <td>16.9736</td>\n",
       "      <td>-5.0933</td>\n",
       "      <td>-13.3969</td>\n",
       "      <td>0.2658</td>\n",
       "      <td>19.7691</td>\n",
       "      <td>-13.7875</td>\n",
       "      <td>14.6716</td>\n",
       "      <td>10.0514</td>\n",
       "      <td>0.6721</td>\n",
       "      <td>-18.5091</td>\n",
       "      <td>7.1056</td>\n",
       "      <td>18.0088</td>\n",
       "      <td>10.8887</td>\n",
       "      <td>10.9388</td>\n",
       "      <td>0.1835</td>\n",
       "      <td>7.0454</td>\n",
       "      <td>25.8421</td>\n",
       "      <td>2.7307</td>\n",
       "      <td>-1.4396</td>\n",
       "      <td>-5.4111</td>\n",
       "      <td>22.3420</td>\n",
       "      <td>4.9562</td>\n",
       "      <td>1.7907</td>\n",
       "      <td>13.3697</td>\n",
       "      <td>2.6089</td>\n",
       "      <td>9.4761</td>\n",
       "      <td>15.0610</td>\n",
       "      <td>14.4500</td>\n",
       "      <td>23.0233</td>\n",
       "      <td>7.2020</td>\n",
       "      <td>5.7471</td>\n",
       "      <td>3.5116</td>\n",
       "      <td>10.2647</td>\n",
       "      <td>3.2930</td>\n",
       "      <td>3.0700</td>\n",
       "      <td>3.1054</td>\n",
       "      <td>23.2861</td>\n",
       "      <td>-11.1304</td>\n",
       "      <td>3.0325</td>\n",
       "      <td>32.9108</td>\n",
       "      <td>12.9628</td>\n",
       "      <td>-5.5922</td>\n",
       "      <td>3.6516</td>\n",
       "      <td>4.8402</td>\n",
       "      <td>12.8812</td>\n",
       "      <td>12.8668</td>\n",
       "      <td>3.4692</td>\n",
       "      <td>-5.4893</td>\n",
       "      <td>15.9122</td>\n",
       "      <td>12.4401</td>\n",
       "      <td>1.3717</td>\n",
       "      <td>6.5303</td>\n",
       "      <td>6.5312</td>\n",
       "      <td>2.1923</td>\n",
       "      <td>2.9965</td>\n",
       "      <td>13.5914</td>\n",
       "      <td>30.7308</td>\n",
       "      <td>-0.3007</td>\n",
       "      <td>11.1424</td>\n",
       "      <td>8.6554</td>\n",
       "      <td>-1.3220</td>\n",
       "      <td>8.2257</td>\n",
       "      <td>12.8353</td>\n",
       "      <td>8.9400</td>\n",
       "      <td>13.3364</td>\n",
       "      <td>11.7449</td>\n",
       "      <td>-0.4871</td>\n",
       "      <td>3.7961</td>\n",
       "      <td>-6.4872</td>\n",
       "      <td>15.4872</td>\n",
       "      <td>17.3317</td>\n",
       "      <td>9.6841</td>\n",
       "      <td>19.7184</td>\n",
       "      <td>-0.3729</td>\n",
       "      <td>2.9087</td>\n",
       "      <td>12.5196</td>\n",
       "      <td>-4.0016</td>\n",
       "      <td>20.3712</td>\n",
       "      <td>9.0545</td>\n",
       "      <td>37.2184</td>\n",
       "      <td>5.7610</td>\n",
       "      <td>3.3126</td>\n",
       "      <td>4.1035</td>\n",
       "      <td>2.9809</td>\n",
       "      <td>25.4507</td>\n",
       "      <td>2.9142</td>\n",
       "      <td>7.0334</td>\n",
       "      <td>7.1331</td>\n",
       "      <td>6.0174</td>\n",
       "      <td>6.0527</td>\n",
       "      <td>3.2659</td>\n",
       "      <td>18.1972</td>\n",
       "      <td>3.4801</td>\n",
       "      <td>26.0080</td>\n",
       "      <td>9.2702</td>\n",
       "      <td>0.1574</td>\n",
       "      <td>13.3378</td>\n",
       "      <td>7.9268</td>\n",
       "      <td>4.2928</td>\n",
       "      <td>2.1362</td>\n",
       "      <td>8.9557</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>3.5912</td>\n",
       "      <td>14.5755</td>\n",
       "      <td>-3.4588</td>\n",
       "      <td>10.1281</td>\n",
       "      <td>-29.7886</td>\n",
       "      <td>20.5672</td>\n",
       "      <td>-0.8228</td>\n",
       "      <td>0.3689</td>\n",
       "      <td>8.3071</td>\n",
       "      <td>1.4771</td>\n",
       "      <td>5.1579</td>\n",
       "      <td>18.0875</td>\n",
       "      <td>-1.6203</td>\n",
       "      <td>2.5211</td>\n",
       "      <td>9.1977</td>\n",
       "      <td>18.1966</td>\n",
       "      <td>-18.0426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49439</th>\n",
       "      <td>0</td>\n",
       "      <td>6.4827</td>\n",
       "      <td>-0.1254</td>\n",
       "      <td>9.8742</td>\n",
       "      <td>6.5121</td>\n",
       "      <td>12.8357</td>\n",
       "      <td>4.7679</td>\n",
       "      <td>6.9999</td>\n",
       "      <td>14.6801</td>\n",
       "      <td>1.2815</td>\n",
       "      <td>7.2162</td>\n",
       "      <td>2.0656</td>\n",
       "      <td>-5.0301</td>\n",
       "      <td>14.0485</td>\n",
       "      <td>6.7701</td>\n",
       "      <td>10.6114</td>\n",
       "      <td>14.7954</td>\n",
       "      <td>8.8016</td>\n",
       "      <td>-1.0517</td>\n",
       "      <td>18.9271</td>\n",
       "      <td>20.3470</td>\n",
       "      <td>22.9514</td>\n",
       "      <td>26.8826</td>\n",
       "      <td>5.7218</td>\n",
       "      <td>2.1176</td>\n",
       "      <td>13.1784</td>\n",
       "      <td>13.9020</td>\n",
       "      <td>-1.8656</td>\n",
       "      <td>-2.7829</td>\n",
       "      <td>5.9136</td>\n",
       "      <td>3.7484</td>\n",
       "      <td>-15.9028</td>\n",
       "      <td>8.0342</td>\n",
       "      <td>-5.0323</td>\n",
       "      <td>11.7231</td>\n",
       "      <td>11.2059</td>\n",
       "      <td>9.3478</td>\n",
       "      <td>3.4257</td>\n",
       "      <td>5.7810</td>\n",
       "      <td>9.0962</td>\n",
       "      <td>6.6659</td>\n",
       "      <td>-4.5044</td>\n",
       "      <td>-1.5156</td>\n",
       "      <td>10.4338</td>\n",
       "      <td>11.2274</td>\n",
       "      <td>11.0851</td>\n",
       "      <td>25.6511</td>\n",
       "      <td>7.3391</td>\n",
       "      <td>-19.6370</td>\n",
       "      <td>20.4729</td>\n",
       "      <td>25.0191</td>\n",
       "      <td>13.0447</td>\n",
       "      <td>13.0015</td>\n",
       "      <td>2.1116</td>\n",
       "      <td>6.8156</td>\n",
       "      <td>16.5393</td>\n",
       "      <td>6.7834</td>\n",
       "      <td>16.5501</td>\n",
       "      <td>7.0152</td>\n",
       "      <td>6.6720</td>\n",
       "      <td>8.6135</td>\n",
       "      <td>15.5433</td>\n",
       "      <td>-19.6367</td>\n",
       "      <td>2.1462</td>\n",
       "      <td>1.8310</td>\n",
       "      <td>4.1149</td>\n",
       "      <td>-7.3638</td>\n",
       "      <td>5.8404</td>\n",
       "      <td>5.2266</td>\n",
       "      <td>5.0183</td>\n",
       "      <td>-0.8607</td>\n",
       "      <td>19.4773</td>\n",
       "      <td>0.9079</td>\n",
       "      <td>-3.6426</td>\n",
       "      <td>21.6163</td>\n",
       "      <td>23.0934</td>\n",
       "      <td>23.3446</td>\n",
       "      <td>6.5704</td>\n",
       "      <td>21.7256</td>\n",
       "      <td>7.5744</td>\n",
       "      <td>12.4480</td>\n",
       "      <td>1.0592</td>\n",
       "      <td>16.5670</td>\n",
       "      <td>14.2049</td>\n",
       "      <td>3.0032</td>\n",
       "      <td>-7.9250</td>\n",
       "      <td>21.4960</td>\n",
       "      <td>11.9197</td>\n",
       "      <td>12.5635</td>\n",
       "      <td>7.8239</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>-16.4258</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>12.1038</td>\n",
       "      <td>11.4935</td>\n",
       "      <td>8.8204</td>\n",
       "      <td>-0.5129</td>\n",
       "      <td>22.1831</td>\n",
       "      <td>15.6572</td>\n",
       "      <td>1.4812</td>\n",
       "      <td>1.4177</td>\n",
       "      <td>-3.7730</td>\n",
       "      <td>14.4499</td>\n",
       "      <td>11.2579</td>\n",
       "      <td>1.2567</td>\n",
       "      <td>10.1182</td>\n",
       "      <td>3.8538</td>\n",
       "      <td>8.7105</td>\n",
       "      <td>18.4011</td>\n",
       "      <td>13.9360</td>\n",
       "      <td>13.9145</td>\n",
       "      <td>0.3429</td>\n",
       "      <td>6.3459</td>\n",
       "      <td>2.4649</td>\n",
       "      <td>11.1353</td>\n",
       "      <td>4.2721</td>\n",
       "      <td>-2.0414</td>\n",
       "      <td>2.4057</td>\n",
       "      <td>17.0167</td>\n",
       "      <td>-12.0394</td>\n",
       "      <td>7.0021</td>\n",
       "      <td>40.1013</td>\n",
       "      <td>14.7136</td>\n",
       "      <td>2.9664</td>\n",
       "      <td>13.1119</td>\n",
       "      <td>5.0863</td>\n",
       "      <td>12.9621</td>\n",
       "      <td>12.4400</td>\n",
       "      <td>-0.3585</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>21.0114</td>\n",
       "      <td>12.2539</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>10.8232</td>\n",
       "      <td>6.7557</td>\n",
       "      <td>-3.4472</td>\n",
       "      <td>4.9295</td>\n",
       "      <td>14.9176</td>\n",
       "      <td>11.6466</td>\n",
       "      <td>-6.1010</td>\n",
       "      <td>12.5493</td>\n",
       "      <td>10.9734</td>\n",
       "      <td>3.6920</td>\n",
       "      <td>8.4625</td>\n",
       "      <td>11.3665</td>\n",
       "      <td>8.2130</td>\n",
       "      <td>4.8325</td>\n",
       "      <td>10.0756</td>\n",
       "      <td>-19.9297</td>\n",
       "      <td>3.9025</td>\n",
       "      <td>8.0388</td>\n",
       "      <td>15.9884</td>\n",
       "      <td>11.2205</td>\n",
       "      <td>6.7047</td>\n",
       "      <td>17.3389</td>\n",
       "      <td>10.5394</td>\n",
       "      <td>-1.8955</td>\n",
       "      <td>12.0981</td>\n",
       "      <td>3.8825</td>\n",
       "      <td>34.6457</td>\n",
       "      <td>9.1157</td>\n",
       "      <td>20.3925</td>\n",
       "      <td>5.6120</td>\n",
       "      <td>5.3182</td>\n",
       "      <td>19.0850</td>\n",
       "      <td>-2.7405</td>\n",
       "      <td>19.5322</td>\n",
       "      <td>2.5623</td>\n",
       "      <td>-14.6855</td>\n",
       "      <td>5.5657</td>\n",
       "      <td>5.2742</td>\n",
       "      <td>-7.3142</td>\n",
       "      <td>7.6154</td>\n",
       "      <td>12.7750</td>\n",
       "      <td>0.3091</td>\n",
       "      <td>3.9351</td>\n",
       "      <td>14.6798</td>\n",
       "      <td>-8.9574</td>\n",
       "      <td>14.1055</td>\n",
       "      <td>7.5467</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>3.5579</td>\n",
       "      <td>10.6485</td>\n",
       "      <td>-12.1161</td>\n",
       "      <td>16.5248</td>\n",
       "      <td>26.2340</td>\n",
       "      <td>-11.3249</td>\n",
       "      <td>13.2318</td>\n",
       "      <td>5.8970</td>\n",
       "      <td>14.0280</td>\n",
       "      <td>-1.0962</td>\n",
       "      <td>1.2615</td>\n",
       "      <td>8.5350</td>\n",
       "      <td>2.2625</td>\n",
       "      <td>6.9436</td>\n",
       "      <td>22.4766</td>\n",
       "      <td>2.1484</td>\n",
       "      <td>9.3593</td>\n",
       "      <td>6.8472</td>\n",
       "      <td>13.0793</td>\n",
       "      <td>4.4414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134720</th>\n",
       "      <td>0</td>\n",
       "      <td>6.8704</td>\n",
       "      <td>-0.7463</td>\n",
       "      <td>12.1506</td>\n",
       "      <td>9.4347</td>\n",
       "      <td>10.8363</td>\n",
       "      <td>-12.1858</td>\n",
       "      <td>4.7211</td>\n",
       "      <td>24.1893</td>\n",
       "      <td>1.2180</td>\n",
       "      <td>5.3363</td>\n",
       "      <td>-3.7784</td>\n",
       "      <td>-10.7418</td>\n",
       "      <td>14.1667</td>\n",
       "      <td>8.3164</td>\n",
       "      <td>6.9363</td>\n",
       "      <td>14.2238</td>\n",
       "      <td>10.2787</td>\n",
       "      <td>-7.9082</td>\n",
       "      <td>29.4910</td>\n",
       "      <td>16.4082</td>\n",
       "      <td>17.6172</td>\n",
       "      <td>21.3965</td>\n",
       "      <td>3.9215</td>\n",
       "      <td>2.6106</td>\n",
       "      <td>9.1471</td>\n",
       "      <td>13.4345</td>\n",
       "      <td>-2.5250</td>\n",
       "      <td>-0.5275</td>\n",
       "      <td>4.0404</td>\n",
       "      <td>10.3723</td>\n",
       "      <td>-19.9593</td>\n",
       "      <td>7.6379</td>\n",
       "      <td>-4.6488</td>\n",
       "      <td>16.2245</td>\n",
       "      <td>10.6183</td>\n",
       "      <td>-2.0612</td>\n",
       "      <td>6.6444</td>\n",
       "      <td>4.4462</td>\n",
       "      <td>13.2900</td>\n",
       "      <td>-3.9788</td>\n",
       "      <td>-18.5511</td>\n",
       "      <td>-2.3109</td>\n",
       "      <td>10.5114</td>\n",
       "      <td>12.0860</td>\n",
       "      <td>14.5412</td>\n",
       "      <td>7.6649</td>\n",
       "      <td>10.6118</td>\n",
       "      <td>1.0478</td>\n",
       "      <td>2.4986</td>\n",
       "      <td>24.9719</td>\n",
       "      <td>12.0302</td>\n",
       "      <td>4.0296</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>5.9675</td>\n",
       "      <td>-15.7085</td>\n",
       "      <td>3.6506</td>\n",
       "      <td>16.2006</td>\n",
       "      <td>6.2206</td>\n",
       "      <td>1.9487</td>\n",
       "      <td>9.3900</td>\n",
       "      <td>17.3474</td>\n",
       "      <td>-1.3388</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>-1.9230</td>\n",
       "      <td>8.6771</td>\n",
       "      <td>-4.9034</td>\n",
       "      <td>7.0388</td>\n",
       "      <td>10.5973</td>\n",
       "      <td>5.0177</td>\n",
       "      <td>-3.5720</td>\n",
       "      <td>23.3534</td>\n",
       "      <td>0.2777</td>\n",
       "      <td>-2.6334</td>\n",
       "      <td>27.3166</td>\n",
       "      <td>30.3428</td>\n",
       "      <td>10.7064</td>\n",
       "      <td>5.8044</td>\n",
       "      <td>21.0938</td>\n",
       "      <td>6.0646</td>\n",
       "      <td>13.8305</td>\n",
       "      <td>8.8740</td>\n",
       "      <td>16.8102</td>\n",
       "      <td>-5.9580</td>\n",
       "      <td>7.7356</td>\n",
       "      <td>3.6891</td>\n",
       "      <td>17.2579</td>\n",
       "      <td>8.2796</td>\n",
       "      <td>10.9035</td>\n",
       "      <td>7.7676</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>-9.9755</td>\n",
       "      <td>7.2324</td>\n",
       "      <td>6.0425</td>\n",
       "      <td>9.4391</td>\n",
       "      <td>10.2446</td>\n",
       "      <td>-0.2816</td>\n",
       "      <td>33.5611</td>\n",
       "      <td>29.8170</td>\n",
       "      <td>0.1766</td>\n",
       "      <td>-2.8194</td>\n",
       "      <td>-3.5142</td>\n",
       "      <td>14.3217</td>\n",
       "      <td>37.2495</td>\n",
       "      <td>1.7883</td>\n",
       "      <td>11.5307</td>\n",
       "      <td>3.8483</td>\n",
       "      <td>6.3715</td>\n",
       "      <td>11.8986</td>\n",
       "      <td>14.1428</td>\n",
       "      <td>11.2411</td>\n",
       "      <td>6.6548</td>\n",
       "      <td>4.4822</td>\n",
       "      <td>3.2330</td>\n",
       "      <td>17.0146</td>\n",
       "      <td>3.9640</td>\n",
       "      <td>3.2305</td>\n",
       "      <td>-1.4797</td>\n",
       "      <td>4.2941</td>\n",
       "      <td>-5.5423</td>\n",
       "      <td>4.3318</td>\n",
       "      <td>21.3508</td>\n",
       "      <td>9.3747</td>\n",
       "      <td>-5.7679</td>\n",
       "      <td>0.4538</td>\n",
       "      <td>5.7097</td>\n",
       "      <td>12.1853</td>\n",
       "      <td>11.6951</td>\n",
       "      <td>-3.4794</td>\n",
       "      <td>-0.1509</td>\n",
       "      <td>17.0766</td>\n",
       "      <td>12.9071</td>\n",
       "      <td>0.6502</td>\n",
       "      <td>8.3375</td>\n",
       "      <td>7.5104</td>\n",
       "      <td>-10.4414</td>\n",
       "      <td>3.7784</td>\n",
       "      <td>4.0272</td>\n",
       "      <td>20.2614</td>\n",
       "      <td>0.8237</td>\n",
       "      <td>-6.6060</td>\n",
       "      <td>1.3595</td>\n",
       "      <td>-6.5414</td>\n",
       "      <td>12.2554</td>\n",
       "      <td>15.4690</td>\n",
       "      <td>9.7661</td>\n",
       "      <td>0.4663</td>\n",
       "      <td>10.9107</td>\n",
       "      <td>-6.4811</td>\n",
       "      <td>3.9032</td>\n",
       "      <td>18.4450</td>\n",
       "      <td>15.2502</td>\n",
       "      <td>9.4846</td>\n",
       "      <td>9.6340</td>\n",
       "      <td>13.5600</td>\n",
       "      <td>13.7344</td>\n",
       "      <td>-8.6708</td>\n",
       "      <td>12.8264</td>\n",
       "      <td>-8.0187</td>\n",
       "      <td>22.8800</td>\n",
       "      <td>6.1382</td>\n",
       "      <td>30.2004</td>\n",
       "      <td>5.5373</td>\n",
       "      <td>2.4294</td>\n",
       "      <td>17.4894</td>\n",
       "      <td>-2.1165</td>\n",
       "      <td>13.7832</td>\n",
       "      <td>2.8288</td>\n",
       "      <td>-1.7876</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>5.0457</td>\n",
       "      <td>3.5383</td>\n",
       "      <td>8.8912</td>\n",
       "      <td>18.4672</td>\n",
       "      <td>1.8273</td>\n",
       "      <td>30.2822</td>\n",
       "      <td>9.4743</td>\n",
       "      <td>5.3983</td>\n",
       "      <td>10.1313</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>3.9345</td>\n",
       "      <td>-10.4854</td>\n",
       "      <td>8.7874</td>\n",
       "      <td>3.4940</td>\n",
       "      <td>17.0222</td>\n",
       "      <td>22.0108</td>\n",
       "      <td>-0.1467</td>\n",
       "      <td>10.5577</td>\n",
       "      <td>-1.8767</td>\n",
       "      <td>16.6669</td>\n",
       "      <td>2.2175</td>\n",
       "      <td>2.8475</td>\n",
       "      <td>10.6147</td>\n",
       "      <td>3.0183</td>\n",
       "      <td>2.5069</td>\n",
       "      <td>16.0805</td>\n",
       "      <td>0.4006</td>\n",
       "      <td>-5.5772</td>\n",
       "      <td>8.4289</td>\n",
       "      <td>20.8129</td>\n",
       "      <td>-18.4453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158756</th>\n",
       "      <td>0</td>\n",
       "      <td>12.8574</td>\n",
       "      <td>2.0760</td>\n",
       "      <td>13.7639</td>\n",
       "      <td>6.1902</td>\n",
       "      <td>10.2858</td>\n",
       "      <td>-17.0299</td>\n",
       "      <td>4.2630</td>\n",
       "      <td>11.8193</td>\n",
       "      <td>1.7253</td>\n",
       "      <td>5.5037</td>\n",
       "      <td>-8.3439</td>\n",
       "      <td>6.4556</td>\n",
       "      <td>13.9008</td>\n",
       "      <td>10.1658</td>\n",
       "      <td>4.7202</td>\n",
       "      <td>14.4317</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>4.3089</td>\n",
       "      <td>20.9777</td>\n",
       "      <td>30.1888</td>\n",
       "      <td>8.7158</td>\n",
       "      <td>23.8110</td>\n",
       "      <td>9.6532</td>\n",
       "      <td>2.8999</td>\n",
       "      <td>16.3957</td>\n",
       "      <td>13.9663</td>\n",
       "      <td>-5.7937</td>\n",
       "      <td>-1.7385</td>\n",
       "      <td>4.3329</td>\n",
       "      <td>7.2083</td>\n",
       "      <td>1.7930</td>\n",
       "      <td>9.6019</td>\n",
       "      <td>3.1234</td>\n",
       "      <td>10.8722</td>\n",
       "      <td>12.2387</td>\n",
       "      <td>8.4917</td>\n",
       "      <td>1.4631</td>\n",
       "      <td>3.4179</td>\n",
       "      <td>17.7466</td>\n",
       "      <td>-0.4296</td>\n",
       "      <td>-21.1903</td>\n",
       "      <td>15.1338</td>\n",
       "      <td>11.6920</td>\n",
       "      <td>11.1213</td>\n",
       "      <td>14.7781</td>\n",
       "      <td>-35.2833</td>\n",
       "      <td>8.1010</td>\n",
       "      <td>-15.8923</td>\n",
       "      <td>31.0343</td>\n",
       "      <td>3.0895</td>\n",
       "      <td>11.9509</td>\n",
       "      <td>9.4212</td>\n",
       "      <td>2.3883</td>\n",
       "      <td>6.3269</td>\n",
       "      <td>7.7916</td>\n",
       "      <td>22.5844</td>\n",
       "      <td>10.0625</td>\n",
       "      <td>5.8148</td>\n",
       "      <td>7.4935</td>\n",
       "      <td>8.1759</td>\n",
       "      <td>9.2272</td>\n",
       "      <td>4.8759</td>\n",
       "      <td>1.5125</td>\n",
       "      <td>-2.4770</td>\n",
       "      <td>6.7519</td>\n",
       "      <td>-2.4966</td>\n",
       "      <td>5.0772</td>\n",
       "      <td>4.3735</td>\n",
       "      <td>5.0172</td>\n",
       "      <td>3.2189</td>\n",
       "      <td>10.4415</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>1.9811</td>\n",
       "      <td>20.7102</td>\n",
       "      <td>-1.1383</td>\n",
       "      <td>26.6183</td>\n",
       "      <td>-5.7328</td>\n",
       "      <td>24.1632</td>\n",
       "      <td>5.3862</td>\n",
       "      <td>16.0197</td>\n",
       "      <td>2.3496</td>\n",
       "      <td>13.8723</td>\n",
       "      <td>5.7315</td>\n",
       "      <td>3.4639</td>\n",
       "      <td>-10.7063</td>\n",
       "      <td>23.6615</td>\n",
       "      <td>8.4274</td>\n",
       "      <td>10.7796</td>\n",
       "      <td>8.1086</td>\n",
       "      <td>-2.2447</td>\n",
       "      <td>-12.3504</td>\n",
       "      <td>7.0019</td>\n",
       "      <td>17.6296</td>\n",
       "      <td>11.5772</td>\n",
       "      <td>6.8764</td>\n",
       "      <td>1.1258</td>\n",
       "      <td>0.8873</td>\n",
       "      <td>40.5860</td>\n",
       "      <td>2.3236</td>\n",
       "      <td>1.3196</td>\n",
       "      <td>-0.1771</td>\n",
       "      <td>15.3244</td>\n",
       "      <td>16.7790</td>\n",
       "      <td>1.3234</td>\n",
       "      <td>10.6094</td>\n",
       "      <td>3.3319</td>\n",
       "      <td>7.4488</td>\n",
       "      <td>28.0465</td>\n",
       "      <td>14.2082</td>\n",
       "      <td>21.1184</td>\n",
       "      <td>6.8550</td>\n",
       "      <td>4.9583</td>\n",
       "      <td>4.1763</td>\n",
       "      <td>12.2843</td>\n",
       "      <td>3.2424</td>\n",
       "      <td>-1.8641</td>\n",
       "      <td>3.0779</td>\n",
       "      <td>-5.3664</td>\n",
       "      <td>-15.3091</td>\n",
       "      <td>-3.6629</td>\n",
       "      <td>44.7402</td>\n",
       "      <td>12.5925</td>\n",
       "      <td>10.3961</td>\n",
       "      <td>14.8784</td>\n",
       "      <td>2.8145</td>\n",
       "      <td>12.0404</td>\n",
       "      <td>12.0525</td>\n",
       "      <td>1.2739</td>\n",
       "      <td>-5.2733</td>\n",
       "      <td>21.4766</td>\n",
       "      <td>11.6402</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>5.2714</td>\n",
       "      <td>6.8046</td>\n",
       "      <td>5.5424</td>\n",
       "      <td>-3.4735</td>\n",
       "      <td>6.1762</td>\n",
       "      <td>1.0187</td>\n",
       "      <td>0.0791</td>\n",
       "      <td>-4.6594</td>\n",
       "      <td>4.2849</td>\n",
       "      <td>-10.0889</td>\n",
       "      <td>6.3656</td>\n",
       "      <td>15.1580</td>\n",
       "      <td>8.9310</td>\n",
       "      <td>4.1386</td>\n",
       "      <td>8.6308</td>\n",
       "      <td>-4.3091</td>\n",
       "      <td>3.8517</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>16.5291</td>\n",
       "      <td>6.3871</td>\n",
       "      <td>3.2788</td>\n",
       "      <td>15.5939</td>\n",
       "      <td>9.3531</td>\n",
       "      <td>-0.6829</td>\n",
       "      <td>13.1624</td>\n",
       "      <td>-2.5642</td>\n",
       "      <td>23.3752</td>\n",
       "      <td>10.5991</td>\n",
       "      <td>5.5211</td>\n",
       "      <td>5.4439</td>\n",
       "      <td>6.3204</td>\n",
       "      <td>7.6297</td>\n",
       "      <td>3.6555</td>\n",
       "      <td>16.5508</td>\n",
       "      <td>2.2963</td>\n",
       "      <td>2.4910</td>\n",
       "      <td>6.9498</td>\n",
       "      <td>5.9717</td>\n",
       "      <td>-7.4600</td>\n",
       "      <td>-7.1950</td>\n",
       "      <td>17.6936</td>\n",
       "      <td>0.8592</td>\n",
       "      <td>20.4348</td>\n",
       "      <td>14.2191</td>\n",
       "      <td>4.5858</td>\n",
       "      <td>10.0751</td>\n",
       "      <td>-11.9766</td>\n",
       "      <td>5.8413</td>\n",
       "      <td>0.4660</td>\n",
       "      <td>11.9586</td>\n",
       "      <td>4.6020</td>\n",
       "      <td>2.2069</td>\n",
       "      <td>23.2069</td>\n",
       "      <td>-3.0549</td>\n",
       "      <td>10.4571</td>\n",
       "      <td>-5.7635</td>\n",
       "      <td>7.0846</td>\n",
       "      <td>2.5808</td>\n",
       "      <td>-0.3987</td>\n",
       "      <td>9.0477</td>\n",
       "      <td>2.2993</td>\n",
       "      <td>4.8613</td>\n",
       "      <td>15.4447</td>\n",
       "      <td>-0.3476</td>\n",
       "      <td>5.7398</td>\n",
       "      <td>10.8543</td>\n",
       "      <td>11.2183</td>\n",
       "      <td>-9.3580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows  201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target    var_0   var_1    var_2    var_3    var_4    var_5   var_6  \\\n",
       "51590        0  10.3974 -5.9136  17.9117   3.8008   9.2937   7.5353  5.1777   \n",
       "27699        0  10.8077  2.4330   9.6733   4.1676   9.6226  -1.2920  5.1978   \n",
       "197832       0   8.4101 -6.1183   9.0142   4.5054  10.4665   4.7996  6.4269   \n",
       "165345       1  13.9492 -1.9923  10.1264   5.5747  12.1341   0.2001  4.9879   \n",
       "193250       0   9.0401 -0.1224   8.6602  10.6992  10.3945  -6.3093  3.8334   \n",
       "...        ...      ...     ...      ...      ...      ...      ...     ...   \n",
       "54444        0  12.4904  4.0971   9.5729   7.1030  12.2411  -3.8714  5.3379   \n",
       "77521        0  12.0756  0.6536  14.9369   5.3230   7.8097   2.3978  5.5501   \n",
       "49439        0   6.4827 -0.1254   9.8742   6.5121  12.8357   4.7679  6.9999   \n",
       "134720       0   6.8704 -0.7463  12.1506   9.4347  10.8363 -12.1858  4.7211   \n",
       "158756       0  12.8574  2.0760  13.7639   6.1902  10.2858 -17.0299  4.2630   \n",
       "\n",
       "          var_7   var_8   var_9  var_10   var_11   var_12   var_13   var_14  \\\n",
       "51590   17.5569  3.1515  5.4242  1.3896  -8.8148  14.0323  12.2651   8.2894   \n",
       "27699   16.2652 -1.8513  8.8580  5.1333   3.8998  14.0904   8.1751   4.3692   \n",
       "197832  18.4828  1.3070  8.3639  6.3789  -1.5832  13.8791  -0.2168   7.1908   \n",
       "165345  15.6888 -0.3861  7.2170  8.2519   1.1034  14.2549   5.5744   6.1776   \n",
       "193250  20.7618 -4.1906  8.3753  8.8166   5.8952  13.6831  10.0875   5.9167   \n",
       "...         ...     ...     ...     ...      ...      ...      ...      ...   \n",
       "54444   13.5599 -2.5057  6.5893  4.4358  -4.1894  13.9236  14.2479   9.2883   \n",
       "77521   23.0557 -4.4645  7.9690  3.1193  -1.5357  13.9197   9.7544  10.2130   \n",
       "49439   14.6801  1.2815  7.2162  2.0656  -5.0301  14.0485   6.7701  10.6114   \n",
       "134720  24.1893  1.2180  5.3363 -3.7784 -10.7418  14.1667   8.3164   6.9363   \n",
       "158756  11.8193  1.7253  5.5037 -8.3439   6.4556  13.9008  10.1658   4.7202   \n",
       "\n",
       "         var_15   var_16  var_17   var_18   var_19   var_20   var_21  var_22  \\\n",
       "51590   14.4554   8.7900  2.1192   9.3334   6.4745  13.8313   3.6496  3.2625   \n",
       "27699   14.7130  11.7514 -4.5023  17.8784   5.1581  21.2157  15.0899  6.1326   \n",
       "197832  14.7899   9.0505  3.1331   0.4895   6.7446   5.7861  13.7175  2.3444   \n",
       "165345  15.0508  15.2053 -2.7194  34.9080  18.2220  11.5677  15.0358  9.4101   \n",
       "193250  14.8246   6.7432  6.2375  12.3850  25.4931   3.6677  18.4586 -0.7245   \n",
       "...         ...      ...     ...      ...      ...      ...      ...     ...   \n",
       "54444   14.9302   9.6972  0.1625   7.6976  17.0411   9.4789   9.0912  6.6730   \n",
       "77521   14.6041  12.6836 -5.5208   4.5473  19.0755  24.6041  14.9972  5.8234   \n",
       "49439   14.7954   8.8016 -1.0517  18.9271  20.3470  22.9514  26.8826  5.7218   \n",
       "134720  14.2238  10.2787 -7.9082  29.4910  16.4082  17.6172  21.3965  3.9215   \n",
       "158756  14.4317   6.2654  4.3089  20.9777  30.1888   8.7158  23.8110  9.6532   \n",
       "\n",
       "        var_23   var_24   var_25   var_26  var_27  var_28   var_29   var_30  \\\n",
       "51590   2.5559   7.4217  13.4497 -10.1505 -3.0011  4.2576   4.8010  -8.4010   \n",
       "27699   2.0407  10.6846  13.9065  -6.7307 -1.2344  4.8528   5.5035 -24.4808   \n",
       "197832  2.3788   6.2723  13.8015   1.6004  0.0926  4.7560   2.6710  -2.7872   \n",
       "165345  3.1746  14.2896  13.8907   1.2564 -0.2592  6.3966   5.4585  -3.6115   \n",
       "193250  3.5133  14.4356  13.4201  -9.4178 -0.3241  6.5649   8.5876   1.4522   \n",
       "...        ...      ...      ...      ...     ...     ...      ...      ...   \n",
       "54444   2.7345   7.9862  13.7540 -13.0887 -3.1314  5.4377   6.7783 -10.7636   \n",
       "77521   3.5070   9.9510  13.8415  -8.9524 -1.3360  4.4207   4.4859 -16.4447   \n",
       "49439   2.1176  13.1784  13.9020  -1.8656 -2.7829  5.9136   3.7484 -15.9028   \n",
       "134720  2.6106   9.1471  13.4345  -2.5250 -0.5275  4.0404  10.3723 -19.9593   \n",
       "158756  2.8999  16.3957  13.9663  -5.7937 -1.7385  4.3329   7.2083   1.7930   \n",
       "\n",
       "         var_31  var_32   var_33   var_34  var_35  var_36  var_37   var_38  \\\n",
       "51590   10.6937 -2.8658  12.2040  11.2481  1.2608  2.6706  5.2132   6.5590   \n",
       "27699    9.3302 -2.6454  17.6198  10.9933  5.2141 -3.3251  9.4923   8.8039   \n",
       "197832   8.1213  3.9318   5.2983  12.0709  7.3224  9.6489  1.8804  12.6210   \n",
       "165345   5.8313  0.6771  13.3021  11.1677  1.5277 -0.4913  3.7415   4.9268   \n",
       "193250  11.6911 -3.9003  11.7968  11.1777 -8.4646  0.3705  4.2674   8.7305   \n",
       "...         ...     ...      ...      ...     ...     ...     ...      ...   \n",
       "54444   12.6498  0.8116  22.1980  11.4345  8.5225  0.0637  8.3442   9.6516   \n",
       "77521    9.8363  0.9087  18.9750  12.4266 -0.7335  2.0133  6.0986   9.3941   \n",
       "49439    8.0342 -5.0323  11.7231  11.2059  9.3478  3.4257  5.7810   9.0962   \n",
       "134720   7.6379 -4.6488  16.2245  10.6183 -2.0612  6.6444  4.4462  13.2900   \n",
       "158756   9.6019  3.1234  10.8722  12.2387  8.4917  1.4631  3.4179  17.7466   \n",
       "\n",
       "        var_39   var_40   var_41   var_42   var_43   var_44   var_45   var_46  \\\n",
       "51590   6.7980 -11.9985  18.5377  11.0236  11.1724   3.6655 -21.0209  15.5505   \n",
       "27699   1.2540  -5.1073  12.9980  11.5622  11.7701   5.1420 -29.8639  12.0049   \n",
       "197832  0.1015  -0.3617  10.2813  10.4964  11.2196   7.9191  -2.1707  10.3119   \n",
       "165345 -3.8037  -6.8895  14.5932  11.7038  12.0420  -2.2053  -8.0283  13.5874   \n",
       "193250  2.7650 -19.4462   0.3172  10.4609  11.1559  -0.2677  -4.4435   7.1143   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   5.1408 -16.9031  14.7474  10.8899  10.8464   5.7639  21.4889   8.0400   \n",
       "77521  -6.2155  -9.6860  16.2583  11.0130  11.5700  10.4327 -48.8558  13.8643   \n",
       "49439   6.6659  -4.5044  -1.5156  10.4338  11.2274  11.0851  25.6511   7.3391   \n",
       "134720 -3.9788 -18.5511  -2.3109  10.5114  12.0860  14.5412   7.6649  10.6118   \n",
       "158756 -0.4296 -21.1903  15.1338  11.6920  11.1213  14.7781 -35.2833   8.1010   \n",
       "\n",
       "         var_47   var_48   var_49   var_50   var_51   var_52  var_53   var_54  \\\n",
       "51590    0.4496  18.3813   3.9219  13.0180  15.0474  -9.7847  6.2350   9.7629   \n",
       "27699    1.2001  20.2347  21.4472  13.0744  12.5868  -4.4248  5.1277  -8.5259   \n",
       "197832 -11.3343   0.5858  11.5365  12.7841   1.7925  -7.1104  5.4393  -6.1413   \n",
       "165345  -5.6721   4.8935  18.3564  13.3420   2.4080 -13.0974  6.2421  -1.2747   \n",
       "193250  -8.8506   0.9537  16.4792  11.7772  13.8041  -4.9998  5.4787  -6.6192   \n",
       "...         ...      ...      ...      ...      ...      ...     ...      ...   \n",
       "54444  -11.2252  15.7474  31.3351  13.8472   0.4772  -5.5649  6.5326  -8.2557   \n",
       "77521  -11.1942  15.8437  15.1464  13.3754   1.5810   0.9192  6.5946   3.6180   \n",
       "49439  -19.6370  20.4729  25.0191  13.0447  13.0015   2.1116  6.8156  16.5393   \n",
       "134720   1.0478   2.4986  24.9719  12.0302   4.0296   0.9277  5.9675 -15.7085   \n",
       "158756 -15.8923  31.0343   3.0895  11.9509   9.4212   2.3883  6.3269   7.7916   \n",
       "\n",
       "         var_55   var_56  var_57  var_58   var_59   var_60   var_61  var_62  \\\n",
       "51590    8.7289  20.5470  5.8125  8.6421   7.0765  15.5821 -14.2911  2.1746   \n",
       "27699   14.2198  21.5951  4.6118  1.4066   7.8775  14.0038 -15.8245  3.9117   \n",
       "197832  20.8171  15.2871  6.3777 -0.7626  10.3784   6.5166 -20.3622 -0.8036   \n",
       "165345   8.5735  16.7074  7.7924  6.1628   9.1747   8.5397  -4.8144  1.2895   \n",
       "193250  22.0729  18.1839  5.7545 -1.7124  10.2638  16.2137 -18.6633  3.6817   \n",
       "...         ...      ...     ...     ...      ...      ...      ...     ...   \n",
       "54444   14.5900  17.2057  7.0540  1.0443  10.1205  10.3625   0.6524  2.4346   \n",
       "77521   17.3741  18.7799  6.2369  3.8269   8.4710   7.8216 -17.6335 -0.2381   \n",
       "49439    6.7834  16.5501  7.0152  6.6720   8.6135  15.5433 -19.6367  2.1462   \n",
       "134720   3.6506  16.2006  6.2206  1.9487   9.3900  17.3474  -1.3388  0.3700   \n",
       "158756  22.5844  10.0625  5.8148  7.4935   8.1759   9.2272   4.8759  1.5125   \n",
       "\n",
       "        var_63  var_64   var_65  var_66   var_67  var_68  var_69   var_70  \\\n",
       "51590   5.8409  5.4659  11.4184  3.9363  10.3117  5.0200 -6.4885  29.9411   \n",
       "27699   0.3438  5.4982  -0.3704  5.8907   6.6712  5.0176 -4.7609  34.1382   \n",
       "197832  3.0076  9.9220  -3.8246  5.9093  14.9093  5.0252 -3.0187  33.0230   \n",
       "165345  2.8638  7.9320  -5.4780  3.1890  -0.8125  5.0236 -3.4999  30.8681   \n",
       "193250  3.3001  6.3637   0.0330  4.3216   3.3222  5.0265  4.9121  25.9164   \n",
       "...        ...     ...      ...     ...      ...     ...     ...      ...   \n",
       "54444  -1.7746  6.0212   2.7454  4.7968   4.3666  5.0092 -9.8259  16.0449   \n",
       "77521  -1.1925  5.4755   0.6752  5.2577  13.4306  5.0252 -0.5491  33.0498   \n",
       "49439   1.8310  4.1149  -7.3638  5.8404   5.2266  5.0183 -0.8607  19.4773   \n",
       "134720 -1.9230  8.6771  -4.9034  7.0388  10.5973  5.0177 -3.5720  23.3534   \n",
       "158756 -2.4770  6.7519  -2.4966  5.0772   4.3735  5.0172  3.2189  10.4415   \n",
       "\n",
       "        var_71  var_72   var_73   var_74   var_75   var_76   var_77  var_78  \\\n",
       "51590   1.0439  5.1866  23.1108  27.9835  18.3182  13.4834  20.9578  0.6799   \n",
       "27699   0.6152  4.8368  22.1731   0.8923  17.7268   8.1880  15.2038  4.2709   \n",
       "197832  0.6794 -5.1704  15.1073  10.6373  15.6992   3.8031  20.5601  6.2441   \n",
       "165345  0.8785 -1.7515  17.1378  49.0808   3.4056  -6.5321  18.9915  8.1503   \n",
       "193250  0.5259 -0.3095  24.0295  42.7918   7.8532   3.8120  15.4269  6.2945   \n",
       "...        ...     ...      ...      ...      ...      ...      ...     ...   \n",
       "54444   0.7549 -1.7623  15.2569  23.0787  20.9913  -0.0052  15.3383  3.0695   \n",
       "77521   0.6976  2.8715   6.3629  15.0014   5.6505   5.9231  14.8444  5.8406   \n",
       "49439   0.9079 -3.6426  21.6163  23.0934  23.3446   6.5704  21.7256  7.5744   \n",
       "134720  0.2777 -2.6334  27.3166  30.3428  10.7064   5.8044  21.0938  6.0646   \n",
       "158756  0.5339  1.9811  20.7102  -1.1383  26.6183  -5.7328  24.1632  5.3862   \n",
       "\n",
       "         var_79   var_80   var_81   var_82   var_83   var_84   var_85  \\\n",
       "51590   13.3298  12.2419  14.8175 -18.4265   1.1093   1.4114  15.1929   \n",
       "27699   16.0487  12.3891  16.7520  -4.6216   2.5369   3.7545  23.4362   \n",
       "197832  14.5273   2.6096  17.9988  -0.3455 -10.6491   0.3746  26.0284   \n",
       "165345  15.1397  -7.2213  14.6778  -5.9811  -3.6889  -3.4124  21.2494   \n",
       "193250  15.2070  12.4370  16.3856  -4.9115   7.0759   2.9776  13.8435   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   13.5407  10.8407  19.1084  -3.0142  -2.1188   1.4255  16.1721   \n",
       "77521   13.9866  -5.2695  16.9736  -5.0933 -13.3969   0.2658  19.7691   \n",
       "49439   12.4480   1.0592  16.5670  14.2049   3.0032  -7.9250  21.4960   \n",
       "134720  13.8305   8.8740  16.8102  -5.9580   7.7356   3.6891  17.2579   \n",
       "158756  16.0197   2.3496  13.8723   5.7315   3.4639 -10.7063  23.6615   \n",
       "\n",
       "         var_86   var_87   var_88  var_89   var_90  var_91   var_92   var_93  \\\n",
       "51590    3.1095   1.1317  12.3433  2.4092 -47.7806  7.0021  12.0954  10.8004   \n",
       "27699    8.2782   8.1417   9.9846  0.8077  -8.6084  6.9590  13.2010  11.2578   \n",
       "197832  -1.1477   9.3538   5.6864  7.5691 -23.0817  6.8478  11.8861  10.1202   \n",
       "165345  -8.4622  14.7261  10.9138  2.4000 -23.1283  6.8857   4.0456  10.4192   \n",
       "193250   6.3386  10.2212  11.3405  3.9957  -8.4941  7.1574  17.1164   9.7088   \n",
       "...         ...      ...      ...     ...      ...     ...      ...      ...   \n",
       "54444   15.3741  13.3447   9.9718  0.4362  -8.9597  7.1372  11.9309  10.6130   \n",
       "77521  -13.7875  14.6716  10.0514  0.6721 -18.5091  7.1056  18.0088  10.8887   \n",
       "49439   11.9197  12.5635   7.8239  2.9544 -16.4258  7.0642  12.1038  11.4935   \n",
       "134720   8.2796  10.9035   7.7676  0.2128  -9.9755  7.2324   6.0425   9.4391   \n",
       "158756   8.4274  10.7796   8.1086 -2.2447 -12.3504  7.0019  17.6296  11.5772   \n",
       "\n",
       "         var_94  var_95   var_96   var_97  var_98  var_99  var_100  var_101  \\\n",
       "51590   13.6917 -1.2344  23.2131   6.5506  2.4426 -0.7730  -3.4700   7.4344   \n",
       "27699   14.3965 -0.6759  23.9976  15.3405  1.7837 -1.3212  -0.8734  18.7022   \n",
       "197832  12.5871 -0.4373  13.8322   0.6236  2.3191 -1.9994  -9.5843  16.2810   \n",
       "165345  11.3547  0.4168  10.0579  20.4419  2.7041  0.3250  -4.9686  18.5949   \n",
       "193250  12.6389  0.1626  10.1873  22.8263  1.0635  1.0045 -10.3631  16.6667   \n",
       "...         ...     ...      ...      ...     ...     ...      ...      ...   \n",
       "54444   16.3536  0.0897  21.5935  42.3055  0.8472 -0.1292  -9.2950  14.3201   \n",
       "77521   10.9388  0.1835   7.0454  25.8421  2.7307 -1.4396  -5.4111  22.3420   \n",
       "49439    8.8204 -0.5129  22.1831  15.6572  1.4812  1.4177  -3.7730  14.4499   \n",
       "134720  10.2446 -0.2816  33.5611  29.8170  0.1766 -2.8194  -3.5142  14.3217   \n",
       "158756   6.8764  1.1258   0.8873  40.5860  2.3236  1.3196  -0.1771  15.3244   \n",
       "\n",
       "        var_102  var_103  var_104  var_105  var_106  var_107  var_108  \\\n",
       "51590   33.2912   1.6448  10.6332   3.8944   6.1798  26.9649  14.0846   \n",
       "27699   14.4519   1.7998  11.3719   5.8409   8.7993   5.7631  13.9356   \n",
       "197832   9.7850   1.7776  13.5545   5.4013   5.8620  31.5440  14.1263   \n",
       "165345   7.7888   1.1942  12.6430   5.3534   5.4759   9.0434  14.2632   \n",
       "193250  22.0601   1.6457  10.6608   3.4468   7.1010  12.5263  14.4222   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   36.7479   1.6059   9.6548   5.1387   8.1292  16.3701  14.2756   \n",
       "77521    4.9562   1.7907  13.3697   2.6089   9.4761  15.0610  14.4500   \n",
       "49439   11.2579   1.2567  10.1182   3.8538   8.7105  18.4011  13.9360   \n",
       "134720  37.2495   1.7883  11.5307   3.8483   6.3715  11.8986  14.1428   \n",
       "158756  16.7790   1.3234  10.6094   3.3319   7.4488  28.0465  14.2082   \n",
       "\n",
       "        var_109  var_110  var_111  var_112  var_113  var_114  var_115  \\\n",
       "51590   11.3842  13.5426   5.9930   2.5665   1.3369   2.9007   3.2890   \n",
       "27699   21.0540   9.8195   6.9385   1.9997  13.4560   3.7082  -1.5049   \n",
       "197832  20.8892  11.5224   5.4217   6.2807  15.0956   4.3287  -0.2405   \n",
       "165345  14.1875   1.9956   8.3070   2.7901  14.7004   2.6474  -0.1101   \n",
       "193250  14.8845   0.9971   6.1599   1.1125   6.3257   2.0698   1.0220   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   19.7277   1.7365   7.2307   3.2307  12.3384   4.4210   4.2612   \n",
       "77521   23.0233   7.2020   5.7471   3.5116  10.2647   3.2930   3.0700   \n",
       "49439   13.9145   0.3429   6.3459   2.4649  11.1353   4.2721  -2.0414   \n",
       "134720  11.2411   6.6548   4.4822   3.2330  17.0146   3.9640   3.2305   \n",
       "158756  21.1184   6.8550   4.9583   4.1763  12.2843   3.2424  -1.8641   \n",
       "\n",
       "        var_116  var_117  var_118  var_119  var_120  var_121  var_122  \\\n",
       "51590    2.9834   6.8565 -15.4291   0.4685   8.0814  11.3221   3.0953   \n",
       "27699    0.4664  30.5513 -12.6981   1.9631   2.6705  13.8372  -5.0672   \n",
       "197832  -0.1866 -10.7498  -9.1729  -2.0486  26.2988  10.1711   1.4742   \n",
       "165345   5.3927  14.9807  -3.1040   8.0356  33.8359  12.1279   3.1443   \n",
       "193250   2.0541  24.7152  -8.4836  -1.3805  34.7891   8.5580  -0.2724   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444    3.5299  22.4758 -14.5210  -0.6718  28.8213  11.1212  -0.3664   \n",
       "77521    3.1054  23.2861 -11.1304   3.0325  32.9108  12.9628  -5.5922   \n",
       "49439    2.4057  17.0167 -12.0394   7.0021  40.1013  14.7136   2.9664   \n",
       "134720  -1.4797   4.2941  -5.5423   4.3318  21.3508   9.3747  -5.7679   \n",
       "158756   3.0779  -5.3664 -15.3091  -3.6629  44.7402  12.5925  10.3961   \n",
       "\n",
       "        var_123  var_124  var_125  var_126  var_127  var_128  var_129  \\\n",
       "51590    6.6758   9.0801  12.4159  12.2032   1.6486  -0.0429  15.4525   \n",
       "27699    2.0362   4.3461  12.8119  12.6473   5.4512  -1.7088  19.9176   \n",
       "197832   5.9040   1.8949  12.3401  13.5984   5.8305   0.5503   6.0700   \n",
       "165345   9.8729  11.0123  12.6269  13.5159   5.1756  -2.6192  16.4620   \n",
       "193250   3.2507   1.3492  12.1872  13.4285   3.9408   2.3803  20.2104   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   12.6570   0.9710  12.7450  12.6463  -0.3941  -3.5594  18.6261   \n",
       "77521    3.6516   4.8402  12.8812  12.8668   3.4692  -5.4893  15.9122   \n",
       "49439   13.1119   5.0863  12.9621  12.4400  -0.3585   0.0893  21.0114   \n",
       "134720   0.4538   5.7097  12.1853  11.6951  -3.4794  -0.1509  17.0766   \n",
       "158756  14.8784   2.8145  12.0404  12.0525   1.2739  -5.2733  21.4766   \n",
       "\n",
       "        var_130  var_131  var_132  var_133  var_134  var_135  var_136  \\\n",
       "51590   13.8214   0.8637   6.1031   6.4719  -7.0811  -7.3782  10.9045   \n",
       "27699   11.8387   1.5735   9.4802   6.5240  -1.0628   1.4667  24.4042   \n",
       "197832  12.0291   0.8113   9.2694   7.4065  -4.3575  -1.3944  37.9452   \n",
       "165345  11.0076  -0.1398   6.0427   7.3026  -3.9317   5.2346  15.9069   \n",
       "193250  11.2887   0.2988   8.9386   6.6793   0.7156 -12.2984   8.3998   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   11.1020   0.1366   9.8752   6.6927 -11.8125  -8.3717  26.3973   \n",
       "77521   12.4401   1.3717   6.5303   6.5312   2.1923   2.9965  13.5914   \n",
       "49439   12.2539   0.6833  10.8232   6.7557  -3.4472   4.9295  14.9176   \n",
       "134720  12.9071   0.6502   8.3375   7.5104 -10.4414   3.7784   4.0272   \n",
       "158756  11.6402   0.9804   5.2714   6.8046   5.5424  -3.4735   6.1762   \n",
       "\n",
       "        var_137  var_138  var_139  var_140  var_141  var_142  var_143  \\\n",
       "51590   17.5765   0.7724   9.7178   1.1914  -6.1055  15.9892  14.7951   \n",
       "27699    6.7886   9.4675   4.8104  -3.8836   2.8022  14.0605  10.6581   \n",
       "197832  21.4039  -3.5463  16.0042  10.0060  10.0897   6.9294  11.3180   \n",
       "165345  13.0027  -8.2954   0.1984  10.8152  -2.5145  23.0875   8.8675   \n",
       "193250  20.1393   7.5714   5.7452   0.1243  -2.0019  11.0512  10.9457   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   11.7138  -0.5914   9.3315  -2.3878   1.8984   5.8921   8.3873   \n",
       "77521   30.7308  -0.3007  11.1424   8.6554  -1.3220   8.2257  12.8353   \n",
       "49439   11.6466  -6.1010  12.5493  10.9734   3.6920   8.4625  11.3665   \n",
       "134720  20.2614   0.8237  -6.6060   1.3595  -6.5414  12.2554  15.4690   \n",
       "158756   1.0187   0.0791  -4.6594   4.2849 -10.0889   6.3656  15.1580   \n",
       "\n",
       "        var_144  var_145  var_146  var_147  var_148  var_149  var_150  \\\n",
       "51590    8.6756  -3.5333   8.9492 -12.6419   3.8829   2.8378  15.1440   \n",
       "27699    8.9250   4.0323   7.0351   3.6557   4.2898  21.1085  19.0920   \n",
       "197832   7.8723   8.7166  11.6819   7.9200   3.8769  14.7180  13.3159   \n",
       "165345   9.4659   8.6054   5.1493 -11.6052   4.2768 -15.9487  15.3167   \n",
       "193250   7.8604   2.9153  10.1934 -14.4886   4.1416  -4.0070  17.8525   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444    7.6926   1.1354   8.9153  -7.3375   3.8037  -7.5865  19.0708   \n",
       "77521    8.9400  13.3364  11.7449  -0.4871   3.7961  -6.4872  15.4872   \n",
       "49439    8.2130   4.8325  10.0756 -19.9297   3.9025   8.0388  15.9884   \n",
       "134720   9.7661   0.4663  10.9107  -6.4811   3.9032  18.4450  15.2502   \n",
       "158756   8.9310   4.1386   8.6308  -4.3091   3.8517   0.7258  16.5291   \n",
       "\n",
       "        var_151  var_152  var_153  var_154  var_155  var_156  var_157  \\\n",
       "51590   11.7746   8.9811  14.9659  -5.1464   0.5794  13.2541 -10.0531   \n",
       "27699    7.2070   7.5173  15.0867   0.2947  -5.6385  13.3836  -0.9613   \n",
       "197832  12.9321   7.0719  19.9343   2.3284  -0.8067  13.6185  -2.6548   \n",
       "165345  13.1224   6.1052  15.9495  13.0445   2.9080  13.0394  -8.1259   \n",
       "193250  13.7682   4.9644  16.3574   9.3638 -15.9545  14.1494   7.5398   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   13.6887   7.0208  16.2498  10.5968  12.5921  13.3929 -13.8908   \n",
       "77521   17.3317   9.6841  19.7184  -0.3729   2.9087  12.5196  -4.0016   \n",
       "49439   11.2205   6.7047  17.3389  10.5394  -1.8955  12.0981   3.8825   \n",
       "134720   9.4846   9.6340  13.5600  13.7344  -8.6708  12.8264  -8.0187   \n",
       "158756   6.3871   3.2788  15.5939   9.3531  -0.6829  13.1624  -2.5642   \n",
       "\n",
       "        var_158  var_159  var_160  var_161  var_162  var_163  var_164  \\\n",
       "51590   16.2484   8.8786  29.6674   5.3186   7.2194  10.5803  -4.6134   \n",
       "27699   26.1073  14.1441  16.0268   5.5152   5.2663   8.4773  -3.6637   \n",
       "197832  23.3571  16.7988  36.7339   5.7112   5.8650  13.4639   3.4726   \n",
       "165345  21.7006  14.3895  24.0276   5.6229   7.2125  10.8350 -14.2928   \n",
       "193250  12.7183   9.1931  30.0005   5.5811   5.7817  11.5418  -5.3090   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   11.8437  11.2875  20.0500   5.8561   4.1490  11.6025   1.9091   \n",
       "77521   20.3712   9.0545  37.2184   5.7610   3.3126   4.1035   2.9809   \n",
       "49439   34.6457   9.1157  20.3925   5.6120   5.3182  19.0850  -2.7405   \n",
       "134720  22.8800   6.1382  30.2004   5.5373   2.4294  17.4894  -2.1165   \n",
       "158756  23.3752  10.5991   5.5211   5.4439   6.3204   7.6297   3.6555   \n",
       "\n",
       "        var_165  var_166  var_167  var_168  var_169  var_170  var_171  \\\n",
       "51590   27.9719   2.5454 -10.8715   5.5696   5.2786   4.1202 -12.4553   \n",
       "27699   15.7517   3.2652  -8.4806   4.3225   5.5613  -3.5126  -5.4094   \n",
       "197832  19.7280   2.9546 -17.2973   5.3176   5.6618   8.9496   0.3537   \n",
       "165345  17.6844   2.1648 -12.1571  11.4552   5.0698   4.4769  -7.4787   \n",
       "193250   8.4325   3.1602  -4.3750   6.4332   6.0029  -5.2903   2.6729   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   18.4879   2.9838  -5.4682   4.4443   5.8550   4.3673  -3.1908   \n",
       "77521   25.4507   2.9142   7.0334   7.1331   6.0174   6.0527   3.2659   \n",
       "49439   19.5322   2.5623 -14.6855   5.5657   5.2742  -7.3142   7.6154   \n",
       "134720  13.7832   2.8288  -1.7876   7.0118   5.0457   3.5383   8.8912   \n",
       "158756  16.5508   2.2963   2.4910   6.9498   5.9717  -7.4600  -7.1950   \n",
       "\n",
       "        var_172  var_173  var_174  var_175  var_176  var_177  var_178  \\\n",
       "51590   21.5342   1.8643  14.5976  10.1628  -2.0807  10.2684 -14.7208   \n",
       "27699    9.3867   8.4791  19.6515  12.9587   0.5598  12.7234   3.4768   \n",
       "197832  29.1036   2.1857  30.0889   7.7824   0.2339   8.8942 -13.6776   \n",
       "165345  -4.3707  -5.9778  13.7006   9.6750  -3.3032   8.2330   4.4451   \n",
       "193250  20.1646   9.0153  14.7169  13.8902   1.0081  11.3524   4.8887   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   12.1283   2.3392  22.8965  13.7958   2.9110  11.7910  -8.3711   \n",
       "77521   18.1972   3.4801  26.0080   9.2702   0.1574  13.3378   7.9268   \n",
       "49439   12.7750   0.3091   3.9351  14.6798  -8.9574  14.1055   7.5467   \n",
       "134720  18.4672   1.8273  30.2822   9.4743   5.3983  10.1313   0.9704   \n",
       "158756  17.6936   0.8592  20.4348  14.2191   4.5858  10.0751 -11.9766   \n",
       "\n",
       "        var_179  var_180  var_181  var_182  var_183  var_184  var_185  \\\n",
       "51590    2.6609 -12.6520   9.3089   3.3315   2.6496  16.0369  -4.5869   \n",
       "27699    3.6308   0.8005  11.2543  14.0389   9.5484   3.1812   1.7877   \n",
       "197832  -3.6005  -5.5389  10.3645   3.7962  13.8262  -3.8803 -10.7126   \n",
       "165345   2.5123  -2.7197  10.6328 -13.3813   6.2846  -1.5199  -3.3075   \n",
       "193250  -0.1036   8.1350  10.3610   5.4340  10.0519   4.1118  -3.7783   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444   -0.9903  -8.7420   8.3954   6.7660  16.2471  14.3125  -4.0452   \n",
       "77521    4.2928   2.1362   8.9557   0.9003   3.5912  14.5755  -3.4588   \n",
       "49439    0.8776   3.5579  10.6485 -12.1161  16.5248  26.2340 -11.3249   \n",
       "134720   3.9345 -10.4854   8.7874   3.4940  17.0222  22.0108  -0.1467   \n",
       "158756   5.8413   0.4660  11.9586   4.6020   2.2069  23.2069  -3.0549   \n",
       "\n",
       "        var_186  var_187  var_188  var_189  var_190  var_191  var_192  \\\n",
       "51590    7.8993  -9.2009  19.8196   2.3064  -2.9676   4.9930  -0.8115   \n",
       "27699   13.0856 -22.5574  22.7242  -0.4419   5.7070   8.7324   2.2800   \n",
       "197832   9.9370 -15.4758  18.7759  -0.8889   1.8659   7.6620   0.9727   \n",
       "165345   7.9794 -15.3099  18.4142   0.9790   4.0636   4.7780  -0.8486   \n",
       "193250   5.7147  -2.6823  21.2950   0.0053   5.5215   9.1564   2.9410   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "54444    8.6593 -20.7593  15.1800   2.4887  11.4350   7.5056   0.6534   \n",
       "77521   10.1281 -29.7886  20.5672  -0.8228   0.3689   8.3071   1.4771   \n",
       "49439   13.2318   5.8970  14.0280  -1.0962   1.2615   8.5350   2.2625   \n",
       "134720  10.5577  -1.8767  16.6669   2.2175   2.8475  10.6147   3.0183   \n",
       "158756  10.4571  -5.7635   7.0846   2.5808  -0.3987   9.0477   2.2993   \n",
       "\n",
       "        var_193  var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "51590    3.2842  23.4969   1.3758   0.6114   9.5197  18.5446 -12.9629  \n",
       "27699    6.8707  19.6816  -0.3650  -0.4176   8.9947  16.9930   5.4797  \n",
       "197832   2.9958  13.1156  -1.0957  -1.6878   9.5792  15.7510 -13.8635  \n",
       "165345   6.5988  22.5410  -0.9957  -7.3223   8.3877  17.9658  10.0786  \n",
       "193250   2.5742  22.1022  -2.0655   7.5666   8.4366  17.7034   3.3621  \n",
       "...         ...      ...      ...      ...      ...      ...      ...  \n",
       "54444    5.0310  19.6522   0.2626  -6.8085   8.2127  21.1710 -20.4727  \n",
       "77521    5.1579  18.0875  -1.6203   2.5211   9.1977  18.1966 -18.0426  \n",
       "49439    6.9436  22.4766   2.1484   9.3593   6.8472  13.0793   4.4414  \n",
       "134720   2.5069  16.0805   0.4006  -5.5772   8.4289  20.8129 -18.4453  \n",
       "158756   4.8613  15.4447  -0.3476   5.7398  10.8543  11.2183  -9.3580  \n",
       "\n",
       "[200000 rows x 201 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c051b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>2.1743</td>\n",
       "      <td>-3.4132</td>\n",
       "      <td>9.4763</td>\n",
       "      <td>13.3102</td>\n",
       "      <td>26.5376</td>\n",
       "      <td>1.4403</td>\n",
       "      <td>14.7100</td>\n",
       "      <td>6.0454</td>\n",
       "      <td>9.5426</td>\n",
       "      <td>17.1554</td>\n",
       "      <td>14.1104</td>\n",
       "      <td>24.3627</td>\n",
       "      <td>2.0323</td>\n",
       "      <td>6.7602</td>\n",
       "      <td>3.9141</td>\n",
       "      <td>-0.4851</td>\n",
       "      <td>2.5240</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>2.5516</td>\n",
       "      <td>15.5752</td>\n",
       "      <td>-13.4221</td>\n",
       "      <td>7.2739</td>\n",
       "      <td>16.0094</td>\n",
       "      <td>9.7268</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>12.0039</td>\n",
       "      <td>13.8571</td>\n",
       "      <td>-0.7338</td>\n",
       "      <td>-1.9245</td>\n",
       "      <td>15.4462</td>\n",
       "      <td>12.8287</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>9.6508</td>\n",
       "      <td>6.5674</td>\n",
       "      <td>5.1726</td>\n",
       "      <td>3.1345</td>\n",
       "      <td>29.4547</td>\n",
       "      <td>31.4045</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>15.6599</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>-5.6011</td>\n",
       "      <td>19.0614</td>\n",
       "      <td>11.2663</td>\n",
       "      <td>8.6989</td>\n",
       "      <td>8.3694</td>\n",
       "      <td>11.5659</td>\n",
       "      <td>-16.4727</td>\n",
       "      <td>4.0288</td>\n",
       "      <td>17.9244</td>\n",
       "      <td>18.5177</td>\n",
       "      <td>10.7800</td>\n",
       "      <td>9.0056</td>\n",
       "      <td>16.6964</td>\n",
       "      <td>10.4838</td>\n",
       "      <td>1.6573</td>\n",
       "      <td>12.1749</td>\n",
       "      <td>-13.1324</td>\n",
       "      <td>17.6054</td>\n",
       "      <td>11.5423</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>1.8944</td>\n",
       "      <td>0.6939</td>\n",
       "      <td>-13.6950</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>35.4734</td>\n",
       "      <td>1.7093</td>\n",
       "      <td>15.1866</td>\n",
       "      <td>2.6227</td>\n",
       "      <td>7.3412</td>\n",
       "      <td>32.0888</td>\n",
       "      <td>13.9550</td>\n",
       "      <td>13.0858</td>\n",
       "      <td>6.6203</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>5.3523</td>\n",
       "      <td>8.5426</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>4.1569</td>\n",
       "      <td>3.0454</td>\n",
       "      <td>7.8522</td>\n",
       "      <td>-11.5100</td>\n",
       "      <td>7.5109</td>\n",
       "      <td>31.5899</td>\n",
       "      <td>9.5018</td>\n",
       "      <td>8.2736</td>\n",
       "      <td>10.1633</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>12.5942</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>2.4354</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>16.5346</td>\n",
       "      <td>12.4205</td>\n",
       "      <td>-0.1780</td>\n",
       "      <td>5.7582</td>\n",
       "      <td>7.0513</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>-8.9921</td>\n",
       "      <td>9.7797</td>\n",
       "      <td>18.1577</td>\n",
       "      <td>-1.9721</td>\n",
       "      <td>16.1622</td>\n",
       "      <td>3.6937</td>\n",
       "      <td>6.6803</td>\n",
       "      <td>-0.3243</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>8.6086</td>\n",
       "      <td>11.0738</td>\n",
       "      <td>8.9231</td>\n",
       "      <td>11.7700</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>-4.4223</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>14.8743</td>\n",
       "      <td>9.4317</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>-0.5687</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>-9.6953</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>10.6261</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>-13.5174</td>\n",
       "      <td>2.2439</td>\n",
       "      <td>11.5283</td>\n",
       "      <td>12.0406</td>\n",
       "      <td>4.1006</td>\n",
       "      <td>-7.9078</td>\n",
       "      <td>11.1405</td>\n",
       "      <td>-5.7864</td>\n",
       "      <td>20.7477</td>\n",
       "      <td>6.8874</td>\n",
       "      <td>12.9143</td>\n",
       "      <td>19.5856</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>6.4059</td>\n",
       "      <td>9.3124</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>15.6372</td>\n",
       "      <td>5.8200</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>9.1854</td>\n",
       "      <td>12.5963</td>\n",
       "      <td>-10.3734</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>3.7163</td>\n",
       "      <td>-1.1016</td>\n",
       "      <td>7.3667</td>\n",
       "      <td>9.8565</td>\n",
       "      <td>5.0228</td>\n",
       "      <td>-5.7828</td>\n",
       "      <td>2.3612</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>12.1719</td>\n",
       "      <td>19.7312</td>\n",
       "      <td>19.4465</td>\n",
       "      <td>4.5048</td>\n",
       "      <td>23.2378</td>\n",
       "      <td>6.3191</td>\n",
       "      <td>12.8046</td>\n",
       "      <td>7.4729</td>\n",
       "      <td>15.7811</td>\n",
       "      <td>13.3529</td>\n",
       "      <td>10.1852</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>19.0773</td>\n",
       "      <td>-4.4577</td>\n",
       "      <td>9.5413</td>\n",
       "      <td>11.9052</td>\n",
       "      <td>2.1447</td>\n",
       "      <td>-22.4038</td>\n",
       "      <td>7.0883</td>\n",
       "      <td>14.1613</td>\n",
       "      <td>10.5080</td>\n",
       "      <td>14.2621</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>20.4031</td>\n",
       "      <td>17.0360</td>\n",
       "      <td>1.6981</td>\n",
       "      <td>-0.0269</td>\n",
       "      <td>-0.3939</td>\n",
       "      <td>12.6317</td>\n",
       "      <td>14.8863</td>\n",
       "      <td>1.3854</td>\n",
       "      <td>15.0284</td>\n",
       "      <td>3.9995</td>\n",
       "      <td>5.3683</td>\n",
       "      <td>8.6273</td>\n",
       "      <td>14.1963</td>\n",
       "      <td>20.3882</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>5.7033</td>\n",
       "      <td>4.5255</td>\n",
       "      <td>2.1929</td>\n",
       "      <td>3.1290</td>\n",
       "      <td>2.9044</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>28.7632</td>\n",
       "      <td>-17.2738</td>\n",
       "      <td>2.1056</td>\n",
       "      <td>21.1613</td>\n",
       "      <td>8.9573</td>\n",
       "      <td>2.7768</td>\n",
       "      <td>-2.1746</td>\n",
       "      <td>3.6932</td>\n",
       "      <td>12.4653</td>\n",
       "      <td>14.1978</td>\n",
       "      <td>-2.5511</td>\n",
       "      <td>-0.9479</td>\n",
       "      <td>17.1092</td>\n",
       "      <td>11.5419</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>8.8186</td>\n",
       "      <td>6.6231</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>-11.7218</td>\n",
       "      <td>24.5437</td>\n",
       "      <td>15.5827</td>\n",
       "      <td>3.8212</td>\n",
       "      <td>8.6674</td>\n",
       "      <td>7.3834</td>\n",
       "      <td>-2.4438</td>\n",
       "      <td>10.2158</td>\n",
       "      <td>7.4844</td>\n",
       "      <td>9.1104</td>\n",
       "      <td>4.3649</td>\n",
       "      <td>11.4934</td>\n",
       "      <td>1.7624</td>\n",
       "      <td>4.0714</td>\n",
       "      <td>-1.2681</td>\n",
       "      <td>14.3330</td>\n",
       "      <td>8.0088</td>\n",
       "      <td>4.4015</td>\n",
       "      <td>14.1479</td>\n",
       "      <td>-5.1747</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>14.5362</td>\n",
       "      <td>-1.7624</td>\n",
       "      <td>33.8820</td>\n",
       "      <td>11.6041</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>5.8442</td>\n",
       "      <td>4.7086</td>\n",
       "      <td>5.7141</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>20.5092</td>\n",
       "      <td>3.2790</td>\n",
       "      <td>-5.5952</td>\n",
       "      <td>7.3176</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>-7.0927</td>\n",
       "      <td>-3.9116</td>\n",
       "      <td>7.2569</td>\n",
       "      <td>-5.8234</td>\n",
       "      <td>25.6820</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-9.7009</td>\n",
       "      <td>2.4013</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>9.3908</td>\n",
       "      <td>-13.2648</td>\n",
       "      <td>3.1545</td>\n",
       "      <td>23.0866</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>5.3745</td>\n",
       "      <td>-6.2660</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-0.8417</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>9.1738</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>-3.8294</td>\n",
       "      <td>-1.0370</td>\n",
       "      <td>11.7770</td>\n",
       "      <td>11.2834</td>\n",
       "      <td>8.0485</td>\n",
       "      <td>-24.6840</td>\n",
       "      <td>12.7404</td>\n",
       "      <td>-35.1659</td>\n",
       "      <td>0.7613</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>12.6832</td>\n",
       "      <td>9.5503</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>5.2091</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>12.3972</td>\n",
       "      <td>14.4698</td>\n",
       "      <td>6.5850</td>\n",
       "      <td>3.3164</td>\n",
       "      <td>9.4638</td>\n",
       "      <td>15.7820</td>\n",
       "      <td>-25.0222</td>\n",
       "      <td>3.4418</td>\n",
       "      <td>-4.3923</td>\n",
       "      <td>8.6464</td>\n",
       "      <td>6.3072</td>\n",
       "      <td>5.6221</td>\n",
       "      <td>23.6143</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>-3.9989</td>\n",
       "      <td>4.0462</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>24.4187</td>\n",
       "      <td>4.5290</td>\n",
       "      <td>15.4235</td>\n",
       "      <td>11.6875</td>\n",
       "      <td>23.6273</td>\n",
       "      <td>4.0806</td>\n",
       "      <td>15.2733</td>\n",
       "      <td>0.7839</td>\n",
       "      <td>10.5404</td>\n",
       "      <td>1.6212</td>\n",
       "      <td>-5.2896</td>\n",
       "      <td>1.6027</td>\n",
       "      <td>17.9762</td>\n",
       "      <td>-2.3174</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>4.5474</td>\n",
       "      <td>7.5509</td>\n",
       "      <td>-7.5866</td>\n",
       "      <td>7.0364</td>\n",
       "      <td>14.4027</td>\n",
       "      <td>10.7795</td>\n",
       "      <td>7.2887</td>\n",
       "      <td>-1.0930</td>\n",
       "      <td>11.3596</td>\n",
       "      <td>18.1486</td>\n",
       "      <td>2.8344</td>\n",
       "      <td>1.9480</td>\n",
       "      <td>-19.8592</td>\n",
       "      <td>22.5316</td>\n",
       "      <td>18.6129</td>\n",
       "      <td>1.3512</td>\n",
       "      <td>9.3291</td>\n",
       "      <td>4.2835</td>\n",
       "      <td>10.3907</td>\n",
       "      <td>7.0874</td>\n",
       "      <td>14.3256</td>\n",
       "      <td>14.4135</td>\n",
       "      <td>4.2827</td>\n",
       "      <td>6.9750</td>\n",
       "      <td>1.6480</td>\n",
       "      <td>11.6896</td>\n",
       "      <td>2.5762</td>\n",
       "      <td>-2.5459</td>\n",
       "      <td>5.3446</td>\n",
       "      <td>38.1015</td>\n",
       "      <td>3.5732</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>30.5644</td>\n",
       "      <td>11.3025</td>\n",
       "      <td>3.9618</td>\n",
       "      <td>-8.2464</td>\n",
       "      <td>2.7038</td>\n",
       "      <td>12.3441</td>\n",
       "      <td>12.5431</td>\n",
       "      <td>-1.3683</td>\n",
       "      <td>3.5974</td>\n",
       "      <td>13.9761</td>\n",
       "      <td>14.3003</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>8.9500</td>\n",
       "      <td>7.1954</td>\n",
       "      <td>-1.1984</td>\n",
       "      <td>1.9586</td>\n",
       "      <td>27.5609</td>\n",
       "      <td>24.6065</td>\n",
       "      <td>-2.8233</td>\n",
       "      <td>8.9821</td>\n",
       "      <td>3.8873</td>\n",
       "      <td>15.9638</td>\n",
       "      <td>10.0142</td>\n",
       "      <td>7.8388</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>2.9253</td>\n",
       "      <td>10.4994</td>\n",
       "      <td>4.1622</td>\n",
       "      <td>3.7613</td>\n",
       "      <td>2.3701</td>\n",
       "      <td>18.0984</td>\n",
       "      <td>17.1765</td>\n",
       "      <td>7.6508</td>\n",
       "      <td>18.2452</td>\n",
       "      <td>17.0336</td>\n",
       "      <td>-10.9370</td>\n",
       "      <td>12.0500</td>\n",
       "      <td>-1.2155</td>\n",
       "      <td>19.9750</td>\n",
       "      <td>12.3892</td>\n",
       "      <td>31.8833</td>\n",
       "      <td>5.9684</td>\n",
       "      <td>7.2084</td>\n",
       "      <td>3.8899</td>\n",
       "      <td>-11.0882</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>2.5881</td>\n",
       "      <td>-2.7018</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>-7.1541</td>\n",
       "      <td>-6.1920</td>\n",
       "      <td>18.2366</td>\n",
       "      <td>11.7134</td>\n",
       "      <td>14.7483</td>\n",
       "      <td>8.1013</td>\n",
       "      <td>11.8771</td>\n",
       "      <td>13.9552</td>\n",
       "      <td>-10.4701</td>\n",
       "      <td>5.6961</td>\n",
       "      <td>-3.7546</td>\n",
       "      <td>8.4117</td>\n",
       "      <td>1.8986</td>\n",
       "      <td>7.2601</td>\n",
       "      <td>-0.4639</td>\n",
       "      <td>-0.0498</td>\n",
       "      <td>7.9336</td>\n",
       "      <td>-12.8279</td>\n",
       "      <td>12.4124</td>\n",
       "      <td>1.8489</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>19.3038</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>-5.5963</td>\n",
       "      <td>14.0685</td>\n",
       "      <td>11.9171</td>\n",
       "      <td>11.5111</td>\n",
       "      <td>6.9087</td>\n",
       "      <td>-65.4863</td>\n",
       "      <td>13.8657</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>-0.1346</td>\n",
       "      <td>14.4268</td>\n",
       "      <td>13.3273</td>\n",
       "      <td>10.4857</td>\n",
       "      <td>-1.4367</td>\n",
       "      <td>5.7555</td>\n",
       "      <td>-8.5414</td>\n",
       "      <td>14.1482</td>\n",
       "      <td>16.9840</td>\n",
       "      <td>6.1812</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>9.2048</td>\n",
       "      <td>8.6591</td>\n",
       "      <td>-27.7439</td>\n",
       "      <td>-0.4952</td>\n",
       "      <td>-1.7839</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>-4.3205</td>\n",
       "      <td>6.9860</td>\n",
       "      <td>1.6184</td>\n",
       "      <td>5.0301</td>\n",
       "      <td>-3.2431</td>\n",
       "      <td>40.1236</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>-0.7264</td>\n",
       "      <td>4.5886</td>\n",
       "      <td>-4.5346</td>\n",
       "      <td>23.3521</td>\n",
       "      <td>1.0273</td>\n",
       "      <td>19.1600</td>\n",
       "      <td>7.1734</td>\n",
       "      <td>14.3937</td>\n",
       "      <td>2.9598</td>\n",
       "      <td>13.3317</td>\n",
       "      <td>-9.2587</td>\n",
       "      <td>-6.7075</td>\n",
       "      <td>7.8984</td>\n",
       "      <td>14.5265</td>\n",
       "      <td>7.0799</td>\n",
       "      <td>20.1670</td>\n",
       "      <td>8.0053</td>\n",
       "      <td>3.7954</td>\n",
       "      <td>-39.7997</td>\n",
       "      <td>7.0065</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>14.0553</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>14.7246</td>\n",
       "      <td>35.2988</td>\n",
       "      <td>1.6844</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>-22.9264</td>\n",
       "      <td>12.3562</td>\n",
       "      <td>17.3410</td>\n",
       "      <td>1.6940</td>\n",
       "      <td>7.1179</td>\n",
       "      <td>5.1934</td>\n",
       "      <td>8.8230</td>\n",
       "      <td>10.6617</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>28.2749</td>\n",
       "      <td>-0.1937</td>\n",
       "      <td>5.9654</td>\n",
       "      <td>1.0719</td>\n",
       "      <td>7.9923</td>\n",
       "      <td>2.9138</td>\n",
       "      <td>-3.6135</td>\n",
       "      <td>1.4684</td>\n",
       "      <td>25.6795</td>\n",
       "      <td>13.8224</td>\n",
       "      <td>4.7478</td>\n",
       "      <td>41.1037</td>\n",
       "      <td>12.7140</td>\n",
       "      <td>5.2964</td>\n",
       "      <td>9.7289</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>12.1316</td>\n",
       "      <td>12.5815</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>5.6518</td>\n",
       "      <td>10.9346</td>\n",
       "      <td>11.4266</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>7.7532</td>\n",
       "      <td>6.6173</td>\n",
       "      <td>-6.8304</td>\n",
       "      <td>6.4730</td>\n",
       "      <td>17.1728</td>\n",
       "      <td>25.8128</td>\n",
       "      <td>2.6791</td>\n",
       "      <td>13.9547</td>\n",
       "      <td>6.6289</td>\n",
       "      <td>-4.3965</td>\n",
       "      <td>11.7159</td>\n",
       "      <td>16.1080</td>\n",
       "      <td>7.6874</td>\n",
       "      <td>9.1570</td>\n",
       "      <td>11.5670</td>\n",
       "      <td>-12.7047</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>9.9110</td>\n",
       "      <td>20.1461</td>\n",
       "      <td>1.2995</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>19.8234</td>\n",
       "      <td>4.7022</td>\n",
       "      <td>10.6101</td>\n",
       "      <td>13.0021</td>\n",
       "      <td>-12.6068</td>\n",
       "      <td>27.0846</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>33.5107</td>\n",
       "      <td>5.6953</td>\n",
       "      <td>5.4663</td>\n",
       "      <td>18.2201</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>21.2607</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>-1.7759</td>\n",
       "      <td>3.1283</td>\n",
       "      <td>5.5518</td>\n",
       "      <td>1.4493</td>\n",
       "      <td>-2.6627</td>\n",
       "      <td>19.8056</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>-3.3456</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>1.7189</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>9.7685</td>\n",
       "      <td>4.8910</td>\n",
       "      <td>12.2198</td>\n",
       "      <td>11.8503</td>\n",
       "      <td>-7.8931</td>\n",
       "      <td>6.4209</td>\n",
       "      <td>5.9270</td>\n",
       "      <td>16.0201</td>\n",
       "      <td>-0.2829</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>0</td>\n",
       "      <td>12.4904</td>\n",
       "      <td>4.0971</td>\n",
       "      <td>9.5729</td>\n",
       "      <td>7.1030</td>\n",
       "      <td>12.2411</td>\n",
       "      <td>-3.8714</td>\n",
       "      <td>5.3379</td>\n",
       "      <td>13.5599</td>\n",
       "      <td>-2.5057</td>\n",
       "      <td>6.5893</td>\n",
       "      <td>4.4358</td>\n",
       "      <td>-4.1894</td>\n",
       "      <td>13.9236</td>\n",
       "      <td>14.2479</td>\n",
       "      <td>9.2883</td>\n",
       "      <td>14.9302</td>\n",
       "      <td>9.6972</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>7.6976</td>\n",
       "      <td>17.0411</td>\n",
       "      <td>9.4789</td>\n",
       "      <td>9.0912</td>\n",
       "      <td>6.6730</td>\n",
       "      <td>2.7345</td>\n",
       "      <td>7.9862</td>\n",
       "      <td>13.7540</td>\n",
       "      <td>-13.0887</td>\n",
       "      <td>-3.1314</td>\n",
       "      <td>5.4377</td>\n",
       "      <td>6.7783</td>\n",
       "      <td>-10.7636</td>\n",
       "      <td>12.6498</td>\n",
       "      <td>0.8116</td>\n",
       "      <td>22.1980</td>\n",
       "      <td>11.4345</td>\n",
       "      <td>8.5225</td>\n",
       "      <td>0.0637</td>\n",
       "      <td>8.3442</td>\n",
       "      <td>9.6516</td>\n",
       "      <td>5.1408</td>\n",
       "      <td>-16.9031</td>\n",
       "      <td>14.7474</td>\n",
       "      <td>10.8899</td>\n",
       "      <td>10.8464</td>\n",
       "      <td>5.7639</td>\n",
       "      <td>21.4889</td>\n",
       "      <td>8.0400</td>\n",
       "      <td>-11.2252</td>\n",
       "      <td>15.7474</td>\n",
       "      <td>31.3351</td>\n",
       "      <td>13.8472</td>\n",
       "      <td>0.4772</td>\n",
       "      <td>-5.5649</td>\n",
       "      <td>6.5326</td>\n",
       "      <td>-8.2557</td>\n",
       "      <td>14.5900</td>\n",
       "      <td>17.2057</td>\n",
       "      <td>7.0540</td>\n",
       "      <td>1.0443</td>\n",
       "      <td>10.1205</td>\n",
       "      <td>10.3625</td>\n",
       "      <td>0.6524</td>\n",
       "      <td>2.4346</td>\n",
       "      <td>-1.7746</td>\n",
       "      <td>6.0212</td>\n",
       "      <td>2.7454</td>\n",
       "      <td>4.7968</td>\n",
       "      <td>4.3666</td>\n",
       "      <td>5.0092</td>\n",
       "      <td>-9.8259</td>\n",
       "      <td>16.0449</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>-1.7623</td>\n",
       "      <td>15.2569</td>\n",
       "      <td>23.0787</td>\n",
       "      <td>20.9913</td>\n",
       "      <td>-0.0052</td>\n",
       "      <td>15.3383</td>\n",
       "      <td>3.0695</td>\n",
       "      <td>13.5407</td>\n",
       "      <td>10.8407</td>\n",
       "      <td>19.1084</td>\n",
       "      <td>-3.0142</td>\n",
       "      <td>-2.1188</td>\n",
       "      <td>1.4255</td>\n",
       "      <td>16.1721</td>\n",
       "      <td>15.3741</td>\n",
       "      <td>13.3447</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>0.4362</td>\n",
       "      <td>-8.9597</td>\n",
       "      <td>7.1372</td>\n",
       "      <td>11.9309</td>\n",
       "      <td>10.6130</td>\n",
       "      <td>16.3536</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>21.5935</td>\n",
       "      <td>42.3055</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>-0.1292</td>\n",
       "      <td>-9.2950</td>\n",
       "      <td>14.3201</td>\n",
       "      <td>36.7479</td>\n",
       "      <td>1.6059</td>\n",
       "      <td>9.6548</td>\n",
       "      <td>5.1387</td>\n",
       "      <td>8.1292</td>\n",
       "      <td>16.3701</td>\n",
       "      <td>14.2756</td>\n",
       "      <td>19.7277</td>\n",
       "      <td>1.7365</td>\n",
       "      <td>7.2307</td>\n",
       "      <td>3.2307</td>\n",
       "      <td>12.3384</td>\n",
       "      <td>4.4210</td>\n",
       "      <td>4.2612</td>\n",
       "      <td>3.5299</td>\n",
       "      <td>22.4758</td>\n",
       "      <td>-14.5210</td>\n",
       "      <td>-0.6718</td>\n",
       "      <td>28.8213</td>\n",
       "      <td>11.1212</td>\n",
       "      <td>-0.3664</td>\n",
       "      <td>12.6570</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>12.7450</td>\n",
       "      <td>12.6463</td>\n",
       "      <td>-0.3941</td>\n",
       "      <td>-3.5594</td>\n",
       "      <td>18.6261</td>\n",
       "      <td>11.1020</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>9.8752</td>\n",
       "      <td>6.6927</td>\n",
       "      <td>-11.8125</td>\n",
       "      <td>-8.3717</td>\n",
       "      <td>26.3973</td>\n",
       "      <td>11.7138</td>\n",
       "      <td>-0.5914</td>\n",
       "      <td>9.3315</td>\n",
       "      <td>-2.3878</td>\n",
       "      <td>1.8984</td>\n",
       "      <td>5.8921</td>\n",
       "      <td>8.3873</td>\n",
       "      <td>7.6926</td>\n",
       "      <td>1.1354</td>\n",
       "      <td>8.9153</td>\n",
       "      <td>-7.3375</td>\n",
       "      <td>3.8037</td>\n",
       "      <td>-7.5865</td>\n",
       "      <td>19.0708</td>\n",
       "      <td>13.6887</td>\n",
       "      <td>7.0208</td>\n",
       "      <td>16.2498</td>\n",
       "      <td>10.5968</td>\n",
       "      <td>12.5921</td>\n",
       "      <td>13.3929</td>\n",
       "      <td>-13.8908</td>\n",
       "      <td>11.8437</td>\n",
       "      <td>11.2875</td>\n",
       "      <td>20.0500</td>\n",
       "      <td>5.8561</td>\n",
       "      <td>4.1490</td>\n",
       "      <td>11.6025</td>\n",
       "      <td>1.9091</td>\n",
       "      <td>18.4879</td>\n",
       "      <td>2.9838</td>\n",
       "      <td>-5.4682</td>\n",
       "      <td>4.4443</td>\n",
       "      <td>5.8550</td>\n",
       "      <td>4.3673</td>\n",
       "      <td>-3.1908</td>\n",
       "      <td>12.1283</td>\n",
       "      <td>2.3392</td>\n",
       "      <td>22.8965</td>\n",
       "      <td>13.7958</td>\n",
       "      <td>2.9110</td>\n",
       "      <td>11.7910</td>\n",
       "      <td>-8.3711</td>\n",
       "      <td>-0.9903</td>\n",
       "      <td>-8.7420</td>\n",
       "      <td>8.3954</td>\n",
       "      <td>6.7660</td>\n",
       "      <td>16.2471</td>\n",
       "      <td>14.3125</td>\n",
       "      <td>-4.0452</td>\n",
       "      <td>8.6593</td>\n",
       "      <td>-20.7593</td>\n",
       "      <td>15.1800</td>\n",
       "      <td>2.4887</td>\n",
       "      <td>11.4350</td>\n",
       "      <td>7.5056</td>\n",
       "      <td>0.6534</td>\n",
       "      <td>5.0310</td>\n",
       "      <td>19.6522</td>\n",
       "      <td>0.2626</td>\n",
       "      <td>-6.8085</td>\n",
       "      <td>8.2127</td>\n",
       "      <td>21.1710</td>\n",
       "      <td>-20.4727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>0</td>\n",
       "      <td>12.0756</td>\n",
       "      <td>0.6536</td>\n",
       "      <td>14.9369</td>\n",
       "      <td>5.3230</td>\n",
       "      <td>7.8097</td>\n",
       "      <td>2.3978</td>\n",
       "      <td>5.5501</td>\n",
       "      <td>23.0557</td>\n",
       "      <td>-4.4645</td>\n",
       "      <td>7.9690</td>\n",
       "      <td>3.1193</td>\n",
       "      <td>-1.5357</td>\n",
       "      <td>13.9197</td>\n",
       "      <td>9.7544</td>\n",
       "      <td>10.2130</td>\n",
       "      <td>14.6041</td>\n",
       "      <td>12.6836</td>\n",
       "      <td>-5.5208</td>\n",
       "      <td>4.5473</td>\n",
       "      <td>19.0755</td>\n",
       "      <td>24.6041</td>\n",
       "      <td>14.9972</td>\n",
       "      <td>5.8234</td>\n",
       "      <td>3.5070</td>\n",
       "      <td>9.9510</td>\n",
       "      <td>13.8415</td>\n",
       "      <td>-8.9524</td>\n",
       "      <td>-1.3360</td>\n",
       "      <td>4.4207</td>\n",
       "      <td>4.4859</td>\n",
       "      <td>-16.4447</td>\n",
       "      <td>9.8363</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>18.9750</td>\n",
       "      <td>12.4266</td>\n",
       "      <td>-0.7335</td>\n",
       "      <td>2.0133</td>\n",
       "      <td>6.0986</td>\n",
       "      <td>9.3941</td>\n",
       "      <td>-6.2155</td>\n",
       "      <td>-9.6860</td>\n",
       "      <td>16.2583</td>\n",
       "      <td>11.0130</td>\n",
       "      <td>11.5700</td>\n",
       "      <td>10.4327</td>\n",
       "      <td>-48.8558</td>\n",
       "      <td>13.8643</td>\n",
       "      <td>-11.1942</td>\n",
       "      <td>15.8437</td>\n",
       "      <td>15.1464</td>\n",
       "      <td>13.3754</td>\n",
       "      <td>1.5810</td>\n",
       "      <td>0.9192</td>\n",
       "      <td>6.5946</td>\n",
       "      <td>3.6180</td>\n",
       "      <td>17.3741</td>\n",
       "      <td>18.7799</td>\n",
       "      <td>6.2369</td>\n",
       "      <td>3.8269</td>\n",
       "      <td>8.4710</td>\n",
       "      <td>7.8216</td>\n",
       "      <td>-17.6335</td>\n",
       "      <td>-0.2381</td>\n",
       "      <td>-1.1925</td>\n",
       "      <td>5.4755</td>\n",
       "      <td>0.6752</td>\n",
       "      <td>5.2577</td>\n",
       "      <td>13.4306</td>\n",
       "      <td>5.0252</td>\n",
       "      <td>-0.5491</td>\n",
       "      <td>33.0498</td>\n",
       "      <td>0.6976</td>\n",
       "      <td>2.8715</td>\n",
       "      <td>6.3629</td>\n",
       "      <td>15.0014</td>\n",
       "      <td>5.6505</td>\n",
       "      <td>5.9231</td>\n",
       "      <td>14.8444</td>\n",
       "      <td>5.8406</td>\n",
       "      <td>13.9866</td>\n",
       "      <td>-5.2695</td>\n",
       "      <td>16.9736</td>\n",
       "      <td>-5.0933</td>\n",
       "      <td>-13.3969</td>\n",
       "      <td>0.2658</td>\n",
       "      <td>19.7691</td>\n",
       "      <td>-13.7875</td>\n",
       "      <td>14.6716</td>\n",
       "      <td>10.0514</td>\n",
       "      <td>0.6721</td>\n",
       "      <td>-18.5091</td>\n",
       "      <td>7.1056</td>\n",
       "      <td>18.0088</td>\n",
       "      <td>10.8887</td>\n",
       "      <td>10.9388</td>\n",
       "      <td>0.1835</td>\n",
       "      <td>7.0454</td>\n",
       "      <td>25.8421</td>\n",
       "      <td>2.7307</td>\n",
       "      <td>-1.4396</td>\n",
       "      <td>-5.4111</td>\n",
       "      <td>22.3420</td>\n",
       "      <td>4.9562</td>\n",
       "      <td>1.7907</td>\n",
       "      <td>13.3697</td>\n",
       "      <td>2.6089</td>\n",
       "      <td>9.4761</td>\n",
       "      <td>15.0610</td>\n",
       "      <td>14.4500</td>\n",
       "      <td>23.0233</td>\n",
       "      <td>7.2020</td>\n",
       "      <td>5.7471</td>\n",
       "      <td>3.5116</td>\n",
       "      <td>10.2647</td>\n",
       "      <td>3.2930</td>\n",
       "      <td>3.0700</td>\n",
       "      <td>3.1054</td>\n",
       "      <td>23.2861</td>\n",
       "      <td>-11.1304</td>\n",
       "      <td>3.0325</td>\n",
       "      <td>32.9108</td>\n",
       "      <td>12.9628</td>\n",
       "      <td>-5.5922</td>\n",
       "      <td>3.6516</td>\n",
       "      <td>4.8402</td>\n",
       "      <td>12.8812</td>\n",
       "      <td>12.8668</td>\n",
       "      <td>3.4692</td>\n",
       "      <td>-5.4893</td>\n",
       "      <td>15.9122</td>\n",
       "      <td>12.4401</td>\n",
       "      <td>1.3717</td>\n",
       "      <td>6.5303</td>\n",
       "      <td>6.5312</td>\n",
       "      <td>2.1923</td>\n",
       "      <td>2.9965</td>\n",
       "      <td>13.5914</td>\n",
       "      <td>30.7308</td>\n",
       "      <td>-0.3007</td>\n",
       "      <td>11.1424</td>\n",
       "      <td>8.6554</td>\n",
       "      <td>-1.3220</td>\n",
       "      <td>8.2257</td>\n",
       "      <td>12.8353</td>\n",
       "      <td>8.9400</td>\n",
       "      <td>13.3364</td>\n",
       "      <td>11.7449</td>\n",
       "      <td>-0.4871</td>\n",
       "      <td>3.7961</td>\n",
       "      <td>-6.4872</td>\n",
       "      <td>15.4872</td>\n",
       "      <td>17.3317</td>\n",
       "      <td>9.6841</td>\n",
       "      <td>19.7184</td>\n",
       "      <td>-0.3729</td>\n",
       "      <td>2.9087</td>\n",
       "      <td>12.5196</td>\n",
       "      <td>-4.0016</td>\n",
       "      <td>20.3712</td>\n",
       "      <td>9.0545</td>\n",
       "      <td>37.2184</td>\n",
       "      <td>5.7610</td>\n",
       "      <td>3.3126</td>\n",
       "      <td>4.1035</td>\n",
       "      <td>2.9809</td>\n",
       "      <td>25.4507</td>\n",
       "      <td>2.9142</td>\n",
       "      <td>7.0334</td>\n",
       "      <td>7.1331</td>\n",
       "      <td>6.0174</td>\n",
       "      <td>6.0527</td>\n",
       "      <td>3.2659</td>\n",
       "      <td>18.1972</td>\n",
       "      <td>3.4801</td>\n",
       "      <td>26.0080</td>\n",
       "      <td>9.2702</td>\n",
       "      <td>0.1574</td>\n",
       "      <td>13.3378</td>\n",
       "      <td>7.9268</td>\n",
       "      <td>4.2928</td>\n",
       "      <td>2.1362</td>\n",
       "      <td>8.9557</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>3.5912</td>\n",
       "      <td>14.5755</td>\n",
       "      <td>-3.4588</td>\n",
       "      <td>10.1281</td>\n",
       "      <td>-29.7886</td>\n",
       "      <td>20.5672</td>\n",
       "      <td>-0.8228</td>\n",
       "      <td>0.3689</td>\n",
       "      <td>8.3071</td>\n",
       "      <td>1.4771</td>\n",
       "      <td>5.1579</td>\n",
       "      <td>18.0875</td>\n",
       "      <td>-1.6203</td>\n",
       "      <td>2.5211</td>\n",
       "      <td>9.1977</td>\n",
       "      <td>18.1966</td>\n",
       "      <td>-18.0426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>0</td>\n",
       "      <td>6.4827</td>\n",
       "      <td>-0.1254</td>\n",
       "      <td>9.8742</td>\n",
       "      <td>6.5121</td>\n",
       "      <td>12.8357</td>\n",
       "      <td>4.7679</td>\n",
       "      <td>6.9999</td>\n",
       "      <td>14.6801</td>\n",
       "      <td>1.2815</td>\n",
       "      <td>7.2162</td>\n",
       "      <td>2.0656</td>\n",
       "      <td>-5.0301</td>\n",
       "      <td>14.0485</td>\n",
       "      <td>6.7701</td>\n",
       "      <td>10.6114</td>\n",
       "      <td>14.7954</td>\n",
       "      <td>8.8016</td>\n",
       "      <td>-1.0517</td>\n",
       "      <td>18.9271</td>\n",
       "      <td>20.3470</td>\n",
       "      <td>22.9514</td>\n",
       "      <td>26.8826</td>\n",
       "      <td>5.7218</td>\n",
       "      <td>2.1176</td>\n",
       "      <td>13.1784</td>\n",
       "      <td>13.9020</td>\n",
       "      <td>-1.8656</td>\n",
       "      <td>-2.7829</td>\n",
       "      <td>5.9136</td>\n",
       "      <td>3.7484</td>\n",
       "      <td>-15.9028</td>\n",
       "      <td>8.0342</td>\n",
       "      <td>-5.0323</td>\n",
       "      <td>11.7231</td>\n",
       "      <td>11.2059</td>\n",
       "      <td>9.3478</td>\n",
       "      <td>3.4257</td>\n",
       "      <td>5.7810</td>\n",
       "      <td>9.0962</td>\n",
       "      <td>6.6659</td>\n",
       "      <td>-4.5044</td>\n",
       "      <td>-1.5156</td>\n",
       "      <td>10.4338</td>\n",
       "      <td>11.2274</td>\n",
       "      <td>11.0851</td>\n",
       "      <td>25.6511</td>\n",
       "      <td>7.3391</td>\n",
       "      <td>-19.6370</td>\n",
       "      <td>20.4729</td>\n",
       "      <td>25.0191</td>\n",
       "      <td>13.0447</td>\n",
       "      <td>13.0015</td>\n",
       "      <td>2.1116</td>\n",
       "      <td>6.8156</td>\n",
       "      <td>16.5393</td>\n",
       "      <td>6.7834</td>\n",
       "      <td>16.5501</td>\n",
       "      <td>7.0152</td>\n",
       "      <td>6.6720</td>\n",
       "      <td>8.6135</td>\n",
       "      <td>15.5433</td>\n",
       "      <td>-19.6367</td>\n",
       "      <td>2.1462</td>\n",
       "      <td>1.8310</td>\n",
       "      <td>4.1149</td>\n",
       "      <td>-7.3638</td>\n",
       "      <td>5.8404</td>\n",
       "      <td>5.2266</td>\n",
       "      <td>5.0183</td>\n",
       "      <td>-0.8607</td>\n",
       "      <td>19.4773</td>\n",
       "      <td>0.9079</td>\n",
       "      <td>-3.6426</td>\n",
       "      <td>21.6163</td>\n",
       "      <td>23.0934</td>\n",
       "      <td>23.3446</td>\n",
       "      <td>6.5704</td>\n",
       "      <td>21.7256</td>\n",
       "      <td>7.5744</td>\n",
       "      <td>12.4480</td>\n",
       "      <td>1.0592</td>\n",
       "      <td>16.5670</td>\n",
       "      <td>14.2049</td>\n",
       "      <td>3.0032</td>\n",
       "      <td>-7.9250</td>\n",
       "      <td>21.4960</td>\n",
       "      <td>11.9197</td>\n",
       "      <td>12.5635</td>\n",
       "      <td>7.8239</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>-16.4258</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>12.1038</td>\n",
       "      <td>11.4935</td>\n",
       "      <td>8.8204</td>\n",
       "      <td>-0.5129</td>\n",
       "      <td>22.1831</td>\n",
       "      <td>15.6572</td>\n",
       "      <td>1.4812</td>\n",
       "      <td>1.4177</td>\n",
       "      <td>-3.7730</td>\n",
       "      <td>14.4499</td>\n",
       "      <td>11.2579</td>\n",
       "      <td>1.2567</td>\n",
       "      <td>10.1182</td>\n",
       "      <td>3.8538</td>\n",
       "      <td>8.7105</td>\n",
       "      <td>18.4011</td>\n",
       "      <td>13.9360</td>\n",
       "      <td>13.9145</td>\n",
       "      <td>0.3429</td>\n",
       "      <td>6.3459</td>\n",
       "      <td>2.4649</td>\n",
       "      <td>11.1353</td>\n",
       "      <td>4.2721</td>\n",
       "      <td>-2.0414</td>\n",
       "      <td>2.4057</td>\n",
       "      <td>17.0167</td>\n",
       "      <td>-12.0394</td>\n",
       "      <td>7.0021</td>\n",
       "      <td>40.1013</td>\n",
       "      <td>14.7136</td>\n",
       "      <td>2.9664</td>\n",
       "      <td>13.1119</td>\n",
       "      <td>5.0863</td>\n",
       "      <td>12.9621</td>\n",
       "      <td>12.4400</td>\n",
       "      <td>-0.3585</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>21.0114</td>\n",
       "      <td>12.2539</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>10.8232</td>\n",
       "      <td>6.7557</td>\n",
       "      <td>-3.4472</td>\n",
       "      <td>4.9295</td>\n",
       "      <td>14.9176</td>\n",
       "      <td>11.6466</td>\n",
       "      <td>-6.1010</td>\n",
       "      <td>12.5493</td>\n",
       "      <td>10.9734</td>\n",
       "      <td>3.6920</td>\n",
       "      <td>8.4625</td>\n",
       "      <td>11.3665</td>\n",
       "      <td>8.2130</td>\n",
       "      <td>4.8325</td>\n",
       "      <td>10.0756</td>\n",
       "      <td>-19.9297</td>\n",
       "      <td>3.9025</td>\n",
       "      <td>8.0388</td>\n",
       "      <td>15.9884</td>\n",
       "      <td>11.2205</td>\n",
       "      <td>6.7047</td>\n",
       "      <td>17.3389</td>\n",
       "      <td>10.5394</td>\n",
       "      <td>-1.8955</td>\n",
       "      <td>12.0981</td>\n",
       "      <td>3.8825</td>\n",
       "      <td>34.6457</td>\n",
       "      <td>9.1157</td>\n",
       "      <td>20.3925</td>\n",
       "      <td>5.6120</td>\n",
       "      <td>5.3182</td>\n",
       "      <td>19.0850</td>\n",
       "      <td>-2.7405</td>\n",
       "      <td>19.5322</td>\n",
       "      <td>2.5623</td>\n",
       "      <td>-14.6855</td>\n",
       "      <td>5.5657</td>\n",
       "      <td>5.2742</td>\n",
       "      <td>-7.3142</td>\n",
       "      <td>7.6154</td>\n",
       "      <td>12.7750</td>\n",
       "      <td>0.3091</td>\n",
       "      <td>3.9351</td>\n",
       "      <td>14.6798</td>\n",
       "      <td>-8.9574</td>\n",
       "      <td>14.1055</td>\n",
       "      <td>7.5467</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>3.5579</td>\n",
       "      <td>10.6485</td>\n",
       "      <td>-12.1161</td>\n",
       "      <td>16.5248</td>\n",
       "      <td>26.2340</td>\n",
       "      <td>-11.3249</td>\n",
       "      <td>13.2318</td>\n",
       "      <td>5.8970</td>\n",
       "      <td>14.0280</td>\n",
       "      <td>-1.0962</td>\n",
       "      <td>1.2615</td>\n",
       "      <td>8.5350</td>\n",
       "      <td>2.2625</td>\n",
       "      <td>6.9436</td>\n",
       "      <td>22.4766</td>\n",
       "      <td>2.1484</td>\n",
       "      <td>9.3593</td>\n",
       "      <td>6.8472</td>\n",
       "      <td>13.0793</td>\n",
       "      <td>4.4414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>0</td>\n",
       "      <td>6.8704</td>\n",
       "      <td>-0.7463</td>\n",
       "      <td>12.1506</td>\n",
       "      <td>9.4347</td>\n",
       "      <td>10.8363</td>\n",
       "      <td>-12.1858</td>\n",
       "      <td>4.7211</td>\n",
       "      <td>24.1893</td>\n",
       "      <td>1.2180</td>\n",
       "      <td>5.3363</td>\n",
       "      <td>-3.7784</td>\n",
       "      <td>-10.7418</td>\n",
       "      <td>14.1667</td>\n",
       "      <td>8.3164</td>\n",
       "      <td>6.9363</td>\n",
       "      <td>14.2238</td>\n",
       "      <td>10.2787</td>\n",
       "      <td>-7.9082</td>\n",
       "      <td>29.4910</td>\n",
       "      <td>16.4082</td>\n",
       "      <td>17.6172</td>\n",
       "      <td>21.3965</td>\n",
       "      <td>3.9215</td>\n",
       "      <td>2.6106</td>\n",
       "      <td>9.1471</td>\n",
       "      <td>13.4345</td>\n",
       "      <td>-2.5250</td>\n",
       "      <td>-0.5275</td>\n",
       "      <td>4.0404</td>\n",
       "      <td>10.3723</td>\n",
       "      <td>-19.9593</td>\n",
       "      <td>7.6379</td>\n",
       "      <td>-4.6488</td>\n",
       "      <td>16.2245</td>\n",
       "      <td>10.6183</td>\n",
       "      <td>-2.0612</td>\n",
       "      <td>6.6444</td>\n",
       "      <td>4.4462</td>\n",
       "      <td>13.2900</td>\n",
       "      <td>-3.9788</td>\n",
       "      <td>-18.5511</td>\n",
       "      <td>-2.3109</td>\n",
       "      <td>10.5114</td>\n",
       "      <td>12.0860</td>\n",
       "      <td>14.5412</td>\n",
       "      <td>7.6649</td>\n",
       "      <td>10.6118</td>\n",
       "      <td>1.0478</td>\n",
       "      <td>2.4986</td>\n",
       "      <td>24.9719</td>\n",
       "      <td>12.0302</td>\n",
       "      <td>4.0296</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>5.9675</td>\n",
       "      <td>-15.7085</td>\n",
       "      <td>3.6506</td>\n",
       "      <td>16.2006</td>\n",
       "      <td>6.2206</td>\n",
       "      <td>1.9487</td>\n",
       "      <td>9.3900</td>\n",
       "      <td>17.3474</td>\n",
       "      <td>-1.3388</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>-1.9230</td>\n",
       "      <td>8.6771</td>\n",
       "      <td>-4.9034</td>\n",
       "      <td>7.0388</td>\n",
       "      <td>10.5973</td>\n",
       "      <td>5.0177</td>\n",
       "      <td>-3.5720</td>\n",
       "      <td>23.3534</td>\n",
       "      <td>0.2777</td>\n",
       "      <td>-2.6334</td>\n",
       "      <td>27.3166</td>\n",
       "      <td>30.3428</td>\n",
       "      <td>10.7064</td>\n",
       "      <td>5.8044</td>\n",
       "      <td>21.0938</td>\n",
       "      <td>6.0646</td>\n",
       "      <td>13.8305</td>\n",
       "      <td>8.8740</td>\n",
       "      <td>16.8102</td>\n",
       "      <td>-5.9580</td>\n",
       "      <td>7.7356</td>\n",
       "      <td>3.6891</td>\n",
       "      <td>17.2579</td>\n",
       "      <td>8.2796</td>\n",
       "      <td>10.9035</td>\n",
       "      <td>7.7676</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>-9.9755</td>\n",
       "      <td>7.2324</td>\n",
       "      <td>6.0425</td>\n",
       "      <td>9.4391</td>\n",
       "      <td>10.2446</td>\n",
       "      <td>-0.2816</td>\n",
       "      <td>33.5611</td>\n",
       "      <td>29.8170</td>\n",
       "      <td>0.1766</td>\n",
       "      <td>-2.8194</td>\n",
       "      <td>-3.5142</td>\n",
       "      <td>14.3217</td>\n",
       "      <td>37.2495</td>\n",
       "      <td>1.7883</td>\n",
       "      <td>11.5307</td>\n",
       "      <td>3.8483</td>\n",
       "      <td>6.3715</td>\n",
       "      <td>11.8986</td>\n",
       "      <td>14.1428</td>\n",
       "      <td>11.2411</td>\n",
       "      <td>6.6548</td>\n",
       "      <td>4.4822</td>\n",
       "      <td>3.2330</td>\n",
       "      <td>17.0146</td>\n",
       "      <td>3.9640</td>\n",
       "      <td>3.2305</td>\n",
       "      <td>-1.4797</td>\n",
       "      <td>4.2941</td>\n",
       "      <td>-5.5423</td>\n",
       "      <td>4.3318</td>\n",
       "      <td>21.3508</td>\n",
       "      <td>9.3747</td>\n",
       "      <td>-5.7679</td>\n",
       "      <td>0.4538</td>\n",
       "      <td>5.7097</td>\n",
       "      <td>12.1853</td>\n",
       "      <td>11.6951</td>\n",
       "      <td>-3.4794</td>\n",
       "      <td>-0.1509</td>\n",
       "      <td>17.0766</td>\n",
       "      <td>12.9071</td>\n",
       "      <td>0.6502</td>\n",
       "      <td>8.3375</td>\n",
       "      <td>7.5104</td>\n",
       "      <td>-10.4414</td>\n",
       "      <td>3.7784</td>\n",
       "      <td>4.0272</td>\n",
       "      <td>20.2614</td>\n",
       "      <td>0.8237</td>\n",
       "      <td>-6.6060</td>\n",
       "      <td>1.3595</td>\n",
       "      <td>-6.5414</td>\n",
       "      <td>12.2554</td>\n",
       "      <td>15.4690</td>\n",
       "      <td>9.7661</td>\n",
       "      <td>0.4663</td>\n",
       "      <td>10.9107</td>\n",
       "      <td>-6.4811</td>\n",
       "      <td>3.9032</td>\n",
       "      <td>18.4450</td>\n",
       "      <td>15.2502</td>\n",
       "      <td>9.4846</td>\n",
       "      <td>9.6340</td>\n",
       "      <td>13.5600</td>\n",
       "      <td>13.7344</td>\n",
       "      <td>-8.6708</td>\n",
       "      <td>12.8264</td>\n",
       "      <td>-8.0187</td>\n",
       "      <td>22.8800</td>\n",
       "      <td>6.1382</td>\n",
       "      <td>30.2004</td>\n",
       "      <td>5.5373</td>\n",
       "      <td>2.4294</td>\n",
       "      <td>17.4894</td>\n",
       "      <td>-2.1165</td>\n",
       "      <td>13.7832</td>\n",
       "      <td>2.8288</td>\n",
       "      <td>-1.7876</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>5.0457</td>\n",
       "      <td>3.5383</td>\n",
       "      <td>8.8912</td>\n",
       "      <td>18.4672</td>\n",
       "      <td>1.8273</td>\n",
       "      <td>30.2822</td>\n",
       "      <td>9.4743</td>\n",
       "      <td>5.3983</td>\n",
       "      <td>10.1313</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>3.9345</td>\n",
       "      <td>-10.4854</td>\n",
       "      <td>8.7874</td>\n",
       "      <td>3.4940</td>\n",
       "      <td>17.0222</td>\n",
       "      <td>22.0108</td>\n",
       "      <td>-0.1467</td>\n",
       "      <td>10.5577</td>\n",
       "      <td>-1.8767</td>\n",
       "      <td>16.6669</td>\n",
       "      <td>2.2175</td>\n",
       "      <td>2.8475</td>\n",
       "      <td>10.6147</td>\n",
       "      <td>3.0183</td>\n",
       "      <td>2.5069</td>\n",
       "      <td>16.0805</td>\n",
       "      <td>0.4006</td>\n",
       "      <td>-5.5772</td>\n",
       "      <td>8.4289</td>\n",
       "      <td>20.8129</td>\n",
       "      <td>-18.4453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>0</td>\n",
       "      <td>12.8574</td>\n",
       "      <td>2.0760</td>\n",
       "      <td>13.7639</td>\n",
       "      <td>6.1902</td>\n",
       "      <td>10.2858</td>\n",
       "      <td>-17.0299</td>\n",
       "      <td>4.2630</td>\n",
       "      <td>11.8193</td>\n",
       "      <td>1.7253</td>\n",
       "      <td>5.5037</td>\n",
       "      <td>-8.3439</td>\n",
       "      <td>6.4556</td>\n",
       "      <td>13.9008</td>\n",
       "      <td>10.1658</td>\n",
       "      <td>4.7202</td>\n",
       "      <td>14.4317</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>4.3089</td>\n",
       "      <td>20.9777</td>\n",
       "      <td>30.1888</td>\n",
       "      <td>8.7158</td>\n",
       "      <td>23.8110</td>\n",
       "      <td>9.6532</td>\n",
       "      <td>2.8999</td>\n",
       "      <td>16.3957</td>\n",
       "      <td>13.9663</td>\n",
       "      <td>-5.7937</td>\n",
       "      <td>-1.7385</td>\n",
       "      <td>4.3329</td>\n",
       "      <td>7.2083</td>\n",
       "      <td>1.7930</td>\n",
       "      <td>9.6019</td>\n",
       "      <td>3.1234</td>\n",
       "      <td>10.8722</td>\n",
       "      <td>12.2387</td>\n",
       "      <td>8.4917</td>\n",
       "      <td>1.4631</td>\n",
       "      <td>3.4179</td>\n",
       "      <td>17.7466</td>\n",
       "      <td>-0.4296</td>\n",
       "      <td>-21.1903</td>\n",
       "      <td>15.1338</td>\n",
       "      <td>11.6920</td>\n",
       "      <td>11.1213</td>\n",
       "      <td>14.7781</td>\n",
       "      <td>-35.2833</td>\n",
       "      <td>8.1010</td>\n",
       "      <td>-15.8923</td>\n",
       "      <td>31.0343</td>\n",
       "      <td>3.0895</td>\n",
       "      <td>11.9509</td>\n",
       "      <td>9.4212</td>\n",
       "      <td>2.3883</td>\n",
       "      <td>6.3269</td>\n",
       "      <td>7.7916</td>\n",
       "      <td>22.5844</td>\n",
       "      <td>10.0625</td>\n",
       "      <td>5.8148</td>\n",
       "      <td>7.4935</td>\n",
       "      <td>8.1759</td>\n",
       "      <td>9.2272</td>\n",
       "      <td>4.8759</td>\n",
       "      <td>1.5125</td>\n",
       "      <td>-2.4770</td>\n",
       "      <td>6.7519</td>\n",
       "      <td>-2.4966</td>\n",
       "      <td>5.0772</td>\n",
       "      <td>4.3735</td>\n",
       "      <td>5.0172</td>\n",
       "      <td>3.2189</td>\n",
       "      <td>10.4415</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>1.9811</td>\n",
       "      <td>20.7102</td>\n",
       "      <td>-1.1383</td>\n",
       "      <td>26.6183</td>\n",
       "      <td>-5.7328</td>\n",
       "      <td>24.1632</td>\n",
       "      <td>5.3862</td>\n",
       "      <td>16.0197</td>\n",
       "      <td>2.3496</td>\n",
       "      <td>13.8723</td>\n",
       "      <td>5.7315</td>\n",
       "      <td>3.4639</td>\n",
       "      <td>-10.7063</td>\n",
       "      <td>23.6615</td>\n",
       "      <td>8.4274</td>\n",
       "      <td>10.7796</td>\n",
       "      <td>8.1086</td>\n",
       "      <td>-2.2447</td>\n",
       "      <td>-12.3504</td>\n",
       "      <td>7.0019</td>\n",
       "      <td>17.6296</td>\n",
       "      <td>11.5772</td>\n",
       "      <td>6.8764</td>\n",
       "      <td>1.1258</td>\n",
       "      <td>0.8873</td>\n",
       "      <td>40.5860</td>\n",
       "      <td>2.3236</td>\n",
       "      <td>1.3196</td>\n",
       "      <td>-0.1771</td>\n",
       "      <td>15.3244</td>\n",
       "      <td>16.7790</td>\n",
       "      <td>1.3234</td>\n",
       "      <td>10.6094</td>\n",
       "      <td>3.3319</td>\n",
       "      <td>7.4488</td>\n",
       "      <td>28.0465</td>\n",
       "      <td>14.2082</td>\n",
       "      <td>21.1184</td>\n",
       "      <td>6.8550</td>\n",
       "      <td>4.9583</td>\n",
       "      <td>4.1763</td>\n",
       "      <td>12.2843</td>\n",
       "      <td>3.2424</td>\n",
       "      <td>-1.8641</td>\n",
       "      <td>3.0779</td>\n",
       "      <td>-5.3664</td>\n",
       "      <td>-15.3091</td>\n",
       "      <td>-3.6629</td>\n",
       "      <td>44.7402</td>\n",
       "      <td>12.5925</td>\n",
       "      <td>10.3961</td>\n",
       "      <td>14.8784</td>\n",
       "      <td>2.8145</td>\n",
       "      <td>12.0404</td>\n",
       "      <td>12.0525</td>\n",
       "      <td>1.2739</td>\n",
       "      <td>-5.2733</td>\n",
       "      <td>21.4766</td>\n",
       "      <td>11.6402</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>5.2714</td>\n",
       "      <td>6.8046</td>\n",
       "      <td>5.5424</td>\n",
       "      <td>-3.4735</td>\n",
       "      <td>6.1762</td>\n",
       "      <td>1.0187</td>\n",
       "      <td>0.0791</td>\n",
       "      <td>-4.6594</td>\n",
       "      <td>4.2849</td>\n",
       "      <td>-10.0889</td>\n",
       "      <td>6.3656</td>\n",
       "      <td>15.1580</td>\n",
       "      <td>8.9310</td>\n",
       "      <td>4.1386</td>\n",
       "      <td>8.6308</td>\n",
       "      <td>-4.3091</td>\n",
       "      <td>3.8517</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>16.5291</td>\n",
       "      <td>6.3871</td>\n",
       "      <td>3.2788</td>\n",
       "      <td>15.5939</td>\n",
       "      <td>9.3531</td>\n",
       "      <td>-0.6829</td>\n",
       "      <td>13.1624</td>\n",
       "      <td>-2.5642</td>\n",
       "      <td>23.3752</td>\n",
       "      <td>10.5991</td>\n",
       "      <td>5.5211</td>\n",
       "      <td>5.4439</td>\n",
       "      <td>6.3204</td>\n",
       "      <td>7.6297</td>\n",
       "      <td>3.6555</td>\n",
       "      <td>16.5508</td>\n",
       "      <td>2.2963</td>\n",
       "      <td>2.4910</td>\n",
       "      <td>6.9498</td>\n",
       "      <td>5.9717</td>\n",
       "      <td>-7.4600</td>\n",
       "      <td>-7.1950</td>\n",
       "      <td>17.6936</td>\n",
       "      <td>0.8592</td>\n",
       "      <td>20.4348</td>\n",
       "      <td>14.2191</td>\n",
       "      <td>4.5858</td>\n",
       "      <td>10.0751</td>\n",
       "      <td>-11.9766</td>\n",
       "      <td>5.8413</td>\n",
       "      <td>0.4660</td>\n",
       "      <td>11.9586</td>\n",
       "      <td>4.6020</td>\n",
       "      <td>2.2069</td>\n",
       "      <td>23.2069</td>\n",
       "      <td>-3.0549</td>\n",
       "      <td>10.4571</td>\n",
       "      <td>-5.7635</td>\n",
       "      <td>7.0846</td>\n",
       "      <td>2.5808</td>\n",
       "      <td>-0.3987</td>\n",
       "      <td>9.0477</td>\n",
       "      <td>2.2993</td>\n",
       "      <td>4.8613</td>\n",
       "      <td>15.4447</td>\n",
       "      <td>-0.3476</td>\n",
       "      <td>5.7398</td>\n",
       "      <td>10.8543</td>\n",
       "      <td>11.2183</td>\n",
       "      <td>-9.3580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows  201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target    var_0   var_1    var_2   var_3    var_4    var_5   var_6  \\\n",
       "0            0   8.9255 -6.7863  11.9081  5.0930  11.4607  -9.2834  5.1187   \n",
       "1            0  11.5006 -4.1473  13.8588  5.3890  12.3622   7.0433  5.6208   \n",
       "2            0   8.6093 -2.7457  12.0805  7.8928  10.5825  -9.0837  6.9427   \n",
       "3            0  11.0604 -2.1518   8.9522  7.1957  12.5846  -1.8361  5.8428   \n",
       "4            0   9.8369 -1.4834  12.8746  6.6375  12.2772   2.4486  5.9405   \n",
       "...        ...      ...     ...      ...     ...      ...      ...     ...   \n",
       "399995       0  12.4904  4.0971   9.5729  7.1030  12.2411  -3.8714  5.3379   \n",
       "399996       0  12.0756  0.6536  14.9369  5.3230   7.8097   2.3978  5.5501   \n",
       "399997       0   6.4827 -0.1254   9.8742  6.5121  12.8357   4.7679  6.9999   \n",
       "399998       0   6.8704 -0.7463  12.1506  9.4347  10.8363 -12.1858  4.7211   \n",
       "399999       0  12.8574  2.0760  13.7639  6.1902  10.2858 -17.0299  4.2630   \n",
       "\n",
       "          var_7   var_8   var_9  var_10   var_11   var_12   var_13   var_14  \\\n",
       "0       18.6266 -4.9200  5.7470  2.9252   3.1821  14.0137   0.5745   8.7989   \n",
       "1       16.5338  3.1468  8.0851 -0.4032   8.0585  14.0239   8.4135   5.4345   \n",
       "2       14.6155 -4.9193  5.9525 -0.3249 -11.2648  14.1929   7.3124   7.5244   \n",
       "3       14.9250 -5.8609  8.2450  2.3061   2.8102  13.8463  11.9704   6.4569   \n",
       "4       19.2514  6.2654  7.6784 -9.4458 -12.1419  13.8481   7.8895   7.7894   \n",
       "...         ...     ...     ...     ...      ...      ...      ...      ...   \n",
       "399995  13.5599 -2.5057  6.5893  4.4358  -4.1894  13.9236  14.2479   9.2883   \n",
       "399996  23.0557 -4.4645  7.9690  3.1193  -1.5357  13.9197   9.7544  10.2130   \n",
       "399997  14.6801  1.2815  7.2162  2.0656  -5.0301  14.0485   6.7701  10.6114   \n",
       "399998  24.1893  1.2180  5.3363 -3.7784 -10.7418  14.1667   8.3164   6.9363   \n",
       "399999  11.8193  1.7253  5.5037 -8.3439   6.4556  13.9008  10.1658   4.7202   \n",
       "\n",
       "         var_15   var_16   var_17   var_18   var_19   var_20   var_21  \\\n",
       "0       14.5691   5.7487  -7.2393   4.2840  30.7133  10.5350  16.2191   \n",
       "1       13.7003  13.8275 -15.5849   7.8000  28.5708   3.4287   2.7407   \n",
       "2       14.6472   7.6782  -1.7395   4.7011  20.4775  17.7559  18.1377   \n",
       "3       14.8372  10.7430  -0.4299  15.9426  13.7257  20.3010  12.5579   \n",
       "4       15.0553   8.4871  -3.0680   6.5263  11.3152  21.4246  18.9608   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  14.9302   9.6972   0.1625   7.6976  17.0411   9.4789   9.0912   \n",
       "399996  14.6041  12.6836  -5.5208   4.5473  19.0755  24.6041  14.9972   \n",
       "399997  14.7954   8.8016  -1.0517  18.9271  20.3470  22.9514  26.8826   \n",
       "399998  14.2238  10.2787  -7.9082  29.4910  16.4082  17.6172  21.3965   \n",
       "399999  14.4317   6.2654   4.3089  20.9777  30.1888   8.7158  23.8110   \n",
       "\n",
       "         var_22  var_23   var_24   var_25   var_26  var_27  var_28   var_29  \\\n",
       "0        2.5791  2.4716  14.3831  13.4325  -5.1488 -0.4073  4.9306   5.9965   \n",
       "1        8.5524  3.3716   6.9779  13.8910 -11.7684 -2.5586  5.0464   0.5481   \n",
       "2        1.2145  3.5137   5.6777  13.2177  -7.9940 -2.9029  5.8463   6.1439   \n",
       "3        6.8202  2.7229  12.1354  13.7367   0.8135 -0.9059  5.9070   2.8407   \n",
       "4       10.1102  2.7142  14.2080  13.5433   3.1736 -3.3423  5.9015   7.9352   \n",
       "...         ...     ...      ...      ...      ...     ...     ...      ...   \n",
       "399995   6.6730  2.7345   7.9862  13.7540 -13.0887 -3.1314  5.4377   6.7783   \n",
       "399996   5.8234  3.5070   9.9510  13.8415  -8.9524 -1.3360  4.4207   4.4859   \n",
       "399997   5.7218  2.1176  13.1784  13.9020  -1.8656 -2.7829  5.9136   3.7484   \n",
       "399998   3.9215  2.6106   9.1471  13.4345  -2.5250 -0.5275  4.0404  10.3723   \n",
       "399999   9.6532  2.8999  16.3957  13.9663  -5.7937 -1.7385  4.3329   7.2083   \n",
       "\n",
       "         var_30   var_31  var_32   var_33   var_34   var_35  var_36  var_37  \\\n",
       "0       -0.3085  12.9041 -3.8766  16.8911  11.1920  10.5785  0.6764  7.8871   \n",
       "1       -9.2987   7.8755  1.2859  19.3710  11.3702   0.7399  2.7995  5.8434   \n",
       "2      -11.1025  12.4858 -2.2871  19.0422  11.0449   4.1087  4.6974  6.9346   \n",
       "3      -15.2398  10.4407 -2.5731   6.1796  10.6093  -5.9158  8.1723  2.8521   \n",
       "4       -3.1582   9.4668 -0.0083  19.3239  12.4057   0.6329  2.7922  5.8184   \n",
       "...         ...      ...     ...      ...      ...      ...     ...     ...   \n",
       "399995 -10.7636  12.6498  0.8116  22.1980  11.4345   8.5225  0.0637  8.3442   \n",
       "399996 -16.4447   9.8363  0.9087  18.9750  12.4266  -0.7335  2.0133  6.0986   \n",
       "399997 -15.9028   8.0342 -5.0323  11.7231  11.2059   9.3478  3.4257  5.7810   \n",
       "399998 -19.9593   7.6379 -4.6488  16.2245  10.6183  -2.0612  6.6444  4.4462   \n",
       "399999   1.7930   9.6019  3.1234  10.8722  12.2387   8.4917  1.4631  3.4179   \n",
       "\n",
       "         var_38  var_39   var_40   var_41   var_42   var_43   var_44   var_45  \\\n",
       "0        4.6667  3.8743  -5.2387   7.3746  11.5767  12.0446  11.6418  -7.0170   \n",
       "1       10.8160  3.6783 -11.1147   1.8730   9.8775  11.7842   1.2444 -47.3797   \n",
       "2       10.8917  0.9003 -13.5174   2.2439  11.5283  12.0406   4.1006  -7.9078   \n",
       "3        9.1738  0.6665  -3.8294  -1.0370  11.7770  11.2834   8.0485 -24.6840   \n",
       "4       19.3038  1.4450  -5.5963  14.0685  11.9171  11.5111   6.9087 -65.4863   \n",
       "...         ...     ...      ...      ...      ...      ...      ...      ...   \n",
       "399995   9.6516  5.1408 -16.9031  14.7474  10.8899  10.8464   5.7639  21.4889   \n",
       "399996   9.3941 -6.2155  -9.6860  16.2583  11.0130  11.5700  10.4327 -48.8558   \n",
       "399997   9.0962  6.6659  -4.5044  -1.5156  10.4338  11.2274  11.0851  25.6511   \n",
       "399998  13.2900 -3.9788 -18.5511  -2.3109  10.5114  12.0860  14.5412   7.6649   \n",
       "399999  17.7466 -0.4296 -21.1903  15.1338  11.6920  11.1213  14.7781 -35.2833   \n",
       "\n",
       "         var_46   var_47   var_48   var_49   var_50   var_51  var_52  var_53  \\\n",
       "0        5.9226 -14.2136  16.0283   5.3253  12.9194  29.0460 -0.6940  5.1736   \n",
       "1        7.3718   0.1948  34.4014  25.7037  11.8343  13.2256 -4.1083  6.6885   \n",
       "2       11.1405  -5.7864  20.7477   6.8874  12.9143  19.5856  0.7268  6.4059   \n",
       "3       12.7404 -35.1659   0.7613   8.3838  12.6832   9.5503  1.7895  5.2091   \n",
       "4       13.8657   0.0444  -0.1346  14.4268  13.3273  10.4857 -1.4367  5.7555   \n",
       "...         ...      ...      ...      ...      ...      ...     ...     ...   \n",
       "399995   8.0400 -11.2252  15.7474  31.3351  13.8472   0.4772 -5.5649  6.5326   \n",
       "399996  13.8643 -11.1942  15.8437  15.1464  13.3754   1.5810  0.9192  6.5946   \n",
       "399997   7.3391 -19.6370  20.4729  25.0191  13.0447  13.0015  2.1116  6.8156   \n",
       "399998  10.6118   1.0478   2.4986  24.9719  12.0302   4.0296  0.9277  5.9675   \n",
       "399999   8.1010 -15.8923  31.0343   3.0895  11.9509   9.4212  2.3883  6.3269   \n",
       "\n",
       "         var_54   var_55   var_56  var_57  var_58   var_59   var_60   var_61  \\\n",
       "0       -0.7474  14.8322  11.2668  5.3822  2.0183  10.1166  16.1828   4.9590   \n",
       "1       -8.0946  18.5995  19.3219  7.0118  1.9210   8.8682   8.0109  -7.2417   \n",
       "2        9.3124   6.2846  15.6372  5.8200  1.1000   9.1854  12.5963 -10.3734   \n",
       "3        8.0913  12.3972  14.4698  6.5850  3.3164   9.4638  15.7820 -25.0222   \n",
       "4       -8.5414  14.1482  16.9840  6.1812  1.9548   9.2048   8.6591 -27.7439   \n",
       "...         ...      ...      ...     ...     ...      ...      ...      ...   \n",
       "399995  -8.2557  14.5900  17.2057  7.0540  1.0443  10.1205  10.3625   0.6524   \n",
       "399996   3.6180  17.3741  18.7799  6.2369  3.8269   8.4710   7.8216 -17.6335   \n",
       "399997  16.5393   6.7834  16.5501  7.0152  6.6720   8.6135  15.5433 -19.6367   \n",
       "399998 -15.7085   3.6506  16.2006  6.2206  1.9487   9.3900  17.3474  -1.3388   \n",
       "399999   7.7916  22.5844  10.0625  5.8148  7.4935   8.1759   9.2272   4.8759   \n",
       "\n",
       "        var_62  var_63  var_64  var_65  var_66   var_67  var_68  var_69  \\\n",
       "0       2.0771 -0.2154  8.6748  9.5319  5.8056  22.4321  5.0109 -4.7010   \n",
       "1       1.7944 -1.3147  8.1042  1.5365  5.4007   7.9344  5.0220  2.2302   \n",
       "2       0.8748  5.8042  3.7163 -1.1016  7.3667   9.8565  5.0228 -5.7828   \n",
       "3       3.4418 -4.3923  8.6464  6.3072  5.6221  23.6143  5.0220 -3.9989   \n",
       "4      -0.4952 -1.7839  5.2670 -4.3205  6.9860   1.6184  5.0301 -3.2431   \n",
       "...        ...     ...     ...     ...     ...      ...     ...     ...   \n",
       "399995  2.4346 -1.7746  6.0212  2.7454  4.7968   4.3666  5.0092 -9.8259   \n",
       "399996 -0.2381 -1.1925  5.4755  0.6752  5.2577  13.4306  5.0252 -0.5491   \n",
       "399997  2.1462  1.8310  4.1149 -7.3638  5.8404   5.2266  5.0183 -0.8607   \n",
       "399998  0.3700 -1.9230  8.6771 -4.9034  7.0388  10.5973  5.0177 -3.5720   \n",
       "399999  1.5125 -2.4770  6.7519 -2.4966  5.0772   4.3735  5.0172  3.2189   \n",
       "\n",
       "         var_70  var_71  var_72   var_73   var_74   var_75   var_76   var_77  \\\n",
       "0       21.6374  0.5663  5.1999   8.8600  43.1127  18.3816  -2.3440  23.4104   \n",
       "1       40.5632  0.5134  3.1701  20.1068   7.7841   7.0529   3.2709  23.4822   \n",
       "2        2.3612  0.8520  6.3577  12.1719  19.7312  19.4465   4.5048  23.2378   \n",
       "3        4.0462  0.2500  1.2516  24.4187   4.5290  15.4235  11.6875  23.6273   \n",
       "4       40.1236  0.7737 -0.7264   4.5886  -4.5346  23.3521   1.0273  19.1600   \n",
       "...         ...     ...     ...      ...      ...      ...      ...      ...   \n",
       "399995  16.0449  0.7549 -1.7623  15.2569  23.0787  20.9913  -0.0052  15.3383   \n",
       "399996  33.0498  0.6976  2.8715   6.3629  15.0014   5.6505   5.9231  14.8444   \n",
       "399997  19.4773  0.9079 -3.6426  21.6163  23.0934  23.3446   6.5704  21.7256   \n",
       "399998  23.3534  0.2777 -2.6334  27.3166  30.3428  10.7064   5.8044  21.0938   \n",
       "399999  10.4415  0.5339  1.9811  20.7102  -1.1383  26.6183  -5.7328  24.1632   \n",
       "\n",
       "        var_78   var_79   var_80   var_81   var_82   var_83   var_84   var_85  \\\n",
       "0       6.5199  12.1983  13.6468  13.8372   1.3675   2.9423  -4.5213  21.4669   \n",
       "1       5.5075  13.7814   2.5462  18.1782   0.3683  -4.8210  -5.4850  13.7867   \n",
       "2       6.3191  12.8046   7.4729  15.7811  13.3529  10.1852   5.4604  19.0773   \n",
       "3       4.0806  15.2733   0.7839  10.5404   1.6212  -5.2896   1.6027  17.9762   \n",
       "4       7.1734  14.3937   2.9598  13.3317  -9.2587  -6.7075   7.8984  14.5265   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  3.0695  13.5407  10.8407  19.1084  -3.0142  -2.1188   1.4255  16.1721   \n",
       "399996  5.8406  13.9866  -5.2695  16.9736  -5.0933 -13.3969   0.2658  19.7691   \n",
       "399997  7.5744  12.4480   1.0592  16.5670  14.2049   3.0032  -7.9250  21.4960   \n",
       "399998  6.0646  13.8305   8.8740  16.8102  -5.9580   7.7356   3.6891  17.2579   \n",
       "399999  5.3862  16.0197   2.3496  13.8723   5.7315   3.4639 -10.7063  23.6615   \n",
       "\n",
       "         var_86   var_87   var_88   var_89   var_90  var_91   var_92   var_93  \\\n",
       "0        9.3225  16.4597   7.9984  -1.7069 -21.4494  6.7806  11.0924   9.9913   \n",
       "1      -13.5901  11.0993   7.9022  12.2301   0.4768  6.8852   8.0905  10.9631   \n",
       "2       -4.4577   9.5413  11.9052   2.1447 -22.4038  7.0883  14.1613  10.5080   \n",
       "3       -2.3174  15.6298   4.5474   7.5509  -7.5866  7.0364  14.4027  10.7795   \n",
       "4        7.0799  20.1670   8.0053   3.7954 -39.7997  7.0065   9.3627  10.4316   \n",
       "...         ...      ...      ...      ...      ...     ...      ...      ...   \n",
       "399995  15.3741  13.3447   9.9718   0.4362  -8.9597  7.1372  11.9309  10.6130   \n",
       "399996 -13.7875  14.6716  10.0514   0.6721 -18.5091  7.1056  18.0088  10.8887   \n",
       "399997  11.9197  12.5635   7.8239   2.9544 -16.4258  7.0642  12.1038  11.4935   \n",
       "399998   8.2796  10.9035   7.7676   0.2128  -9.9755  7.2324   6.0425   9.4391   \n",
       "399999   8.4274  10.7796   8.1086  -2.2447 -12.3504  7.0019  17.6296  11.5772   \n",
       "\n",
       "         var_94  var_95   var_96   var_97  var_98  var_99  var_100  var_101  \\\n",
       "0       14.8421  0.1812   8.9642  16.2572  2.1743 -3.4132   9.4763  13.3102   \n",
       "1       11.7569 -1.2722  24.7876  26.6881  1.8944  0.6939 -13.6950   8.4068   \n",
       "2       14.2621  0.2647  20.4031  17.0360  1.6981 -0.0269  -0.3939  12.6317   \n",
       "3        7.2887 -1.0930  11.3596  18.1486  2.8344  1.9480 -19.8592  22.5316   \n",
       "4       14.0553  0.0213  14.7246  35.2988  1.6844  0.6715 -22.9264  12.3562   \n",
       "...         ...     ...      ...      ...     ...     ...      ...      ...   \n",
       "399995  16.3536  0.0897  21.5935  42.3055  0.8472 -0.1292  -9.2950  14.3201   \n",
       "399996  10.9388  0.1835   7.0454  25.8421  2.7307 -1.4396  -5.4111  22.3420   \n",
       "399997   8.8204 -0.5129  22.1831  15.6572  1.4812  1.4177  -3.7730  14.4499   \n",
       "399998  10.2446 -0.2816  33.5611  29.8170  0.1766 -2.8194  -3.5142  14.3217   \n",
       "399999   6.8764  1.1258   0.8873  40.5860  2.3236  1.3196  -0.1771  15.3244   \n",
       "\n",
       "        var_102  var_103  var_104  var_105  var_106  var_107  var_108  \\\n",
       "0       26.5376   1.4403  14.7100   6.0454   9.5426  17.1554  14.1104   \n",
       "1       35.4734   1.7093  15.1866   2.6227   7.3412  32.0888  13.9550   \n",
       "2       14.8863   1.3854  15.0284   3.9995   5.3683   8.6273  14.1963   \n",
       "3       18.6129   1.3512   9.3291   4.2835  10.3907   7.0874  14.3256   \n",
       "4       17.3410   1.6940   7.1179   5.1934   8.8230  10.6617  14.0837   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  36.7479   1.6059   9.6548   5.1387   8.1292  16.3701  14.2756   \n",
       "399996   4.9562   1.7907  13.3697   2.6089   9.4761  15.0610  14.4500   \n",
       "399997  11.2579   1.2567  10.1182   3.8538   8.7105  18.4011  13.9360   \n",
       "399998  37.2495   1.7883  11.5307   3.8483   6.3715  11.8986  14.1428   \n",
       "399999  16.7790   1.3234  10.6094   3.3319   7.4488  28.0465  14.2082   \n",
       "\n",
       "        var_109  var_110  var_111  var_112  var_113  var_114  var_115  \\\n",
       "0       24.3627   2.0323   6.7602   3.9141  -0.4851   2.5240   1.5093   \n",
       "1       13.0858   6.6203   7.1051   5.3523   8.5426   3.6159   4.1569   \n",
       "2       20.3882   3.2304   5.7033   4.5255   2.1929   3.1290   2.9044   \n",
       "3       14.4135   4.2827   6.9750   1.6480  11.6896   2.5762  -2.5459   \n",
       "4       28.2749  -0.1937   5.9654   1.0719   7.9923   2.9138  -3.6135   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  19.7277   1.7365   7.2307   3.2307  12.3384   4.4210   4.2612   \n",
       "399996  23.0233   7.2020   5.7471   3.5116  10.2647   3.2930   3.0700   \n",
       "399997  13.9145   0.3429   6.3459   2.4649  11.1353   4.2721  -2.0414   \n",
       "399998  11.2411   6.6548   4.4822   3.2330  17.0146   3.9640   3.2305   \n",
       "399999  21.1184   6.8550   4.9583   4.1763  12.2843   3.2424  -1.8641   \n",
       "\n",
       "        var_116  var_117  var_118  var_119  var_120  var_121  var_122  \\\n",
       "0        2.5516  15.5752 -13.4221   7.2739  16.0094   9.7268   0.8897   \n",
       "1        3.0454   7.8522 -11.5100   7.5109  31.5899   9.5018   8.2736   \n",
       "2        1.1696  28.7632 -17.2738   2.1056  21.1613   8.9573   2.7768   \n",
       "3        5.3446  38.1015   3.5732   5.0988  30.5644  11.3025   3.9618   \n",
       "4        1.4684  25.6795  13.8224   4.7478  41.1037  12.7140   5.2964   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995   3.5299  22.4758 -14.5210  -0.6718  28.8213  11.1212  -0.3664   \n",
       "399996   3.1054  23.2861 -11.1304   3.0325  32.9108  12.9628  -5.5922   \n",
       "399997   2.4057  17.0167 -12.0394   7.0021  40.1013  14.7136   2.9664   \n",
       "399998  -1.4797   4.2941  -5.5423   4.3318  21.3508   9.3747  -5.7679   \n",
       "399999   3.0779  -5.3664 -15.3091  -3.6629  44.7402  12.5925  10.3961   \n",
       "\n",
       "        var_123  var_124  var_125  var_126  var_127  var_128  var_129  \\\n",
       "0        0.7754   4.2218  12.0039  13.8571  -0.7338  -1.9245  15.4462   \n",
       "1       10.1633   0.1225  12.5942  14.5697   2.4354   0.8194  16.5346   \n",
       "2       -2.1746   3.6932  12.4653  14.1978  -2.5511  -0.9479  17.1092   \n",
       "3       -8.2464   2.7038  12.3441  12.5431  -1.3683   3.5974  13.9761   \n",
       "4        9.7289   3.9370  12.1316  12.5815   7.0642   5.6518  10.9346   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  12.6570   0.9710  12.7450  12.6463  -0.3941  -3.5594  18.6261   \n",
       "399996   3.6516   4.8402  12.8812  12.8668   3.4692  -5.4893  15.9122   \n",
       "399997  13.1119   5.0863  12.9621  12.4400  -0.3585   0.0893  21.0114   \n",
       "399998   0.4538   5.7097  12.1853  11.6951  -3.4794  -0.1509  17.0766   \n",
       "399999  14.8784   2.8145  12.0404  12.0525   1.2739  -5.2733  21.4766   \n",
       "\n",
       "        var_130  var_131  var_132  var_133  var_134  var_135  var_136  \\\n",
       "0       12.8287   0.3587   9.6508   6.5674   5.1726   3.1345  29.4547   \n",
       "1       12.4205  -0.1780   5.7582   7.0513   1.9568  -8.9921   9.7797   \n",
       "2       11.5419   0.0975   8.8186   6.6231   3.9358 -11.7218  24.5437   \n",
       "3       14.3003   1.0486   8.9500   7.1954  -1.1984   1.9586  27.5609   \n",
       "4       11.4266   0.9442   7.7532   6.6173  -6.8304   6.4730  17.1728   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  11.1020   0.1366   9.8752   6.6927 -11.8125  -8.3717  26.3973   \n",
       "399996  12.4401   1.3717   6.5303   6.5312   2.1923   2.9965  13.5914   \n",
       "399997  12.2539   0.6833  10.8232   6.7557  -3.4472   4.9295  14.9176   \n",
       "399998  12.9071   0.6502   8.3375   7.5104 -10.4414   3.7784   4.0272   \n",
       "399999  11.6402   0.9804   5.2714   6.8046   5.5424  -3.4735   6.1762   \n",
       "\n",
       "        var_137  var_138  var_139  var_140  var_141  var_142  var_143  \\\n",
       "0       31.4045   2.8279  15.6599   8.3307  -5.6011  19.0614  11.2663   \n",
       "1       18.1577  -1.9721  16.1622   3.6937   6.6803  -0.3243  12.2806   \n",
       "2       15.5827   3.8212   8.6674   7.3834  -2.4438  10.2158   7.4844   \n",
       "3       24.6065  -2.8233   8.9821   3.8873  15.9638  10.0142   7.8388   \n",
       "4       25.8128   2.6791  13.9547   6.6289  -4.3965  11.7159  16.1080   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  11.7138  -0.5914   9.3315  -2.3878   1.8984   5.8921   8.3873   \n",
       "399996  30.7308  -0.3007  11.1424   8.6554  -1.3220   8.2257  12.8353   \n",
       "399997  11.6466  -6.1010  12.5493  10.9734   3.6920   8.4625  11.3665   \n",
       "399998  20.2614   0.8237  -6.6060   1.3595  -6.5414  12.2554  15.4690   \n",
       "399999   1.0187   0.0791  -4.6594   4.2849 -10.0889   6.3656  15.1580   \n",
       "\n",
       "        var_144  var_145  var_146  var_147  var_148  var_149  var_150  \\\n",
       "0        8.6989   8.3694  11.5659 -16.4727   4.0288  17.9244  18.5177   \n",
       "1        8.6086  11.0738   8.9231  11.7700   4.2578  -4.4223  20.6294   \n",
       "2        9.1104   4.3649  11.4934   1.7624   4.0714  -1.2681  14.3330   \n",
       "3        9.9718   2.9253  10.4994   4.1622   3.7613   2.3701  18.0984   \n",
       "4        7.6874   9.1570  11.5670 -12.7047   3.7574   9.9110  20.1461   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995   7.6926   1.1354   8.9153  -7.3375   3.8037  -7.5865  19.0708   \n",
       "399996   8.9400  13.3364  11.7449  -0.4871   3.7961  -6.4872  15.4872   \n",
       "399997   8.2130   4.8325  10.0756 -19.9297   3.9025   8.0388  15.9884   \n",
       "399998   9.7661   0.4663  10.9107  -6.4811   3.9032  18.4450  15.2502   \n",
       "399999   8.9310   4.1386   8.6308  -4.3091   3.8517   0.7258  16.5291   \n",
       "\n",
       "        var_151  var_152  var_153  var_154  var_155  var_156  var_157  \\\n",
       "0       10.7800   9.0056  16.6964  10.4838   1.6573  12.1749 -13.1324   \n",
       "1       14.8743   9.4317  16.7242  -0.5687   0.1898  12.2419  -9.6953   \n",
       "2        8.0088   4.4015  14.1479  -5.1747   0.5778  14.5362  -1.7624   \n",
       "3       17.1765   7.6508  18.2452  17.0336 -10.9370  12.0500  -1.2155   \n",
       "4        1.2995   5.8493  19.8234   4.7022  10.6101  13.0021 -12.6068   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  13.6887   7.0208  16.2498  10.5968  12.5921  13.3929 -13.8908   \n",
       "399996  17.3317   9.6841  19.7184  -0.3729   2.9087  12.5196  -4.0016   \n",
       "399997  11.2205   6.7047  17.3389  10.5394  -1.8955  12.0981   3.8825   \n",
       "399998   9.4846   9.6340  13.5600  13.7344  -8.6708  12.8264  -8.0187   \n",
       "399999   6.3871   3.2788  15.5939   9.3531  -0.6829  13.1624  -2.5642   \n",
       "\n",
       "        var_158  var_159  var_160  var_161  var_162  var_163  var_164  \\\n",
       "0       17.6054  11.5423  15.4576   5.3133   3.6159   5.0384   6.6760   \n",
       "1       22.3949  10.6261  29.4846   5.8683   3.8208  15.8348  -5.0121   \n",
       "2       33.8820  11.6041  13.2070   5.8442   4.7086   5.7141  -1.0410   \n",
       "3       19.9750  12.3892  31.8833   5.9684   7.2084   3.8899 -11.0882   \n",
       "4       27.0846   8.0913  33.5107   5.6953   5.4663  18.2201   6.5769   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  11.8437  11.2875  20.0500   5.8561   4.1490  11.6025   1.9091   \n",
       "399996  20.3712   9.0545  37.2184   5.7610   3.3126   4.1035   2.9809   \n",
       "399997  34.6457   9.1157  20.3925   5.6120   5.3182  19.0850  -2.7405   \n",
       "399998  22.8800   6.1382  30.2004   5.5373   2.4294  17.4894  -2.1165   \n",
       "399999  23.3752  10.5991   5.5211   5.4439   6.3204   7.6297   3.6555   \n",
       "\n",
       "        var_165  var_166  var_167  var_168  var_169  var_170  var_171  \\\n",
       "0       12.6644   2.7004  -0.6975   9.5981   5.4879  -4.7645  -8.4254   \n",
       "1       15.1345   3.2003   9.3192   3.8821   5.7999   5.5378   5.0988   \n",
       "2       20.5092   3.2790  -5.5952   7.3176   5.7690  -7.0927  -3.9116   \n",
       "3       17.2502   2.5881  -2.7018   0.5641   5.3430  -7.1541  -6.1920   \n",
       "4       21.2607   3.2304  -1.7759   3.1283   5.5518   1.4493  -2.6627   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  18.4879   2.9838  -5.4682   4.4443   5.8550   4.3673  -3.1908   \n",
       "399996  25.4507   2.9142   7.0334   7.1331   6.0174   6.0527   3.2659   \n",
       "399997  19.5322   2.5623 -14.6855   5.5657   5.2742  -7.3142   7.6154   \n",
       "399998  13.7832   2.8288  -1.7876   7.0118   5.0457   3.5383   8.8912   \n",
       "399999  16.5508   2.2963   2.4910   6.9498   5.9717  -7.4600  -7.1950   \n",
       "\n",
       "        var_172  var_173  var_174  var_175  var_176  var_177  var_178  \\\n",
       "0       20.8773   3.1531  18.5618   7.7423 -10.1245  13.7241  -3.5189   \n",
       "1       22.0330   5.5134  30.2645  10.4968  -7.2352  16.5721  -7.3477   \n",
       "2        7.2569  -5.8234  25.6820  10.9202  -0.3104   8.8438  -9.7009   \n",
       "3       18.2366  11.7134  14.7483   8.1013  11.8771  13.9552 -10.4701   \n",
       "4       19.8056   2.3705  18.4685  16.3309  -3.3456  13.5261   1.7189   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  12.1283   2.3392  22.8965  13.7958   2.9110  11.7910  -8.3711   \n",
       "399996  18.1972   3.4801  26.0080   9.2702   0.1574  13.3378   7.9268   \n",
       "399997  12.7750   0.3091   3.9351  14.6798  -8.9574  14.1055   7.5467   \n",
       "399998  18.4672   1.8273  30.2822   9.4743   5.3983  10.1313   0.9704   \n",
       "399999  17.6936   0.8592  20.4348  14.2191   4.5858  10.0751 -11.9766   \n",
       "\n",
       "        var_179  var_180  var_181  var_182  var_183  var_184  var_185  \\\n",
       "0        1.7202  -8.4051   9.0164   3.0657  14.3691  25.8398   5.8764   \n",
       "1       11.0752  -5.5937   9.4878 -14.9100   9.4245  22.5441  -4.8622   \n",
       "2        2.4013  -4.2935   9.3908 -13.2648   3.1545  23.0866  -5.3000   \n",
       "3        5.6961  -3.7546   8.4117   1.8986   7.2601  -0.4639  -0.0498   \n",
       "4        5.1743  -7.6938   9.7685   4.8910  12.2198  11.8503  -7.8931   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995  -0.9903  -8.7420   8.3954   6.7660  16.2471  14.3125  -4.0452   \n",
       "399996   4.2928   2.1362   8.9557   0.9003   3.5912  14.5755  -3.4588   \n",
       "399997   0.8776   3.5579  10.6485 -12.1161  16.5248  26.2340 -11.3249   \n",
       "399998   3.9345 -10.4854   8.7874   3.4940  17.0222  22.0108  -0.1467   \n",
       "399999   5.8413   0.4660  11.9586   4.6020   2.2069  23.2069  -3.0549   \n",
       "\n",
       "        var_186  var_187  var_188  var_189  var_190  var_191  var_192  \\\n",
       "0       11.8411 -19.7159  17.5743   0.5857   4.4354   3.9642   3.1364   \n",
       "1        7.6543 -15.9319  13.3175  -0.3566   7.6421   7.7214   2.5837   \n",
       "2        5.3745  -6.2660  10.1934  -0.8417   2.9057   9.7905   1.6704   \n",
       "3        7.9336 -12.8279  12.4124   1.8489   4.4666   4.7433   0.7178   \n",
       "4        6.4209   5.9270  16.0201  -0.2829  -1.4905   9.5214  -0.1508   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "399995   8.6593 -20.7593  15.1800   2.4887  11.4350   7.5056   0.6534   \n",
       "399996  10.1281 -29.7886  20.5672  -0.8228   0.3689   8.3071   1.4771   \n",
       "399997  13.2318   5.8970  14.0280  -1.0962   1.2615   8.5350   2.2625   \n",
       "399998  10.5577  -1.8767  16.6669   2.2175   2.8475  10.6147   3.0183   \n",
       "399999  10.4571  -5.7635   7.0846   2.5808  -0.3987   9.0477   2.2993   \n",
       "\n",
       "        var_193  var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0        1.6910  18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "1       10.9516  15.4305   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "2        1.6858  21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "3        1.4214  23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4        9.1942  13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104  \n",
       "...         ...      ...      ...      ...      ...      ...      ...  \n",
       "399995   5.0310  19.6522   0.2626  -6.8085   8.2127  21.1710 -20.4727  \n",
       "399996   5.1579  18.0875  -1.6203   2.5211   9.1977  18.1966 -18.0426  \n",
       "399997   6.9436  22.4766   2.1484   9.3593   6.8472  13.0793   4.4414  \n",
       "399998   2.5069  16.0805   0.4006  -5.5772   8.4289  20.8129 -18.4453  \n",
       "399999   4.8613  15.4447  -0.3476   5.7398  10.8543  11.2183  -9.3580  \n",
       "\n",
       "[400000 rows x 201 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73e83dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 200)\n",
      "(160000,)\n",
      "(40000, 200)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "x = augmented_data.drop('target', axis=1)\n",
    "y = augmented_data['target']\n",
    "\n",
    "# Split the augmented data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2810d0",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70d871af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression()\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec72e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "y_pred_lrn = clf.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1508bb9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy_score for test data : 0.914325\n",
      " accuracy_score for train data :0.911475\n"
     ]
    }
   ],
   "source": [
    "print(f\" accuracy_score for test data : {accuracy_score(y_test,y_pred)}\")\n",
    "print(f\" accuracy_score for train data :{accuracy_score(y_train,y_pred_lrn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8257a910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35607   460]\n",
      " [ 2967   966]]\n"
     ]
    }
   ],
   "source": [
    "cm=confusion_matrix(y_test,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9f56d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24561403508771928"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall=recall_score(y_test,y_pred)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e67aba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6774193548387096"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision=precision_score(y_test,y_pred)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b89bb9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3605150214592275"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1score=f1_score(y_test,y_pred)\n",
    "f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e508c6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     36067\n",
      "           1       0.68      0.25      0.36      3933\n",
      "\n",
      "    accuracy                           0.91     40000\n",
      "   macro avg       0.80      0.62      0.66     40000\n",
      "weighted avg       0.90      0.91      0.90     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr=classification_report(y_test,y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e11003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302a300",
   "metadata": {},
   "source": [
    "## LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8dcdd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LightGBM parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'early_stopping_round': 10  # specify early stopping round\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c5b91",
   "metadata": {},
   "source": [
    "# Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "947a71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(x_train, label=y_train)\n",
    "test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24a03144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16136, number of negative: 143864\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100850 -> initscore=-2.187816\n",
      "[LightGBM] [Info] Start training from score -2.187816\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_error: 0.09905\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "539114c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb732e",
   "metadata": {},
   "source": [
    "# Model Evaluation lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40aa97f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90095\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bafbfc",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning by LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2facd9",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c455ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "   \n",
    "    \n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'dart', 'goss'],     \n",
    "    'num_leaves': sp_randint(6, 50),              \n",
    "    'learning_rate': [0.01, 0.05, 0.1],           \n",
    "    'feature_fraction': [0.6, 0.7, 0.8, 0.9],     \n",
    "    'bagging_fraction': [0.6, 0.7, 0.8, 0.9],     \n",
    "    'bagging_freq': sp_randint(1, 10),            \n",
    "    'max_depth': sp_randint(3, 20),               \n",
    "    'lambda_l1': [0, 1, 2],                       \n",
    "    'lambda_l2': [0, 1, 2]                     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8013bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(objective='binary', metric='binary_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b40ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(lgb_model, param_distributions=param_grid, n_iter=100, scoring='accuracy', cv=3, verbose=1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0af37051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066859 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057752 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063787 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055666 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059474 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067921 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067506 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063698 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057504 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063830 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063518 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=2, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10776, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106666, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101026 -> initscore=-2.185880\n",
      "[LightGBM] [Info] Start training from score -2.185880\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Info] Number of positive: 10777, number of negative: 95890\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 106667, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101034 -> initscore=-2.185787\n",
      "[LightGBM] [Info] Start training from score -2.185787\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=1, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=9, subsample_freq=0 will be ignored. Current value: bagging_freq=9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Number of positive: 16165, number of negative: 143835\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101031 -> initscore=-2.185818\n",
      "[LightGBM] [Info] Start training from score -2.185818\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=LGBMClassifier(metric=&#x27;binary_error&#x27;,\n",
       "                                            objective=&#x27;binary&#x27;),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;bagging_fraction&#x27;: [0.6, 0.7, 0.8,\n",
       "                                                             0.9],\n",
       "                                        &#x27;bagging_freq&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000017A51431510&gt;,\n",
       "                                        &#x27;boosting_type&#x27;: [&#x27;gbdt&#x27;, &#x27;dart&#x27;,\n",
       "                                                          &#x27;goss&#x27;],\n",
       "                                        &#x27;feature_fraction&#x27;: [0.6, 0.7, 0.8,\n",
       "                                                             0.9],\n",
       "                                        &#x27;lambda_l1&#x27;: [0, 1, 2],\n",
       "                                        &#x27;lambda_l2&#x27;: [0, 1, 2],\n",
       "                                        &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1],\n",
       "                                        &#x27;max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000017A51433C10&gt;,\n",
       "                                        &#x27;num_leaves&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x00000179833BDF90&gt;},\n",
       "                   random_state=42, scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=LGBMClassifier(metric=&#x27;binary_error&#x27;,\n",
       "                                            objective=&#x27;binary&#x27;),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;bagging_fraction&#x27;: [0.6, 0.7, 0.8,\n",
       "                                                             0.9],\n",
       "                                        &#x27;bagging_freq&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000017A51431510&gt;,\n",
       "                                        &#x27;boosting_type&#x27;: [&#x27;gbdt&#x27;, &#x27;dart&#x27;,\n",
       "                                                          &#x27;goss&#x27;],\n",
       "                                        &#x27;feature_fraction&#x27;: [0.6, 0.7, 0.8,\n",
       "                                                             0.9],\n",
       "                                        &#x27;lambda_l1&#x27;: [0, 1, 2],\n",
       "                                        &#x27;lambda_l2&#x27;: [0, 1, 2],\n",
       "                                        &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1],\n",
       "                                        &#x27;max_depth&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000017A51433C10&gt;,\n",
       "                                        &#x27;num_leaves&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x00000179833BDF90&gt;},\n",
       "                   random_state=42, scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(metric=&#x27;binary_error&#x27;, objective=&#x27;binary&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(metric=&#x27;binary_error&#x27;, objective=&#x27;binary&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=LGBMClassifier(metric='binary_error',\n",
       "                                            objective='binary'),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'bagging_fraction': [0.6, 0.7, 0.8,\n",
       "                                                             0.9],\n",
       "                                        'bagging_freq': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000017A51431510>,\n",
       "                                        'boosting_type': ['gbdt', 'dart',\n",
       "                                                          'goss'],\n",
       "                                        'feature_fraction': [0.6, 0.7, 0.8,\n",
       "                                                             0.9],\n",
       "                                        'lambda_l1': [0, 1, 2],\n",
       "                                        'lambda_l2': [0, 1, 2],\n",
       "                                        'learning_rate': [0.01, 0.05, 0.1],\n",
       "                                        'max_depth': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000017A51433C10>,\n",
       "                                        'num_leaves': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x00000179833BDF90>},\n",
       "                   random_state=42, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "40c9ed6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'bagging_fraction': 0.9, 'bagging_freq': 6, 'boosting_type': 'gbdt', 'feature_fraction': 0.8, 'lambda_l1': 0, 'lambda_l2': 2, 'learning_rate': 0.1, 'max_depth': 17, 'num_leaves': 46}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6982b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_lgt = best_model.predict(x_test)\n",
    "y_trnlgbm_pred = best_model.predict(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "23c9172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91185\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_lgt)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdd87de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification report for test data:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95     36038\n",
      "           1       0.85      0.12      0.20      3962\n",
      "\n",
      "    accuracy                           0.91     40000\n",
      "   macro avg       0.88      0.56      0.58     40000\n",
      "weighted avg       0.91      0.91      0.88     40000\n",
      "\n",
      "Classification report for train data:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96    143864\n",
      "           1       0.95      0.19      0.32     16136\n",
      "\n",
      "    accuracy                           0.92    160000\n",
      "   macro avg       0.93      0.59      0.64    160000\n",
      "weighted avg       0.92      0.92      0.89    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" Classification report for test data: {classification_report(y_test,y_pred)}\")\n",
    "print(f\"Classification report for train data: {classification_report(y_train,y_trnlgbm_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466b3ef",
   "metadata": {},
   "source": [
    "class_weights = {0: 1, 1: 10}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "837f6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3496b93",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "965cb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtc = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8a949ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe186dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Dtc = Dtc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8298f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_Dtc_train = Dtc.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fa16e",
   "metadata": {},
   "source": [
    "# Model Evaluation Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "903aedc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score for test data :0.8367\n",
      " accuracy score for train data :1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\" accuracy score for test data :{accuracy_score(y_test,y_pred_Dtc)}\")\n",
    "print(f\" accuracy score for train data :{accuracy_score(y_train,y_pred_Dtc_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5932ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " classification Report for test data :              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91     36067\n",
      "           1       0.20      0.22      0.21      3933\n",
      "\n",
      "    accuracy                           0.84     40000\n",
      "   macro avg       0.56      0.56      0.56     40000\n",
      "weighted avg       0.84      0.84      0.84     40000\n",
      "\n",
      " Classification Report for train data :              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    143835\n",
      "           1       1.00      1.00      1.00     16165\n",
      "\n",
      "    accuracy                           1.00    160000\n",
      "   macro avg       1.00      1.00      1.00    160000\n",
      "weighted avg       1.00      1.00      1.00    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" classification Report for test data :{classification_report(y_test,y_pred_Dtc)}\")\n",
    "print(f\" Classification Report for train data :{classification_report(y_train,y_pred_Dtc_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "455a1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm=SMOTE()\n",
    "x_sm,y_sm=sm.fit_resample(x_train ,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43b85f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtc.fit(x_sm,y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad13712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = Dtc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23d1c1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = Dtc.predict(x_train)\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "303b796b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred_sm = Dtc.predict(x_sm)\n",
    "y_train_pred_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8eb6304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score for test data :0.703\n",
      " accuracy score for train data :1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\" accuracy score for test data :{accuracy_score(y_test,dt_pred)}\")\n",
    "print(f\" accuracy score for train data :{accuracy_score(y_train,y_pred_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ea6d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification Report for test data :              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91     36067\n",
      "           1       0.20      0.22      0.21      3933\n",
      "\n",
      "    accuracy                           0.84     40000\n",
      "   macro avg       0.56      0.56      0.56     40000\n",
      "weighted avg       0.84      0.84      0.84     40000\n",
      "\n",
      " Classification Report for train data :              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    143835\n",
      "           1       1.00      1.00      1.00     16165\n",
      "\n",
      "    accuracy                           1.00    160000\n",
      "   macro avg       1.00      1.00      1.00    160000\n",
      "weighted avg       1.00      1.00      1.00    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" Classification Report for test data :{classification_report(y_test,y_pred_Dtc)}\")\n",
    "print(f\" Classification Report for train data :{classification_report(y_train,y_pred_Dtc_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "369608b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtc.fit(x_sm,y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44248986",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = Dtc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "647a979b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = Dtc.predict(x_train)\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b470289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred_sm = Dtc.predict(x_sm)\n",
    "y_train_pred_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56b1daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score for test data :0.702725\n",
      " accuracy score for train data :1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\" accuracy score for test data :{accuracy_score(y_test,dt_pred)}\")\n",
    "print(f\" accuracy score for train data :{accuracy_score(y_train,y_pred_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8890c058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification Report for test data :              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91     36067\n",
      "           1       0.20      0.22      0.21      3933\n",
      "\n",
      "    accuracy                           0.84     40000\n",
      "   macro avg       0.56      0.56      0.56     40000\n",
      "weighted avg       0.84      0.84      0.84     40000\n",
      "\n",
      " Classification Report for train data :              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    143835\n",
      "           1       1.00      1.00      1.00     16165\n",
      "\n",
      "    accuracy                           1.00    160000\n",
      "   macro avg       1.00      1.00      1.00    160000\n",
      "weighted avg       1.00      1.00      1.00    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" Classification Report for test data :{classification_report(y_test,y_pred_Dtc)}\")\n",
    "print(f\" Classification Report for train data :{classification_report(y_train,y_pred_Dtc_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4cc88",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8b75ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62a81d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17481d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc = rfc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22cd25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc_train = rfc.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa216d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy_score for test data : 0.901725\n",
      " accuracy_score for train data : 0.99999375\n"
     ]
    }
   ],
   "source": [
    "print(f\" accuracy_score for test data : {accuracy_score (y_test, y_pred_rfc)}\")\n",
    "print(f\" accuracy_score for train data : {accuracy_score (y_train, y_pred_rfc_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b7e9e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification report for test data :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     36067\n",
      "           1       1.00      0.00      0.00      3933\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.95      0.50      0.47     40000\n",
      "weighted avg       0.91      0.90      0.86     40000\n",
      "\n",
      " Classification report for train data :               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    143835\n",
      "           1       1.00      1.00      1.00     16165\n",
      "\n",
      "    accuracy                           1.00    160000\n",
      "   macro avg       1.00      1.00      1.00    160000\n",
      "weighted avg       1.00      1.00      1.00    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" Classification report for test data : {classification_report (y_test, y_pred_rfc)}\")\n",
    "print(f\" Classification report for train data : {classification_report (y_train, y_pred_rfc_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce52bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid  = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None]    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f1f8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(rfc , param_distributions=param_grid, n_iter=10, cv =5 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c3c833a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m     \u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1770\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:186\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    184\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e120df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dd97c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0\n",
      "[LightGBM] [Warning] lambda_l2 is set=2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    }
   ],
   "source": [
    "rfc_pred = best_model.predict(x_test)\n",
    "rfc_pred_train = best_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "43709652",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score for test data : 0.91185\n",
      " accuracy score for train data : 0.92154375\n"
     ]
    }
   ],
   "source": [
    "print(f\" accuracy score for test data : {accuracy_score(y_test,rfc_pred)}\")\n",
    "print(f\" accuracy score for train data : {accuracy_score(y_train,rfc_pred_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c639693e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " classification report for test data :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95     36067\n",
      "           1       0.85      0.12      0.22      3933\n",
      "\n",
      "    accuracy                           0.91     40000\n",
      "   macro avg       0.88      0.56      0.59     40000\n",
      "weighted avg       0.91      0.91      0.88     40000\n",
      "\n",
      " classification report for train data :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96    143835\n",
      "           1       0.97      0.23      0.37     16165\n",
      "\n",
      "    accuracy                           0.92    160000\n",
      "   macro avg       0.94      0.62      0.67    160000\n",
      "weighted avg       0.92      0.92      0.90    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" classification report for test data : {classification_report(y_test,rfc_pred)}\")\n",
    "print(f\" classification report for train data : {classification_report(y_train,rfc_pred_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81625fa6",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98706d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.3729 - accuracy: 0.8930 - val_loss: 0.3206 - val_accuracy: 0.9020\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.3319 - accuracy: 0.8982 - val_loss: 0.3210 - val_accuracy: 0.9020\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.3313 - accuracy: 0.8982 - val_loss: 0.3211 - val_accuracy: 0.9020\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.3302 - accuracy: 0.8982 - val_loss: 0.3209 - val_accuracy: 0.9020\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.3300 - accuracy: 0.8982 - val_loss: 0.3207 - val_accuracy: 0.9020\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.3299 - accuracy: 0.8982 - val_loss: 0.3208 - val_accuracy: 0.9020\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3295 - accuracy: 0.8982 - val_loss: 0.3209 - val_accuracy: 0.9020\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.3294 - accuracy: 0.8982 - val_loss: 0.3209 - val_accuracy: 0.9020\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3293 - accuracy: 0.8982 - val_loss: 0.3208 - val_accuracy: 0.9020\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.3293 - accuracy: 0.8982 - val_loss: 0.3207 - val_accuracy: 0.9020\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 0.3214 - accuracy: 0.9017\n",
      "Test Loss: 0.3213876485824585\n",
      "Test Accuracy: 0.9016749858856201\n",
      "1250/1250 [==============================] - 1s 811us/step\n",
      "5000/5000 [==============================] - 4s 834us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# 3. Build Neural Network Model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Train the Model\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# 5. Evaluate the Model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# 6. Make Predictions\n",
    "y_pred_nn = model.predict(x_test)\n",
    "y_trn_preddnn = model.predict(x_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_binary = (y_pred_nn > 0.5).astype(int) # Convert probabilities to binary predictions\n",
    "y_pred_binary_tn = (y_trn_preddnn > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f25237fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy_score for test data : 0.901675\n",
      " accuracy_score for train data : 0.89896875\n"
     ]
    }
   ],
   "source": [
    "print(f\" accuracy_score for test data : {accuracy_score (y_test, y_pred_binary)}\")\n",
    "print(f\" accuracy_score for train data : {accuracy_score (y_train, y_pred_binary_tn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c1292e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " classification report for test data :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     36067\n",
      "           1       0.00      0.00      0.00      3933\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.45      0.50      0.47     40000\n",
      "weighted avg       0.81      0.90      0.86     40000\n",
      "\n",
      " classification report for train data :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95    143835\n",
      "           1       0.00      0.00      0.00     16165\n",
      "\n",
      "    accuracy                           0.90    160000\n",
      "   macro avg       0.45      0.50      0.47    160000\n",
      "weighted avg       0.81      0.90      0.85    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" classification report for test data : {classification_report(y_test,y_pred_binary)}\")\n",
    "print(f\" classification report for train data : {classification_report(y_train,y_pred_binary_tn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be580fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 834us/step\n",
      "Accuracy: 0.901675\n",
      "Confusion Matrix:\n",
      " [[36067     0]\n",
      " [ 3933     0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     36067\n",
      "           1       0.00      0.00      0.00      3933\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.45      0.50      0.47     40000\n",
      "weighted avg       0.81      0.90      0.86     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y_test, y_pred_binary)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b914e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f809d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b061a",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fccf05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4947379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the xGBoost model\n",
    "model = xgb.XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d07af",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd862ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_xg = model.predict(x_test)\n",
    "y_pred_xg_tr = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed45513",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d6ee6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy_score for test data : 0.9156\n",
      " accuracy_score for train data : 0.95958125\n"
     ]
    }
   ],
   "source": [
    "print(f\" accuracy_score for test data : {accuracy_score (y_test, y_pred_xg)}\")\n",
    "print(f\" accuracy_score for train data : {accuracy_score (y_train, y_pred_xg_tr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6c8db8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " classification report for test data :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     36067\n",
      "           1       0.69      0.26      0.38      3933\n",
      "\n",
      "    accuracy                           0.92     40000\n",
      "   macro avg       0.81      0.62      0.67     40000\n",
      "weighted avg       0.90      0.92      0.90     40000\n",
      "\n",
      " classification report for train data :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98    143835\n",
      "           1       0.98      0.61      0.75     16165\n",
      "\n",
      "    accuracy                           0.96    160000\n",
      "   macro avg       0.97      0.81      0.87    160000\n",
      "weighted avg       0.96      0.96      0.96    160000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" classification report for test data : {classification_report(y_test,y_pred_xg)}\")\n",
    "print(f\" classification report for train data : {classification_report(y_train,y_pred_xg_tr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f177b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "732e3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize xGBoost classifier\n",
    "model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a549e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f66dc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01], &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300],\n",
       "                         &#x27;subsample&#x27;: [0.8, 0.9, 1.0]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01], &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300],\n",
       "                         &#x27;subsample&#x27;: [0.8, 0.9, 1.0]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             param_grid={'learning_rate': [0.1, 0.01], 'max_depth': [3, 5, 7],\n",
       "                         'n_estimators': [100, 200, 300],\n",
       "                         'subsample': [0.8, 0.9, 1.0]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform grid search\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9549697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d5bb638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300, 'subsample': 0.8}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Re-train the model with the best parameters\n",
    "best_model = xgb.XGBClassifier(**best_params)\n",
    "best_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f6c58c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.918475\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model\n",
    "y_pred = best_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01475319",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['LogisticRegression','LightGBM','Decision Tree Classifie','Random Forest','Neural Network','XGBoost']\n",
    "accuracy_scores = [accuracy_score(y_test,y_pred),accuracy_score(y_test, y_pred_lgt),accuracy_score(y_test,y_pred_Dtc),accuracy_score(y_test,rfc_pred),accuracy_score(y_test,y_pred_binary),accuracy_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fa48bf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "bar_plot  = sns.barplot(x = models, y = accuracy_scores, width = 0.5)\n",
    "\n",
    "for index, value in enumerate(accuracy_scores):\n",
    "    bar_plot.text(index, value + 0.001, f'{value:.4f}',ha = 'center', va = 'bottom', fontsize=9)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768c9f0",
   "metadata": {},
   "source": [
    "Comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6cdf9",
   "metadata": {},
   "source": [
    "##### LogisticRegrresion Model:\n",
    "Accuracy_score  : 0.914325\n",
    "\n",
    "Logistic regression is known for its simplicity and interpretability, making it easy to understand and implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c1955",
   "metadata": {},
   "source": [
    "##### LightGBM Model:\n",
    "Accuracy score : 0.91185\n",
    "\n",
    "LightGBM is Fast, efficient, gradient boosting framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007f6be",
   "metadata": {},
   "source": [
    "##### Decision Tree Classifier Model:\n",
    "Accuracy score : 0.8367\n",
    "\n",
    "Decision trees may be prone to overfitting, and their interpretability is a strength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad9e3d",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier Model:\n",
    "Accuracy score : 0.91185\n",
    "\n",
    "Random Forests are an ensemble method known for handling complex relationships and reducing overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e7664",
   "metadata": {},
   "source": [
    "##### Neural Network Model:\n",
    "Accuracy_score : 0.901675\n",
    "\n",
    "Neural Network for complex pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601554c",
   "metadata": {},
   "source": [
    "##### XGBoost Model:\n",
    "Accuracy Score: 0.918475\n",
    "\n",
    "XGBoost Powerful gradient boosting library for efficient and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78215e29",
   "metadata": {},
   "source": [
    "##### Result:\n",
    "\n",
    "Among various models, XGBoost stand out as the most suitable models for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153f813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c57b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
